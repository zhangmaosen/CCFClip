介绍我下面，就直接进入主题。
就讲我今天的内容，就是我们自身智能的这样的一个 PI 模型：感知、想象和执行。因为我在前面的一些讨论中，当然说这个愿景大家能来这今天来这里肯定很清楚，就是我们的愿景是要实现通用机器人。我比较喜欢这部电影是因为这部电影觉得是我认为在可见的未来，是所有的东西都能够实现的。但“西部世界”可能比较遥远，这部电影我觉得是可能实现的，大家有兴趣可以看一看。当然，具身智能的话我也不多讲了，在很早的图灵时代已经就提出了这样的一个居然智能这个概念。为什么他学术上是一个从智能，这是一个更加是一个智能，是一个非常高的一个智能水平？是因为身体的身体联合去学习，就像我们这个这个实验我讲了很多。
我就来快速讲一下：就是这只实验室，它有两只猫。然后，这只猫是主动来走；另一只猫它出生就被关起来。但是，它们看的东西是一样的。最后，这只猫把他被动的猫下来之后，他就失去了行为能力。
那这是为什么呢？这是因为，他没有基于身体的智能，所以他就失去了这种行为的能力。好，这也证明了居然智能的重要性。当然，到今天，不用讲那么多，不言而喻了。因为我记得我之前的讲座还得讲一堆什么是居间智能，可能就我就快速跳过了。
好，我们的，是单车现在捐赠，肯定是大家要做的很多技术路线。
对吧，我们也在一直探索，从16年开始，在探索一种什么样的一个技术智能的框架。就是，就他学术的底座应该什么样子？那我们就类比人的模块，我们会把它塞在这个部分，就是大具身感知，居然想象。
居然，执行是什么逻辑呢？就是其实我们做件事情的话，我们首先要知道他感知到这个世界的模型。然后的话，其实我脑子里是在想象的，就是我们做件事情脑子是在动脑的，比如说我要把它拧开，我脑子是过了一遍，只是我们自己下意识没感觉而已，这属于是大脑功能。我们把这个世界抽象出来，然后第二部分就是执行，执行是完全小脑的功能。大家觉得那执行是不是比较简单？我想我就能做。
其实并不是那么简单的，因为它涉及到我们身体的下意识反应。这个我会后面专门展开讲。所以，这两部分对于世界的功能，是对大脑对世界理解和抽象最后要具象地去执行。我们就来展开讲。第一个部分就是巨深的感知了。当然，大家可能看过很多关于机缘、视觉感知的信息。但是我们需要的感知有什么不一样呢？其实这里面有一个科学问题，就是如何去通用地估计物理的常识。那这些事情我们觉得都很容易是吧？
但其实不这么容易，就是我们把这个箱子，把这个盒子给它检测出来是比较容易的，但是你要推断出这里是可以操作，这个轴可以翻开，这是有难度的。
这件事情，而我们是通用的，做到我们专门去训练，可能就意思不是很有意思了。这个事情其实在2022年时候有一篇《Nature》行为的文章，也就讲深度学习做这件事情其实现在是做不到的，比较乏力。去仔细想他的这个问题是在我们觉得在大模型今天非常强大的当天，今天其实很多时候是卡在数据上面。这里面就是如何大规模的获得这种带有这种操作常识的数据，是一件很难的事情。
就比如上面我们知道，这是可以翻开的，这可以拉的，这是一件很难的事情。那么我们就去创一个什么思路呢？这个思路就是说我们发现了这样的一个问题，就是说我们发现原来这个手的操作和这个物体的操作常识它存在一种对偶关系。我们验证了说我们通过手的操作，我们就能推出这样的一个物体是怎么样的一种操作知识，操作知识我们就能够去生成这个手的操作这种对偶关系。那么发现了对偶关系有什么好处呢？首先这是可以这是最自然的数据。
而且是准确的，可以被规模化的。这就是说，比起你在那个3D里面去标，它是更加准确的对规模化的。为此的话，我们就去构建了一系列的算法去推断这些事情。因为时间有限，我就不讲了。总之，这样我们就将怎么去做手部重建以及手的在里面提取物体的知识。这是一些提取出来的一些操作知识的结果。我们这里因此还构建了一个一系列的一个系统。这个系统的话是说怎么样快速的去通过手的操作上面是视频。
下面是对手的一个分析。手的分析，我们能够去知道他在做一个什么概念——这个物体应该是被怎么操作的？那用方法能快速地制备海量的数据。那有这样数据的话，我们发现这个模型的准确率就会被大大提升。也会这样的通用的将来——这个通用的这样的一个世界模型的困难的world model，就是我们看到什么事情。
我知道怎么去操作它，这样的是带来一个非常好的基础。当然，说基建的大数据之后，我们就是会去训练这样的一个物体知识的模型。就比如说，像这些...
日本也得到了一个很大的提高：好。
因为时间有限，所以我们要抓紧每一分钟进行高效的工作。
我跳到下一部分，其实大家还是能想到柔性物体怎么办？对吧？缸体比较简单。柔性物体的话，其实我们也可以用这种方法，但是我们是在VR/AR眼镜里面，我们配合上仿真引擎，比如叠衣服这样的过程中，其实我们是操作这个叠的知识被记录下来。当然，这里面还有一个环节是我们衣服操作完了，我们要跟真机进行仿真对齐。我们是这种在仿真里面训的，但也是用手的操作记录他的操作知识，最后我们要跟真机对齐，可能需要这么一步。
这是唯一不一样的，为此的话我们就能够做到了。应该我认为第一个实际上，第一个能够去做任意物体的操作，因为衣服它那难点是跟比如去抓个东西、霍拉的东西不一样，它是它整个状态是不确定的，并且你看刚才我们扔进去的衣服是状态不确定的。那么相比于像之前的方法，他们是首先衣服要铺的比较好，并且它别的它是一个纯色的，它要叠起来的话需要很多的很很很多那个就是相对来说是比较简单的。我们是能做复杂的，我们有了这样的一个知识之后，我们可以干什么事情呢？我们可以做交互感知的，比如说我们现在从能改造这个微波炉，是可以这样拉的，但是说我们改造微波炉之后，我们发现还是有问题的，就是他感知可能不准，那么我们就引入了交互去纠正他的感知。那么怎么做呢？因为我们去拉的时候，我们能够预判他下一步应该怎么样子，因为我们有这个事件的模型，对吧，我们可以估计，那么我们就可以真实的对比，像这个是我们估计的，是我们的真实的是，然后我们去最小化它的误差，最小化它误差就是逼着什么？逼这个模型必须估计。对你估计不对的话，你、你这个事情就会出错，通过这个loss之后，我们就能够看到这个很有趣的事情，就是我们从开始这个事件的模型有一点点错误，通过操作过程中引入这个操作者的纠正，然后我们就能够把这个误差大量的大、大量的、大量的下降，在此的话我们有另一个工作，我们可以把这东西放到更难的比如穿针上面，并且我们这个东西不只是对于这个真的模型，并且我们会带上那个观测的机械臂。这个观测机械臂的话可以去看到说去进行不停的对他的纠正，这也是获得今年的国际机器人品会RS的最佳系统论文提名，是唯一的华人单位。
下面我们就有了感知到，基本知道这个世界什么样子。我们脑子里肯定过一遍了。那为什么这件事情那么重要呢？因为我们脑子里的话，仿真把这个物理的约束加进去的。所以，说我们就会有个科学问题：就是如何把物理知识的约束去降低，在开发环境下的在学习这个机器人的话。
这是一个全新的问题。
为什么呢？因为，它要求又准又快。这是一个很复杂的问题。当然，仿真引擎大家可能知道的多了。有游戏仿真引擎，它快但物理上不准；工业引擎它准，但它不够快。因为我们机器学习是需要机器学习要高速地和那个仿真引擎进行交互的。因此，我们对这个数学物理方程进行了重写，并且做了很多软件工程的工作。
之前他们要求快，他们是为了要能把各个模态进行独立的处理。而我们建立了联合方程，还有很多细节我就不讲了。
包括你看，如果他们不这么做，他们会穿模。那我们就能够非常好的，在这种多态中能够取得非常好的仿真效果。我们的实验也表明了，我们的速度能提升四百多倍，跟最好的工业软件相比，我们的误差在一毫米内。就是能够支持高水平的物理仿真和同时能够快速响应这样的一个机器人仿真系统、机器人的真机系统。这时候你可以看到，是我们我们跟力学这这是我们的那个用仿真的，这是真机的力学。我们测出来的这个误差是相对比较小的，也是在里面有很多数学物理化物理方程来在支撑说我们对柔性物体的仿真，在底层上去重写了它的数学物理方程，还有这样子的输液等等。
还有那个水流，这些都是一些非常难度的这些仿真。但是关键它速度需要快，这是还有它的渗透系统，就是相当于是把物理的那种规律嵌到你的学习系统里面，导致它的准确率会更高。好，我们是相比于国际上的这几个仿真引擎，包括STANDS，还有那个MIT。
我们有以下优势：从公开发表的结果来看，有以下的...
这个我们是开源的，就是LogoFlow，大家可以直接搜索一下。已经有康奈尔大学等多家单位在使用，并发表了相关的论文。
那么，有了这个事情的话，我们能干些什么很有趣的事情呢？我们有一个非常好的物理仿真器，必须快跟人工智能系统高速地一起，并且通过协助学习规划之后，我们就得到一个真实场景。我们脑子里其实有一个仿真引擎，对吧？我们仿真这种物理参数，但是物理参数我们不可能去测量的，但是我们可以去仿真的物理参数。如果你仿真错了，你跟真实情况是有一个误差的，我们去约束这个误差，逼着他这个物理参数必须得对，才能够跟甄姬跟真实场景一致，这就逼着他去估计这个真实的物理参数。像这个叠衣服包括这么多量，我们就是等于说整个系统会高速地跑起来，仿真和深度学习联合的好起来，我们就可以看到说，下面是真机，下面是仿真，这种柔性物体其实钢铁还比较好弄，柔性物体是非常难的，我们能够不能说分毫不差，但是很接近，导致我们的行为能比较准确。这是我们估计出来的各种物理参数和真实的比较还是很接近的。
当然，说这个事情的话，当我们进一步有这仿真引擎，同时可能也会支持这个海量的视频去学习那个实践的行为，去实践那个行为。但是，为此我们还构建了从怎么样从演示视频和仿真系统对齐的一个系统，然后最后听到甄姬。
因为时间有限，所以我们要抓紧每一分每一秒来完成任务。
我这就不再讲了，这也获得了 I Lost 的 ISS 的，不是那个 "I" 那个 i Lost 最佳论文机。这是我们的整个拍拍视频理解对接到仿真，然后最后到真机，把刚才那整套把它串起来好。
最后是居身执行，这件事情其实是一个大家可能没感觉到但我们觉得很疼的一个问题。就是你看现在来说，为什么我们觉得大脑规划完就结束了？其实不是我们实现下来，是我们人为什么觉得大脑就已经搞完就结束？是因为我们有个神经下意识反应，就是我们一个大小脑和这个神经系统使得我们神经系统使得我们能够去下意识去做很多东西。但是机器人如何去实现这种鲁棒的下意识，是很关键的，因为机器人存在感知误差、仿真误差还有机械误差，其实人也是一样的。其实大家有没有觉得我们的感知其实是有误差的？就是比如说我们去感知的东西，我们的脑子里其实这么想，但其实我们闭上眼睛的时候其实很大的误差。那么这个时候就是这个误差怎么用人工智能把它吃掉？也就是说为什么能吃掉？是因为我们刚才有个规划，其实规划里面有底层，其实有一个又一个的那个子的原子的操作。如果您原子操作能够被通用化，被这个人工智能吃掉的话，那么我们就有可能去做成这样的一个事情。这为什么能做到呢？是因为其实他和同一个动作他在拓扑上同源的，比如这个翻的动作，其实之后会拓扑同学到这个东西不周转这个事情，所以有没有可能性？
为此我们会去解决这样的一个操作的问题。我们第一步是要解决通用的抓取问题，通用的抓取问题它的最大的问题是数据困境，还是回到了数据问题，就是说我们有这么多的便于，那我们怎么变成这些抓取的密集数据？之前是没办法去制备这种大规模的数据的。因此我们提出了一套半自动的数据采集方式，我们的整个数据效率提高了，标准提高了1万倍，使得我们能够快速的大规模的制备大量的数据，不然的话很长时间。基本原理是我们通过这是真实点名，然后我们有一套数字完成系统，能够去通过这个数字完成到这里，然后我们可以生成它的抓取的在用仿真甚至来抓取的点云，然后就可以在真实的点云上打上很多标签。但是整套系统是里面欠了很多算法和半自动的这样的系统。为此我们能够大大的扩大了，是比目前数据集这21个数据有效数据能够有效的扩张到10万倍这样的数据，并且标注的效率会大大提高。也配套提出了新的大模型的抓取大模型的算法，这里就不展开来讲。为此的话我们就会因此我们得到了一个通用的抓取的模型，通过抓取模型，我们跟别的抓取的不一样，我们没见过的时候我们都能够去抓。比如我们刚才这个东西被敲碎，这个瓷器被敲碎，瞬间其实每一个块都是没有见过的，但没有见过我们仍然能够把它抓起来。我们在5000个没有见过完全没有见过的物体上面能够被通用的抓取，包括我们也扩张到了那个去动态的物体。这是世界上第一个能够抓通用的，没有通过见过那个物体的抓鱼，就是抓那个没有见过的动态的物体。这个也发表在那个机器人的底盘TO上面，我们也是首次去跟人类比较，超过人类水平从准确率和速度上面对没有见过的物体的抓取，超人类水平也被斯坦福的机器人抓取排行榜被被评为近十年影响力第二名。好
最后，讲了一个以立学为中心的框架。我们会思考一个问题：就是说，我们的话，就是说，我们之前谷歌不是提出了一种大模型吗？我们可以反思一下我们的大模型——他是视觉输入，然后通过大模型去得到一个位置决策。那这种事情有什么问题呢？
看起来是没问题的，但是你们有没有发觉他们就是更多的做移动，把这个东西移到了另个地方，就没有做那种rich content，是那个那个rich contact，就是那种很丰富的接触。那么，如果这件事情需要突破很复杂的接触的问题，那么需要以力觉为中心的AI框架，就是位置引导下的力反馈的一个输出。因此构建了这样的力觉的一个大模型去做这样的事情。

我们来看看有一件事情就是位置控制的一定是做不了的。举个例子，如果我们有位置控制，你只要往下挫1毫米就爆掉了；往上一毫米就刮不干净。所以这里面我们的模型的输出是用大模型输出的，它的力的按压力是多少以及它切向力是多少。但是可能大概有一个位置的一个引导。

因此，如果未来用到了家用医疗等等，是需要一个非常好的力反馈的一个力学大模型来做这样的事情。这也是我们一直在希望在做这样的一个过程。包括我们的能够做到非常精密的那种这种力的控制，就是能做到他这样。那其实也是说会在你的力觉参数和这个位置引导是由模型来输出。

这就是说以力觉为中心会是将来去解决小脑这部分很关键的一个点。为此的话我们也是开源了线上的最大的目前最大的以力觉为中心的数据大家欢迎下载。我们的数据集量是GTP数据量的2倍，我们拥有力觉、听觉、语言控制、视觉等等这样的一个事情。

我们的力的操作平台是比较特殊的，我们能够（抱歉，此处内容不完整或有误）。
这里，其实是一张关于视频的描述。这原本是用来说明我们的切口机——即人类在窑操作切火机时，能够记录大量底层的力反馈数据的工具。当时，我们使用的位置为中心的模仿学习数据库。
我们也参与了，我们是大概有40个学校，然后我们也是唯一的国内单位，参与了最大规模的一个这种 open X-body 的，这就是基于位置仿真的一个位置基于位置规划的这样的一个数据。
也欢迎大家来使用。好，那么就是呃，我们这是我们的综合的系统的结果，这是我们综合的系统的结果。也就是说，抱歉。
就是我们把整体的各种技术整合进去，做了一套综合的机器人，比如做早餐、清洁。
你看，这个擦东西其实，我们为什么要插入？因为插入需要力倦，因为力卷你擦不干净。所以，我们需要那个力的模型，包括我们要去把服务拿进去。这里面会整合到我们的柔性物体，如果**没有很好的**，刚才我们讲的钢铁还是比较容易的。当你柔性物体知道哪个是衣领的时候，它的难度就会大大的增加。
好啊，这是当然的。我们也把我们的很多技术开源到了我们的 LoveFlow。我们在上面有一个生态 Toria Document，还有论坛和 Deut 等等。这些连接可以连接 14 种不同的机器人。也希望大家能够多多支持我们。
当然，最后我们时间有限。我来讲讲我们是如何研究大脑和身体之间的关系的——这是基因智能与行为之间的一个非常关键的事情。时间到了。
好，那刚刚好，最后是我们这些人做的这些论文。就包括我们主要还在除了人工智能的，还有机器人学的两大顶刊上面，以及获得的一些最佳论文，分别在各个会议和机器顶会上获得了最佳论文奖。好。
我可能怕时间不够，赶得太快。最后讲一句感慨一下：就是通用之前是人工智能的终极状态。我们**认为计算智能是个灵魂**，也就是很有趣的一个领域。希望大**家能够也能够一起来做**这么一件让人类更加美好的事情。然后再次推荐大家看这部电影——我每次看的时候觉得你的动作我们怎么把它做成，也是我们可以做成的范围内。好
谢谢大家。
