1
00:00:00,030 --> 00:00:01,966
是哈尔滨工业大学张伟楠
2
00:00:01,970 --> 00:00:04,940
目前担任CCF术语审定工作委员会执委
3
00:00:05,360 --> 00:00:06,552
哈尔滨分部秘书长
4
00:00:07,220 --> 00:00:08,881
哈工大计算学部主任助理
5
00:00:08,890 --> 00:00:12,248
也是黑龙江省中文信息重点处理重点实验室的副主任
6
00:00:13,180 --> 00:00:21,408
CCF术语工委将在周一晚间CCF视频号直播间邀请术语专家讲解术语热词
7
00:00:21,920 --> 00:00:24,373
并围绕术语展开相关讨论
8
00:00:24,380 --> 00:00:25,724
在此也提醒朋友们
9
00:00:25,730 --> 00:00:30,014
一定点击左上角图标关注CCF公众号和视频号
10
00:00:30,200 --> 00:00:33,320
获取最新的直播信息并观看每期回放
11
00:00:34,000 --> 00:00:38,776
今天我们非常高兴邀请到具身智能领域的三位重磅专家
12
00:00:39,090 --> 00:00:44,658
他们在机器人具身智能和大模型应用方向做出了很多开创性的探索和研究
13
00:00:45,170 --> 00:00:47,370
欢迎三位老师来到直播间
14
00:00:47,380 --> 00:00:52,970
与我们共话解读具身智能及大模型相关术语概念和专业热点
15
00:00:53,860 --> 00:00:57,920
三位老师分别是CCF智能机器人专委会主任
16
00:00:58,160 --> 00:01:01,860
清华大学计算机科学与技术系教授、博士生导师
17
00:01:02,170 --> 00:01:05,331
i triple ECAAICAA fellow孙富春老师
18
00:01:06,280 --> 00:01:07,960
孙老师可以跟大家打个招呼
19
00:01:07,970 --> 00:01:08,810
大家好
20
00:01:10,740 --> 00:01:14,040
第二位嘉宾是CCF智能机器人专委会副主任
21
00:01:14,300 --> 00:01:18,140
复旦大学智能机器人研究院副院长张文强老师
22
00:01:18,580 --> 00:01:20,130
请张老师和大家打个招呼
23
00:01:22,270 --> 00:01:22,640
好
24
00:01:22,640 --> 00:01:27,436
然后第三位是上海交通大学博士生导师卢策吴老师
25
00:01:27,860 --> 00:01:31,103
也是非常感谢三位老师做客CCF talk直播间
26
00:01:32,100 --> 00:01:36,258
今天的活动我们聚焦聚身智能及大模型概念的解读
27
00:01:36,680 --> 00:01:40,880
因为近期大模型研究的广泛开展和其表现出的良好的性能
28
00:01:41,350 --> 00:01:47,678
学术界和工业界也将目光关注到自身智能和在机器人领域的应用
29
00:01:48,080 --> 00:01:50,834
所以我们这一期其实涉及的是两个术语
30
00:01:50,960 --> 00:01:52,408
具身智能和大模型
31
00:01:52,740 --> 00:01:54,898
或者如果我们进一步限定的话
32
00:01:55,100 --> 00:01:58,075
我们可以把这个概念概括成巨身大模型
33
00:01:58,760 --> 00:02:01,565
这就是本期术语访谈的一个主题的背景
34
00:02:02,170 --> 00:02:07,216
接下来我们先请三位专家围绕具身智能及大模型的概念理论和应用
35
00:02:07,950 --> 00:02:09,374
为我们做引导报告
36
00:02:09,840 --> 00:02:16,020
首先我们邀请孙富春老师为我们做巨深大模型与3C装配应用的报告
37
00:02:16,990 --> 00:02:17,190
好
38
00:02:17,190 --> 00:02:18,300
欢迎孙老师
39
00:02:18,300 --> 00:02:18,560
好的
40
00:02:18,560 --> 00:02:19,460
谢谢主持人
41
00:02:19,790 --> 00:02:23,528
也感谢CCF术术语工委的邀请
42
00:02:24,410 --> 00:02:29,985
我今天跟大家分享的题目是巨深大模型与3C装配的应用
43
00:02:31,000 --> 00:02:32,785
正如主持人所说
44
00:02:33,260 --> 00:02:37,442
如果说现在我们在AI领域哪个最火呢
45
00:02:38,020 --> 00:02:38,960
一个是俱生
46
00:02:39,580 --> 00:02:40,938
一个就是大模型
47
00:02:41,740 --> 00:02:47,909
其身俱生这个词最早来自于1963年麻省理工学院的h heard
48
00:02:50,470 --> 00:02:53,026
他给了巨身一个重要的定义
49
00:02:53,030 --> 00:02:55,043
他当时举了一个叫被动猫
50
00:02:55,050 --> 00:02:57,420
就是有只有局部的特征
51
00:02:57,890 --> 00:03:03,650
一个是主动猫能够拥有的沉浸感和作用行为的这种特征
52
00:03:04,170 --> 00:03:06,809
最后这个主动猫它学会了行走
53
00:03:07,760 --> 00:03:08,824
而这个概念的话
54
00:03:09,250 --> 00:03:14,569
实际上所谓的据称他讲的就是给机器一个物理的身体或者形式
55
00:03:14,580 --> 00:03:17,208
来与物理事件的它能够交互
56
00:03:17,940 --> 00:03:21,748
由此我们想到了人工智能发展里的第一范式、第二范式
57
00:03:22,080 --> 00:03:25,059
符号主义和联结主义
58
00:03:26,050 --> 00:03:34,350
这些的话都没有完全用到巨生所说的根物理世界产生交互
59
00:03:34,510 --> 00:03:35,690
具有沉浸感
60
00:03:36,020 --> 00:03:38,570
包括反作用的这种行为
61
00:03:38,910 --> 00:03:47,679
所以沉浸感和反作用的行为就成为了第一物体是不是具有自身的两个非常重要的特征
62
00:03:48,680 --> 00:03:52,200
而具身的具体体验既包括内在的作用
63
00:03:52,370 --> 00:04:01,078
也被包括外在的这种具有沉浸式和反作用式的这种感知所具有的
64
00:04:01,099 --> 00:04:03,844
所以我们把它叫矩身
65
00:04:03,848 --> 00:04:04,123
好
66
00:04:04,123 --> 00:04:09,614
其实大模型第一次用在机器人的多任务当中
67
00:04:09,896 --> 00:04:13,458
实际上就是微软做了这个工作
68
00:04:13,470 --> 00:04:14,018
做GPT
69
00:04:14,020 --> 00:04:16,276
And have even non technical users to be able to 
70
00:04:16,276 --> 00:04:18,983
work with robots, primarily by interacting with 
71
00:04:18,983 --> 00:04:22,055
the model through natural language instructions 
72
00:04:22,055 --> 00:04:22,503
and you. 
73
00:04:23,020 --> 00:04:23,560
好的
74
00:04:23,890 --> 00:04:33,625
这个工作实际上他就是第一次的把大模型用在机器人的多任务作业过程当中
75
00:04:34,010 --> 00:04:36,236
其实我们在座的很多人搞机器人
76
00:04:36,630 --> 00:04:38,390
我们过去的机器人的操作
77
00:04:38,500 --> 00:04:40,280
我们叫就事论事的操作
78
00:04:40,640 --> 00:04:43,535
所谓的就事论事就是针对一个场景
79
00:04:43,540 --> 00:04:45,010
针对的一个具体的任务
80
00:04:45,230 --> 00:04:49,598
如何通过工程师设计相关的软件
81
00:04:49,910 --> 00:04:52,278
包括像任务规划、轨迹规划
82
00:04:52,780 --> 00:04:54,800
包括操作与控制的算法
83
00:04:55,680 --> 00:04:59,216
但是如何把这种工作用到多任务当中呢
84
00:05:00,420 --> 00:05:03,440
赛康是2022年发表的一篇非常有趣的工作
85
00:05:04,070 --> 00:05:07,870
这个里面他谈到了如何通过我们叫提示学习
86
00:05:08,660 --> 00:05:12,466
包括通过我们叫指令学习
87
00:05:12,820 --> 00:05:18,210
最后找到知识库当中相关的机器人操作的一种技能
88
00:05:18,730 --> 00:05:21,408
然后实现这样一个具体的操作
89
00:05:22,210 --> 00:05:24,548
我们首先看一下他其实是这样的
90
00:05:24,800 --> 00:05:27,738
他如何把一个苹果放在桌子上
91
00:05:29,340 --> 00:05:30,564
他就通过指令学习
92
00:05:30,570 --> 00:05:36,849
在大模型当中找说如果要完成把苹果放在桌子上
93
00:05:37,750 --> 00:05:39,010
他需要这么几个技能
94
00:05:39,500 --> 00:05:41,735
第一个你首先要找到这个苹果在哪
95
00:05:42,510 --> 00:05:44,694
第二个你要抓住这个苹果的技能
96
00:05:45,150 --> 00:05:47,339
第三个才是放置这个苹果
97
00:05:47,770 --> 00:05:48,180
好了
98
00:05:48,770 --> 00:05:50,746
那么这个过程里面需要哪些呢
99
00:05:51,140 --> 00:05:53,184
其实我们能做事情是怎么做的呢
100
00:05:53,190 --> 00:05:56,865
我们后面要慢慢的从自身的角度给大家做来解读
101
00:05:57,070 --> 00:05:57,450
好
102
00:05:58,130 --> 00:05:59,990
我们首先看看今天的大模型
103
00:06:00,890 --> 00:06:02,710
大模型用在机器人当中
104
00:06:03,230 --> 00:06:08,366
我们说机器人他有任务层、规划层和操作层
105
00:06:08,790 --> 00:06:09,784
我们大家都说的
106
00:06:10,240 --> 00:06:13,960
但是现在的对话大模型究竟能用在哪些地方呢
107
00:06:13,960 --> 00:06:19,367
我们看到说通过语言、通过识别、通过自然处理得到标准的指令
108
00:06:21,050 --> 00:06:22,150
主任务规划
109
00:06:22,480 --> 00:06:24,075
像CM就是这样一个例子
110
00:06:24,530 --> 00:06:27,660
但是具体的像规划控制
111
00:06:28,000 --> 00:06:30,958
可能我们经常讲说我们还需要具体场景
112
00:06:31,400 --> 00:06:33,948
我们也特别需要操作有哪些目标
113
00:06:35,530 --> 00:06:38,298
那么这种操作控制应该怎么去做它呢
114
00:06:38,730 --> 00:06:39,050
好
115
00:06:39,050 --> 00:06:40,266
因为我们有发现说
116
00:06:40,590 --> 00:06:44,910
如果说一个移动车在行进的道路上碰到障碍了
117
00:06:46,290 --> 00:06:49,125
那么他就要去什么修改任务规划器
118
00:06:49,380 --> 00:06:51,040
一方面通过语音的形式
119
00:06:51,180 --> 00:06:56,400
另一方面可能需要应急的修改任务规划器
120
00:06:57,110 --> 00:06:58,806
比如说车开的路上
121
00:06:58,810 --> 00:06:59,917
突然一个小孩过马路
122
00:07:00,160 --> 00:07:02,452
像这个姑姑再通过自然语言
123
00:07:02,630 --> 00:07:04,310
再通过任务规划器的话
124
00:07:04,410 --> 00:07:05,290
可能就很慢
125
00:07:05,680 --> 00:07:09,820
我们可能直接通过传感器系统让任务规划器改变规划
126
00:07:10,670 --> 00:07:11,010
好
127
00:07:11,810 --> 00:07:15,605
那么我们现在从机器人如何出具身
128
00:07:15,860 --> 00:07:18,548
我们来分析一下应该做哪些事情
129
00:07:19,140 --> 00:07:21,792
其实我们都说机器人上有大量的传感器
130
00:07:22,300 --> 00:07:24,071
这些传感器有的是具身的
131
00:07:24,280 --> 00:07:26,513
就在机器人的机械臂上面
132
00:07:26,520 --> 00:07:28,140
比如说我们说手眼视觉
133
00:07:28,670 --> 00:07:29,776
就在机械臂上的
134
00:07:30,070 --> 00:07:31,860
那么机器人手上的触觉
135
00:07:32,640 --> 00:07:34,537
包括语音的交互
136
00:07:35,280 --> 00:07:37,976
包括声音等等这些
137
00:07:38,630 --> 00:07:43,760
这些传感器他们是在一个观测的数据空间
138
00:07:44,870 --> 00:07:46,410
过去我们大家做过视觉
139
00:07:46,410 --> 00:07:47,110
做过触觉
140
00:07:47,110 --> 00:07:50,416
首先把单模的这些信息要得到它的特征空间
141
00:07:50,920 --> 00:07:53,606
但是今天我们遇到的恰恰是多模态问题
142
00:07:54,640 --> 00:07:58,856
我如何把多模态的数据空间到特征空间
143
00:07:59,110 --> 00:07:59,550
好了
144
00:07:59,900 --> 00:08:00,988
我们大家都很清楚
145
00:08:00,990 --> 00:08:02,550
我们能做认知的话
146
00:08:02,980 --> 00:08:04,933
需要通过知识去推理
147
00:08:06,140 --> 00:08:08,170
我们首先具有像常识库
148
00:08:08,720 --> 00:08:11,398
还有机器人在特定领域的这些
149
00:08:11,410 --> 00:08:15,130
我们叫专业具有专业背景的专业库
150
00:08:16,210 --> 00:08:17,246
然后我们要推理
151
00:08:18,250 --> 00:08:22,807
我们就特别需要说我们通过传感器得到这些信息
152
00:08:22,950 --> 00:08:24,420
从数据到特征
153
00:08:24,710 --> 00:08:26,618
我们是不是也需要得到知识
154
00:08:27,690 --> 00:08:30,138
这里面就有AI最新研究的比较重要的
155
00:08:30,140 --> 00:08:33,100
比如说我们可以通过神经符号解析器
156
00:08:33,540 --> 00:08:35,190
从特征空间到概念空间
157
00:08:35,480 --> 00:08:38,120
我们可以通过知识推理再到知识空间
158
00:08:40,260 --> 00:08:45,542
知识库根据感知情况得到的知识做行为规划
159
00:08:46,530 --> 00:08:51,290
而这种行为规划最后要通过底层的学习
160
00:08:51,450 --> 00:08:53,984
或者是搞自动化领域的底层控制
161
00:08:54,180 --> 00:08:58,171
来实现机器人达到期望的状态
162
00:08:58,480 --> 00:09:01,400
我们如果发现实际状态和期望状态之间有偏差
163
00:09:01,970 --> 00:09:04,089
我们还要一方面要修改知识库
164
00:09:04,320 --> 00:09:08,604
另一方面要通过底层的学习控制来解决
165
00:09:09,570 --> 00:09:14,045
所以这里面的话我们看到了在具身智能里面很重要的一点
166
00:09:14,050 --> 00:09:16,283
就是实际系统的反馈里面
167
00:09:16,430 --> 00:09:20,765
一方面要通过底层学习或者控制来实现
168
00:09:20,990 --> 00:09:24,574
另一方面我们还要至上要修改知识库
169
00:09:24,580 --> 00:09:25,644
就像我们人一样
170
00:09:25,830 --> 00:09:26,640
遇到了挫折
171
00:09:26,640 --> 00:09:28,344
发现以前以往的知识不对了
172
00:09:28,350 --> 00:09:29,540
我还要回去修改
173
00:09:30,010 --> 00:09:30,460
好
174
00:09:31,690 --> 00:09:34,210
我们想从三个方面来讲如何做提升
175
00:09:34,680 --> 00:09:39,740
第一个方面刚才谈到了说从数据空间要到知识空间
176
00:09:40,800 --> 00:09:43,400
大家讲说每一个机器人它工作的任务也不一样
177
00:09:44,170 --> 00:09:45,690
传感器的设置也不一样
178
00:09:46,640 --> 00:09:51,560
我们是不是在数学上要研究一个把在不同空间里的东西
179
00:09:51,580 --> 00:09:53,478
把它投影到一个浅空间里去呢
180
00:09:54,900 --> 00:09:56,693
就像我们过去说的接电式
181
00:09:57,520 --> 00:09:59,914
过去传感器是放在惯导平台上的
182
00:10:00,700 --> 00:10:05,622
现在是放在机飞机的或者说是机器人的任意的位置上
183
00:10:05,820 --> 00:10:08,298
那我需要构造一个什么映射关系
184
00:10:08,470 --> 00:10:10,780
把它要投影到一个平台上
185
00:10:11,490 --> 00:10:13,698
其实我们要做巨深智能何尝不是这样
186
00:10:14,050 --> 00:10:16,090
我们首先要把传感器的数据
187
00:10:16,270 --> 00:10:17,782
就像我们接电视一样
188
00:10:18,060 --> 00:10:20,388
把它投影到一个数学平台上
189
00:10:21,290 --> 00:10:22,540
在这个数学平台上的话
190
00:10:22,750 --> 00:10:26,395
我可以把它从数据库到概念到知识
191
00:10:27,750 --> 00:10:31,072
这里当然我们也研究过一些所谓的感知模组来实现
192
00:10:31,560 --> 00:10:36,411
我觉得第二个很重要的就是人为什么能做到最深
193
00:10:36,760 --> 00:10:38,630
人为什么能够做多任务
194
00:10:39,230 --> 00:10:42,346
我们有很多人每一个任务做的都那么的精确
195
00:10:42,970 --> 00:10:44,230
那么他靠的是什么呢
196
00:10:44,700 --> 00:10:45,980
靠的是技能
197
00:10:47,430 --> 00:10:50,609
你比如说像我们一生当中都经历过很多
198
00:10:50,830 --> 00:10:51,928
你比如说我们踢足球
199
00:10:53,610 --> 00:10:55,598
那么体育老师教我们说怎么运球
200
00:10:56,160 --> 00:10:58,400
我们叫论之结算做什么
201
00:10:58,910 --> 00:11:02,837
第二个我们要不不断的精炼我们运球的这种能力
202
00:11:04,390 --> 00:11:05,990
包括我们要从书本
203
00:11:06,290 --> 00:11:10,742
包括跟通过不断的练习来实现这样一个经验过程
204
00:11:11,020 --> 00:11:13,096
最后会达到一个很高的境界
205
00:11:13,100 --> 00:11:14,070
叫自主阶段
206
00:11:14,950 --> 00:11:16,118
其实人的技能的话
207
00:11:16,230 --> 00:11:18,810
不光包括了我们踢球这种运动技能
208
00:11:19,290 --> 00:11:22,265
其实我们还包括了知觉智能和认知智能
209
00:11:22,860 --> 00:11:29,054
而很多事情都是这些技能的不同技能的组合
210
00:11:29,790 --> 00:11:31,572
那么如何构建这些模型呢
211
00:11:31,580 --> 00:11:32,777
像雷一样去做呢
212
00:11:32,910 --> 00:11:35,998
我想是实现具身智能非常重要的部分
213
00:11:36,760 --> 00:11:37,408
第三个
214
00:11:37,720 --> 00:11:38,200
油画
215
00:11:39,540 --> 00:11:41,010
再好的方法
216
00:11:41,780 --> 00:11:43,769
包括我们今天讲到的虚实迁移
217
00:11:45,900 --> 00:11:48,229
精炼以后的算法如果放在实际系统里面
218
00:11:48,240 --> 00:11:51,691
它可能会遇到干了摩擦各种不确定因素
219
00:11:51,900 --> 00:11:53,825
我们还需要什么怎么样用
220
00:11:53,830 --> 00:11:55,050
叫first shop learning
221
00:11:55,580 --> 00:11:57,636
我们叫少样本学习
222
00:11:58,250 --> 00:11:59,496
就能够实现什么
223
00:11:59,580 --> 00:12:01,666
在特定场景里面达到最佳的效果
224
00:12:01,950 --> 00:12:04,251
所以自身优化也是比较重要的
225
00:12:04,260 --> 00:12:04,720
好了
226
00:12:05,080 --> 00:12:06,530
下面我就简单介绍一下
227
00:12:06,530 --> 00:12:10,170
第一个方面就是微软的这篇文章
228
00:12:10,510 --> 00:12:12,171
他介绍的多模型怎么做的
229
00:12:12,630 --> 00:12:15,474
第一个就跟我刚才谈到的非常重要的一样
230
00:12:15,770 --> 00:12:22,545
就是我要构建一个机器人做不同任务它所需要的各种技能
231
00:12:22,940 --> 00:12:24,796
就是要建立技能库
232
00:12:25,830 --> 00:12:26,958
我们人也是这样的
233
00:12:27,380 --> 00:12:29,340
我们人为什么能够实现多任务呢
234
00:12:29,750 --> 00:12:33,968
就是把不同技能在空间之间组合形成多任务
235
00:12:35,480 --> 00:12:36,425
第二个部分的话
236
00:12:36,990 --> 00:12:38,238
有了这些以后
237
00:12:38,860 --> 00:12:42,748
我们就可以通过提示学习刚才讲的把苹果放在什么上做提示词
238
00:12:43,040 --> 00:12:45,364
去在数据库当中
239
00:12:45,430 --> 00:12:50,596
在大模型当中找实现这样一个东西需要哪些技能
240
00:12:51,760 --> 00:12:52,080
好
241
00:12:52,410 --> 00:12:54,692
第三个大家讲到了说这个方法好
242
00:12:54,700 --> 00:12:55,729
一会用户要去用
243
00:12:56,310 --> 00:12:58,299
用的过程当中会有很多的反馈
244
00:12:58,820 --> 00:13:00,910
反馈也帮助我们修改知识
245
00:13:01,250 --> 00:13:05,705
同时精炼技能库中的参数
246
00:13:05,710 --> 00:13:08,986
我们叫属性参数这样一个过程
247
00:13:09,200 --> 00:13:09,540
好
248
00:13:10,020 --> 00:13:11,068
第一个过程就来了
249
00:13:11,070 --> 00:13:15,090
就是我们如何做到对机器人操作场景的感知呢
250
00:13:16,080 --> 00:13:17,790
你比如说是我们团队做的一个工作
251
00:13:18,420 --> 00:13:22,732
我们通过固定试点的这种传感器
252
00:13:23,070 --> 00:13:24,710
你还需要移动试点
253
00:13:24,710 --> 00:13:26,294
你比如说有的地方被遮挡了
254
00:13:26,520 --> 00:13:28,577
你看这个桌子后面我们就不知道在哪里
255
00:13:28,970 --> 00:13:31,630
我们通过一个移动机器人加上一个传感器过去
256
00:13:31,930 --> 00:13:35,566
我们就能实现对整个场景的这种感知理解
257
00:13:35,980 --> 00:13:38,326
最后做到我们叫eb caption
258
00:13:38,730 --> 00:13:43,250
能够把它的物理意义这个场景精确的把它描述出来
259
00:13:44,740 --> 00:13:47,400
这个工作也是我们团队做的一个很重要的工作
260
00:13:47,730 --> 00:13:53,040
就是如何的操作场景做分割和重建
261
00:13:54,130 --> 00:13:58,158
其实大家知道我们要重建可能需要各种特征
262
00:13:58,160 --> 00:13:59,600
你比如说场景重建
263
00:13:59,790 --> 00:14:03,540
我需要边缘特征、形状特征、纹理特征
264
00:14:04,340 --> 00:14:05,918
还有颜色光流
265
00:14:06,480 --> 00:14:09,136
那么这些战争如何通过传感器去获取
266
00:14:10,230 --> 00:14:15,055
或者说我们如何判断哪些传感器对这些特征的表达能力呢
267
00:14:16,050 --> 00:14:18,336
所以这个也是我们团队最近做的一个工作
268
00:14:18,710 --> 00:14:22,998
我们能够通过叫正则化的吸收激活度
269
00:14:23,220 --> 00:14:30,948
我们就能够精确的计算出这个传感器的这个信息的母兮
270
00:14:31,210 --> 00:14:34,687
哪些特征比边缘特征的形状特征的表示能力
271
00:14:35,300 --> 00:14:40,015
我们这样的话就能把不同传感器对不同帧的表示特征
272
00:14:40,020 --> 00:14:42,022
最好的特征都把它选出来
273
00:14:42,520 --> 00:14:43,618
然后再做重建
274
00:14:43,990 --> 00:14:46,870
这个也是聚生智能里面我觉得比较重要的部分
275
00:14:47,390 --> 00:14:49,420
这个是我们最近在做的一个工作
276
00:14:50,680 --> 00:14:57,143
我们如何构建一个具有物理交互特性的数字孪生环境
277
00:14:57,460 --> 00:15:01,620
而这种环境它要体现自身的就是他要有沉浸感
278
00:15:02,560 --> 00:15:03,910
另外还有一个叫reaction
279
00:15:04,300 --> 00:15:05,168
叫反作用
280
00:15:05,920 --> 00:15:08,880
大家想说沉浸感和反作究竟怎么做呢
281
00:15:09,180 --> 00:15:10,034
我们第一个方面
282
00:15:10,150 --> 00:15:12,472
我觉得大家可能我们很多人在做love
283
00:15:13,250 --> 00:15:14,594
就是神经辐射场
284
00:15:15,130 --> 00:15:19,249
它可以对场景里的纹理、颜色、几何特性做建模
285
00:15:19,980 --> 00:15:22,360
第二部分的话就是触觉怎么建模
286
00:15:23,170 --> 00:15:24,674
我们做了两套工作
287
00:15:24,680 --> 00:15:26,975
一套工作我们曾经发表过一篇工作
288
00:15:27,200 --> 00:15:29,270
就通过粒子的交互性
289
00:15:29,590 --> 00:15:33,364
弹性粒子来自触觉进行多分辨率的建模
290
00:15:34,930 --> 00:15:35,718
让它重现
291
00:15:36,210 --> 00:15:39,185
第二个我们干脆就直接接到传感器上去
292
00:15:40,210 --> 00:15:46,690
通过机器人在不同任务、不同场景下的学习过程中的真正的触觉来进行建模
293
00:15:48,140 --> 00:15:51,140
同时把动力学也把它加进来
294
00:15:51,620 --> 00:15:54,068
我们最近也把听觉加进里面
295
00:15:54,310 --> 00:15:58,542
形成的话这样一个具有物理交互特性的数字孪生环境
296
00:15:58,920 --> 00:15:59,928
它既有物理特性
297
00:16:00,240 --> 00:16:01,188
也有触觉
298
00:16:01,400 --> 00:16:02,108
也有听觉
299
00:16:03,350 --> 00:16:03,620
好
300
00:16:03,970 --> 00:16:06,340
这个是我们一个示意图
301
00:16:06,350 --> 00:16:09,239
包括我们建立的各种知识库、技能库
302
00:16:09,720 --> 00:16:11,942
包括我们安装的一些场景
303
00:16:12,050 --> 00:16:13,760
比如说暖排线
304
00:16:15,030 --> 00:16:16,158
包括像USB
305
00:16:16,690 --> 00:16:19,370
包括这个旋钮等等
306
00:16:19,480 --> 00:16:22,931
包括这个你看我们也做些弹性体的仿真
307
00:16:24,950 --> 00:16:25,240
好
308
00:16:26,550 --> 00:16:28,748
下面我们重点介绍一下技能学习
309
00:16:29,870 --> 00:16:32,379
其实我们人之所以能做多任务
310
00:16:32,770 --> 00:16:35,110
就是我们能学会了各种技能
311
00:16:36,010 --> 00:16:39,937
那么这些技能在空间时间的组合就形成不同任务
312
00:16:40,580 --> 00:16:42,932
所以这里面大家就想说什么叫技能呢
313
00:16:43,180 --> 00:16:49,390
我想是通过训练或者是教巩固下来的自动化的完善的动作方式
314
00:16:49,960 --> 00:16:52,600
而这个机缘都是有一些动作机缘组成
315
00:16:52,850 --> 00:16:56,420
而这种动作机缘就是不可再分解的部分
316
00:16:58,070 --> 00:17:02,948
你比如说我们刚才讲到的把苹果放在桌上
317
00:17:03,600 --> 00:17:04,476
这是一个任务
318
00:17:05,630 --> 00:17:06,799
他就有三个技能
319
00:17:08,000 --> 00:17:10,673
分别是怎么发现苹果
320
00:17:11,670 --> 00:17:12,906
怎么抓握苹果
321
00:17:12,910 --> 00:17:14,480
怎么放置苹果三个技能
322
00:17:14,800 --> 00:17:15,800
而每一个技能的话
323
00:17:16,430 --> 00:17:18,020
大家看它也有机缘组成
324
00:17:18,430 --> 00:17:23,134
你比如说他要通过摄像机要搜索
325
00:17:24,190 --> 00:17:27,466
那么这些感知智能要搜索找到它
326
00:17:27,470 --> 00:17:28,900
这用的就是一个感知智能
327
00:17:29,580 --> 00:17:33,570
在每一帧图像你通过深度学习去检测苹果在哪里
328
00:17:34,310 --> 00:17:35,330
直到找到为止
329
00:17:35,800 --> 00:17:40,098
接下来的话击剑手就去夹住苹果
330
00:17:40,110 --> 00:17:40,920
然后把它握住
331
00:17:41,390 --> 00:17:42,489
用到了三个技能
332
00:17:43,860 --> 00:17:47,304
放置去把苹果放在那个地方
333
00:17:48,300 --> 00:17:50,106
在基建手再回来
334
00:17:50,330 --> 00:17:52,319
所以这里面也牵扯到三个技能
335
00:17:52,830 --> 00:17:53,730
三个机缘
336
00:17:54,380 --> 00:17:55,960
所以我们就把这样一个
337
00:17:56,640 --> 00:17:59,700
大家看到了一个任务用到了三个计
338
00:18:00,650 --> 00:18:04,393
每一个技能的话你都分解成不同的这种机缘
339
00:18:04,670 --> 00:18:09,566
而这些机缘的话都有属性参数去表征它
340
00:18:10,050 --> 00:18:11,910
这样就建立一个大型知识库
341
00:18:12,100 --> 00:18:14,868
形成映射关系好了
342
00:18:15,340 --> 00:18:18,485
有人讲说咱们建这个知识库容易不容易
343
00:18:18,950 --> 00:18:20,270
我说他又容易
344
00:18:20,270 --> 00:18:21,230
他又不容易
345
00:18:22,100 --> 00:18:24,063
有人就讲说我们能做任何事情
346
00:18:24,070 --> 00:18:25,029
做那么多的事儿
347
00:18:25,910 --> 00:18:28,766
过去像我们车间里面做装备也好
348
00:18:28,970 --> 00:18:30,220
修手表也好
349
00:18:30,790 --> 00:18:31,650
都是能做
350
00:18:32,240 --> 00:18:35,280
我们能把那些人工巧匠做的那些技能
351
00:18:35,470 --> 00:18:36,937
我能把它解析出来吗
352
00:18:37,300 --> 00:18:37,880
我说可以
353
00:18:38,780 --> 00:18:39,908
咱们可以通过视觉
354
00:18:40,350 --> 00:18:42,250
也可以通过触觉和听觉
355
00:18:42,920 --> 00:18:43,300
好
356
00:18:44,220 --> 00:18:44,900
在这里的话
357
00:18:44,990 --> 00:18:47,100
我们用到了四卷摄像机
358
00:18:47,660 --> 00:18:49,398
摄像机最大的好处什么呢
359
00:18:49,580 --> 00:18:51,758
能够看到动作的图谱
360
00:18:52,730 --> 00:18:56,345
而这个触觉和听觉主要是基于事件
361
00:18:56,790 --> 00:18:58,918
你比如说两个物体有没有接触上
362
00:19:00,030 --> 00:19:01,238
触觉传感器有反应
363
00:19:02,200 --> 00:19:06,536
这两个东西合合上没有的一声刻上了
364
00:19:07,010 --> 00:19:10,226
所以声音和触觉在这里主要作为事件
365
00:19:10,810 --> 00:19:11,458
所以这样的话
366
00:19:11,540 --> 00:19:17,063
我们就构建了一个多模态的叫纪元技能的解析器
367
00:19:17,770 --> 00:19:21,880
首先通过视觉进行动作的图谱分解
368
00:19:22,350 --> 00:19:23,040
大家看到了
369
00:19:23,900 --> 00:19:26,710
第二个通过声音接触力
370
00:19:26,810 --> 00:19:28,367
我们通过图神经网络
371
00:19:28,940 --> 00:19:32,120
他表示一个基于事件的特征
372
00:19:32,540 --> 00:19:33,560
然后进行融合
373
00:19:34,310 --> 00:19:35,206
如果大家感兴趣
374
00:19:35,300 --> 00:19:36,620
我们也有一些文章的发表
375
00:19:37,590 --> 00:19:39,000
那么做的结果是这样的
376
00:19:39,680 --> 00:19:40,640
多模态的时候
377
00:19:40,920 --> 00:19:43,872
我们大概的动作解析达到95%的准确率
378
00:19:44,250 --> 00:19:48,750
现在我们通过扩散模型还在提高这样一个结果
379
00:19:49,220 --> 00:19:51,922
这个是分别采用单模态来做解析
380
00:19:52,230 --> 00:19:53,526
大家明显的可以看到
381
00:19:54,000 --> 00:19:57,819
单模态解析跟多模态之间差距还是比较大的
382
00:19:57,830 --> 00:19:58,260
好
383
00:19:58,400 --> 00:20:03,600
我们通过这样一个多模态的解析就能构建这样一个知识库
384
00:20:05,690 --> 00:20:06,258
在此基础
385
00:20:06,340 --> 00:20:08,251
我们团队提出了一个三个维度
386
00:20:08,260 --> 00:20:10,856
六个层次的知识表达方法
387
00:20:10,970 --> 00:20:11,270
好
388
00:20:11,520 --> 00:20:12,825
可能大家比较感兴趣
389
00:20:12,830 --> 00:20:14,666
什么叫维度呢
390
00:20:14,950 --> 00:20:19,042
也就是说我们机器人学习过程里面会有大量的实例
391
00:20:19,520 --> 00:20:23,656
包括我们工人们熟练的工人做各种事情的这种试教
392
00:20:24,980 --> 00:20:27,100
我们在他基础上去学习
393
00:20:27,680 --> 00:20:28,480
学到的话
394
00:20:28,610 --> 00:20:29,486
这个拓扑结构
395
00:20:29,930 --> 00:20:31,700
这个拓扑达到最好状态
396
00:20:32,220 --> 00:20:34,046
有人说最好也这个样子了
397
00:20:34,050 --> 00:20:34,866
咱们就不学了
398
00:20:35,250 --> 00:20:37,200
咱们直接做成模板以后
399
00:20:37,310 --> 00:20:39,382
下次用的时候直接调出来就完了
400
00:20:40,380 --> 00:20:41,619
这叫所谓的模板
401
00:20:41,900 --> 00:20:43,954
当然了我们在描述系统的时候
402
00:20:44,360 --> 00:20:48,140
也用到了动态表示和静静态表示
403
00:20:48,890 --> 00:20:52,124
你比如说像场景里面大量的实体
404
00:20:52,130 --> 00:20:53,506
实体就是操作物体
405
00:20:54,430 --> 00:20:55,340
操作的目标
406
00:20:55,780 --> 00:20:58,692
包括做包括操作场景里面的某些物体
407
00:20:59,310 --> 00:21:01,312
那么智能体就是能够动作行为
408
00:21:01,320 --> 00:21:04,380
能够感知决策和行为的东西
409
00:21:04,470 --> 00:21:06,868
在我们这儿主要是机器人
410
00:21:07,510 --> 00:21:11,704
然后后面是机器人通过技能形成不同任务
411
00:21:12,070 --> 00:21:13,756
包括动作机缘
412
00:21:14,140 --> 00:21:16,228
形成这样一个表示方法来做
413
00:21:18,750 --> 00:21:22,126
那么接下来的话我们就要做知识模板
414
00:21:22,430 --> 00:21:24,038
这个知识模板刚才我讲过了
415
00:21:24,040 --> 00:21:29,548
就是说我通过不断的学习经验得到的最佳脱口结构就这个样了
416
00:21:30,690 --> 00:21:32,692
把它变成一个个知识饱满
417
00:21:34,390 --> 00:21:37,772
大家会想说我做一件事可以各种各样的任务
418
00:21:37,960 --> 00:21:40,494
比如说我这边这个地方做老排线
419
00:21:40,870 --> 00:21:42,454
下步我要安装同轴线
420
00:21:42,670 --> 00:21:44,770
下一步我要装sim卡等等
421
00:21:45,130 --> 00:21:46,930
那么这些过程如何连起来呢
422
00:21:47,510 --> 00:21:51,899
这就需要基于模板知识的这种长距离的学习
423
00:21:52,620 --> 00:21:56,340
把其实把一个复杂任务的多个任务
424
00:21:56,830 --> 00:22:01,310
每个任务它不是刚才讲的都有形成的知识模板
425
00:22:01,750 --> 00:22:06,700
然后通过子任务的序列化形成的话多任务这个过程
426
00:22:07,830 --> 00:22:08,190
好
427
00:22:08,400 --> 00:22:10,248
下面我们再讲讲屈伸优化
428
00:22:11,400 --> 00:22:12,777
其实大家我们都知道
429
00:22:12,940 --> 00:22:15,268
我们刚才说各种技能的学习
430
00:22:15,700 --> 00:22:18,339
我们让机器人更多的向人学习
431
00:22:19,430 --> 00:22:21,487
在这里我们就研究一个非常有趣的问题
432
00:22:21,860 --> 00:22:26,189
就是说手把手的教机器人学习
433
00:22:26,570 --> 00:22:29,962
跟机器人仅仅通过视觉眼睛看去学习
434
00:22:30,280 --> 00:22:33,144
这个之间究竟存在一个什么样的关系
435
00:22:34,510 --> 00:22:43,319
我们通过这个人工智能里面的这个闪度统计机器学习
436
00:22:44,000 --> 00:22:50,168
我们就找到了这个完全视角跟仅仅部分视角之间的关系
437
00:22:50,640 --> 00:22:54,510
而且我们看到了如果找到他们俩这个不同
438
00:22:54,960 --> 00:22:59,916
最后实际上归结的是搞机器人经常研究这样力动力学的不一致性
439
00:23:01,360 --> 00:23:03,760
而这种不一致性我能够把它刻画出来
440
00:23:03,950 --> 00:23:07,911
用它增强仅仅通过部分视角学习的效果
441
00:23:08,470 --> 00:23:13,222
我们这个增强非常有趣的一个结果就是增强以后的性能
442
00:23:13,240 --> 00:23:16,216
通过眼睛看能够达到完全失调的一半
443
00:23:16,610 --> 00:23:18,566
有人讲说我下一次再学一次
444
00:23:18,990 --> 00:23:25,290
剩下的一半我们大概能估计到几次学习就能达到完全失效的学习能力
445
00:23:25,400 --> 00:23:25,770
好
446
00:23:26,220 --> 00:23:31,212
第二个我们去做的一个事情就是说我们人做事情都是有绝活
447
00:23:32,210 --> 00:23:35,458
有的人的话他就是天生手臂灵巧
448
00:23:35,870 --> 00:23:37,310
你比如说打乒乓球
449
00:23:37,310 --> 00:23:38,624
他的弧圈球就发的好
450
00:23:39,250 --> 00:23:41,824
那个人的话是反车打的好
451
00:23:42,670 --> 00:23:49,224
我们就在想我们能把这些人工巧匠他的各种技能能够让机器学到吗
452
00:23:50,300 --> 00:23:52,550
这里的话就是我们做了一个很重要的工作
453
00:23:53,050 --> 00:23:55,750
第一步工作就是如何做表示
454
00:23:56,150 --> 00:24:01,310
就把这种非常好的突破的最佳的动作行为把它表征出来
455
00:24:01,670 --> 00:24:03,812
我们提出叫轨迹概率
456
00:24:04,670 --> 00:24:08,410
第二部分我们要建立轨迹概率跟最大熵之间的关系
457
00:24:08,600 --> 00:24:14,208
通过熵大家知道我们才能构造样本的产生器、甄别器
458
00:24:14,210 --> 00:24:15,326
形成对抗学习
459
00:24:15,670 --> 00:24:20,800
就产生跟我挑选的这个行为一致的这种技能
460
00:24:21,830 --> 00:24:26,890
这样的话我们就能够把人工巧匠做的最佳技能
461
00:24:26,900 --> 00:24:28,104
机器把它学进去
462
00:24:29,740 --> 00:24:30,110
好
463
00:24:30,110 --> 00:24:31,782
接下来我们又做了一件事
464
00:24:32,240 --> 00:24:37,440
就是说你前面的很多训练都是在比如说在虚拟场景下训练
465
00:24:38,590 --> 00:24:39,940
当然我们有一些方法
466
00:24:39,940 --> 00:24:43,965
比如说喻自适应的方法与随机化的办包括知识增6的办法
467
00:24:44,730 --> 00:24:48,570
提高虚实迁移以后的样本的泛化能力
468
00:24:49,400 --> 00:24:50,375
包括鲁棒性
469
00:24:50,990 --> 00:24:55,935
但是的话实际系统总归是有可能有很多的不确定因素
470
00:24:56,510 --> 00:24:57,910
摩擦、干扰
471
00:24:58,910 --> 00:25:00,828
那么我们这里也有做了一件工作
472
00:25:00,930 --> 00:25:04,186
就是我假设在虚拟场景下
473
00:25:04,560 --> 00:25:07,637
前面的学习学到的方法应该是接近最优
474
00:25:08,360 --> 00:25:14,591
这样的话我能不能通过蒙特卡洛把前面学到的最优结果把它做一个放大
475
00:25:15,810 --> 00:25:17,616
找到一个可行域
476
00:25:18,190 --> 00:25:23,118
在这个可行域当中再通过列强化学习再做一次优化
477
00:25:23,530 --> 00:25:26,506
这样的话得到的这个我们叫first shot learning
478
00:25:26,520 --> 00:25:27,920
就是少样本学习
479
00:25:28,860 --> 00:25:32,940
就能把这个结果再进一步的量化优化
480
00:25:33,860 --> 00:25:36,948
这个是我们构建的一个3C装配场景
481
00:25:37,240 --> 00:25:38,608
我们大概做四个任务
482
00:25:39,360 --> 00:25:43,010
这里包括有视觉、触觉、听觉、多模态的感知
483
00:25:43,870 --> 00:25:46,972
为我们构建的这个知识库
484
00:25:47,310 --> 00:25:48,780
几乎覆盖了所有的任务
485
00:25:48,990 --> 00:25:51,580
你比如说我们也可以从人手试教
486
00:25:52,360 --> 00:25:53,650
做动作解析
487
00:25:53,650 --> 00:25:54,756
建立大型知识库
488
00:25:54,760 --> 00:25:55,750
然后再去优化
489
00:25:57,530 --> 00:26:01,634
接下来的话就通过大模型去做任务规划器
490
00:26:01,650 --> 00:26:02,802
我们下面来看这一块
491
00:26:03,480 --> 00:26:03,900
另外一个
492
00:26:04,000 --> 00:26:06,410
我们在这里讲一个非常有趣的事情
493
00:26:06,950 --> 00:26:09,400
就是在机器人装配当中
494
00:26:09,680 --> 00:26:14,449
这种反应式方法对注意力机制是非常有效的
495
00:26:15,020 --> 00:26:19,110
所以我们在这里的话要建立关于反馈选择的注意力机制
496
00:26:21,370 --> 00:26:23,200
来实现这种
497
00:26:25,280 --> 00:26:26,630
对于操作场景的感知
498
00:26:26,630 --> 00:26:28,184
过程感知和认知
499
00:26:28,990 --> 00:26:29,390
好了
500
00:26:29,790 --> 00:26:33,156
我们目前总共建了13000多条知识库
501
00:26:33,550 --> 00:26:38,771
这个13000多条知识库包含了场景和机器人动作
502
00:26:39,720 --> 00:26:41,852
这里可以大概组合32个任务
503
00:26:43,060 --> 00:26:47,088
接下来的话我们要通过大模型做任务规划器
504
00:26:47,210 --> 00:26:48,816
这是我下面要做四个任务
505
00:26:50,420 --> 00:26:51,170
接下来的话
506
00:26:51,310 --> 00:26:53,477
就是通过这个任务规划器
507
00:26:53,480 --> 00:26:57,543
要到知识库当中去找什么做这些任务的
508
00:26:58,820 --> 00:27:00,386
我们讲场景库是什么
509
00:27:01,630 --> 00:27:02,920
机能库是什么
510
00:27:02,920 --> 00:27:04,090
机缘库是什么
511
00:27:04,240 --> 00:27:05,680
这个是最重要的一步
512
00:27:06,070 --> 00:27:10,438
其实我们大家很多人做大模型做都用一句话概括说我要强化学习
513
00:27:10,620 --> 00:27:13,064
其实强化学习对什么做强化呢
514
00:27:13,270 --> 00:27:20,152
是强化还是最后实际上是实际上对应的是技能里的机缘这块的强化学习
515
00:27:22,020 --> 00:27:23,478
进一步做轨迹的优化
516
00:27:24,460 --> 00:27:26,517
接下来的话我们就生成了
517
00:27:28,990 --> 00:27:33,406
通过机缘生成了就是你要做这件事情最佳的力是多少
518
00:27:33,480 --> 00:27:35,168
最佳的位置是什么
519
00:27:36,120 --> 00:27:37,456
让底层控制去实现
520
00:27:37,700 --> 00:27:38,624
像这个底层这块
521
00:27:39,040 --> 00:27:41,120
可能大家用到了自动化的很多办法
522
00:27:45,140 --> 00:27:45,460
好
523
00:27:45,460 --> 00:27:46,920
这个是我们做的一个例子
524
00:27:51,490 --> 00:27:52,498
就是把四个任务
525
00:27:52,500 --> 00:27:54,270
如何把它连接起来
526
00:28:12,390 --> 00:28:17,880
大家看到的这个地方是有一个图谱优化的
527
00:28:19,030 --> 00:28:22,430
就是动机缘和技能之间的一个拓扑结构
528
00:28:23,080 --> 00:28:26,136
通过这个过程能够不断的做动作优化
529
00:28:26,300 --> 00:28:26,770
就这个
530
00:28:49,520 --> 00:28:50,320
第一个任务完成了
531
00:28:50,320 --> 00:28:51,460
现在就到第二个任务
532
00:29:01,090 --> 00:29:03,330
这是装sim卡的第三个任务
533
00:29:07,960 --> 00:29:08,830
接下来的话
534
00:29:08,830 --> 00:29:09,670
最后一个任务
535
00:29:17,220 --> 00:29:17,590
亲
536
00:29:17,780 --> 00:29:19,292
这是装的前置摄像机
537
00:29:19,580 --> 00:29:20,840
这样四个任务就做完了
538
00:29:21,780 --> 00:29:22,150
好
539
00:29:22,230 --> 00:29:24,738
下面我做一下发展的展望
540
00:29:25,880 --> 00:29:31,478
其实以chop CDP为代表的大模型
541
00:29:31,670 --> 00:29:33,408
有人讲它叫做狭义大模型
542
00:29:34,210 --> 00:29:35,776
它究竟是范式的突破
543
00:29:35,780 --> 00:29:39,170
还是AI以往的AI技术的延伸呢
544
00:29:39,560 --> 00:29:43,400
我认为的话应该是一个范式的突破
545
00:29:44,500 --> 00:29:48,724
人工智能第一范式符号主义和连第二范式连接主义
546
00:29:49,440 --> 00:29:52,450
他们是一个开环的智能
547
00:29:54,190 --> 00:29:58,000
但是恰PDP他借用到了知识数据
548
00:29:58,460 --> 00:30:00,396
还有能力反馈的强化学习
549
00:30:00,720 --> 00:30:02,010
包括大模型的机身应用
550
00:30:02,010 --> 00:30:03,582
我们谈到了一个用户的反馈
551
00:30:04,690 --> 00:30:11,505
包括刚才我们谈到了我们这里构造了一个其实去反馈的注意力模块
552
00:30:12,210 --> 00:30:14,790
所以反馈在这里的话是非常重要的
553
00:30:15,020 --> 00:30:16,703
所以它是一个闭环的智能
554
00:30:18,380 --> 00:30:20,278
所以我们认为是一个范式突破
555
00:30:20,440 --> 00:30:20,770
好
556
00:30:22,910 --> 00:30:27,252
目前chat GDP它构建了很多大模型特有的一些技术
557
00:30:28,290 --> 00:30:29,546
它主要是面向对话
558
00:30:29,550 --> 00:30:32,938
但是它也构建了一些大模型所用到的一些共性技术
559
00:30:32,950 --> 00:30:35,974
比如说这个指令学习
560
00:30:37,270 --> 00:30:38,390
这个思维念
561
00:30:39,510 --> 00:30:41,508
包括上下文等等这些
562
00:30:42,160 --> 00:30:43,799
但是他的推理能力的不足
563
00:30:43,800 --> 00:30:47,334
最近我们也看到大模型整改知识图谱在结合
564
00:30:48,250 --> 00:30:49,130
面向多模态
565
00:30:49,930 --> 00:30:52,218
最近跟扩散模型在不断的结合
566
00:30:52,910 --> 00:30:53,660
另外一方面
567
00:30:53,780 --> 00:30:56,600
最近也有人在研究一个非常重要叫行为大模型
568
00:30:57,470 --> 00:31:00,750
就是把CNN怎么样跟强化学习结合
569
00:31:02,890 --> 00:31:05,559
另外我们也有很多人在研究认知大模型
570
00:31:06,550 --> 00:31:08,904
我想这些不同模型的研究
571
00:31:09,710 --> 00:31:13,562
最后有可能再把它集成起来
572
00:31:13,710 --> 00:31:19,330
真正我们得到我们想要的这种通用的这种大模型
573
00:31:22,010 --> 00:31:25,274
在机器人应用领域
574
00:31:25,430 --> 00:31:28,537
我们认为它要依托机构大模型
575
00:31:29,050 --> 00:31:31,515
我们这种机构大模型我们认为它叫通才
576
00:31:32,470 --> 00:31:33,720
然后在下一个任务里面
577
00:31:33,800 --> 00:31:38,516
我们要发展各种领域里的特定任务的专才
578
00:31:38,760 --> 00:31:40,972
这里我刚才谈到的很重要的一点
579
00:31:41,240 --> 00:31:45,144
就是要建立面向不同任务、不同场景的场景库
580
00:31:46,780 --> 00:31:53,773
这个技能库和机缘库这个是大模型学习做具身非常重要的一个
581
00:31:54,100 --> 00:31:55,294
我们叫中间件
582
00:31:56,600 --> 00:31:57,010
好
583
00:31:57,750 --> 00:31:59,190
我今天的报告就到这儿
584
00:31:59,350 --> 00:31:59,930
谢谢大家
585
00:32:09,490 --> 00:32:12,292
非常感谢孙老师带来的精彩报告
586
00:32:14,330 --> 00:32:17,050
我们看一下评论区里面有没有问题
587
00:32:22,480 --> 00:32:24,940
目前没有针对咱们报告的一个问题
588
00:32:24,950 --> 00:32:27,776
只是问有没有回放行
589
00:32:28,110 --> 00:32:33,258
我们把问题留在后面的那个panel的discussion的这个部分
590
00:32:34,350 --> 00:32:35,378
我们这样
591
00:32:35,700 --> 00:32:39,797
我们来欢迎第二位报告嘉宾张文强老师
592
00:32:40,370 --> 00:32:45,100
张文强老师为我们带来的题目是具身智能之发育观
593
00:32:46,090 --> 00:32:47,988
我也是简单的介绍一下张老师
594
00:32:48,460 --> 00:32:50,560
张老师是CCF的杰出会员
595
00:32:50,820 --> 00:32:52,668
智能机器人专委会的副主任
596
00:32:53,210 --> 00:32:54,850
复旦大学的研究员、博导
597
00:32:54,850 --> 00:32:56,819
智能机器人研究院副院长
598
00:32:57,380 --> 00:32:59,718
长期从事人工智能机器人等研究
599
00:33:00,190 --> 00:33:10,706
先后主持了多项国家自然科学基金和科技创新2030新一代人工智能的重大项目的发表了论文多篇
600
00:33:10,720 --> 00:33:14,554
并且获得了中国国际工业博览会的特等奖
601
00:33:15,210 --> 00:33:16,860
教育部技术发明二等奖等
602
00:33:17,420 --> 00:33:20,536
下面我们有请张老师为我们带来精彩的报告
603
00:33:21,400 --> 00:33:21,710
好
604
00:33:22,200 --> 00:33:24,840
谢谢CCF术语工委的邀请
605
00:33:25,530 --> 00:33:29,085
我刚我就接着我们富川老师的这个
606
00:33:29,530 --> 00:33:31,740
我就从我发育这个角度
607
00:33:31,930 --> 00:33:33,906
我来谈谈机身智能
608
00:33:34,130 --> 00:33:37,574
同时也结合我们课题组在这几年所做的这个工作
609
00:33:37,910 --> 00:33:40,070
来谈谈我个人的这一个感想
610
00:33:40,700 --> 00:33:43,675
我们首先来既然是从方宇这个角度来谈
611
00:33:43,950 --> 00:33:46,117
我们来看一看什么是发育
612
00:33:46,600 --> 00:33:48,607
从生物从人来说的话
613
00:33:48,610 --> 00:33:50,614
我们来随着这个年龄的增长
614
00:33:50,790 --> 00:33:51,998
我们器官是发育的
615
00:33:52,090 --> 00:33:53,773
我们身体也是发育的
616
00:33:54,150 --> 00:33:57,462
我们心理和智力也是发育的
617
00:33:57,470 --> 00:34:01,490
当然是通过跟外界不断的交互交流过程中的话
618
00:34:01,800 --> 00:34:04,569
我们有这么些做的发育的功能
619
00:34:04,820 --> 00:34:08,276
所以说我今天所讲的这个内容主要是三块
620
00:34:08,690 --> 00:34:10,292
我们一块是心智发育
621
00:34:10,500 --> 00:34:12,246
另外一块是本体发育
622
00:34:12,480 --> 00:34:15,084
还有一块是群智发育的这三个点
623
00:34:15,600 --> 00:34:18,228
我们先来看一看这个新防御
624
00:34:19,250 --> 00:34:21,340
我们也先来看那两个视频
625
00:34:21,420 --> 00:34:24,247
这也是我经常放给学生看
626
00:34:24,500 --> 00:34:26,500
左边这一个视频我看看
627
00:34:30,680 --> 00:34:36,222
昨天这个视频是东方卫视某个主持人在上海主持节目的时候
628
00:34:36,340 --> 00:34:40,140
他跟这个庞然大物这个机器人一起跳舞来主持
629
00:34:40,240 --> 00:34:42,052
他觉得这个机器人非常智能
630
00:34:42,620 --> 00:34:43,467
右边这一个视频
631
00:34:43,610 --> 00:34:45,590
这是我从抖音上发下来的
632
00:34:46,240 --> 00:34:48,580
我们也来看一看这种低等动物
633
00:34:48,700 --> 00:34:49,925
像阿猫阿狗
634
00:34:50,240 --> 00:34:52,495
他们在跟人交互的过程中
635
00:34:52,500 --> 00:34:54,156
他是怎么来进行这个学习的
636
00:34:54,160 --> 00:34:55,240
我们来看一下
637
00:35:21,670 --> 00:35:23,176
从这两个视频里面
638
00:35:23,180 --> 00:35:25,083
实际上我们也可以看出来
639
00:35:25,240 --> 00:35:30,000
这个阿猫它所具有的这个职能很左边这个庞然大物哪一个智能高
640
00:35:30,370 --> 00:35:32,528
我们这里面是应该是有答案的
641
00:35:33,090 --> 00:35:39,145
那么从这一块我们可以看出来这个阿猫阿狗它所具备的这个智能是一个自主职能
642
00:35:39,620 --> 00:35:40,420
包括我们人
643
00:35:40,850 --> 00:35:41,780
从感知也好
644
00:35:41,780 --> 00:35:42,548
认知也好
645
00:35:42,780 --> 00:35:44,705
推理学习到行为
646
00:35:44,840 --> 00:35:45,776
它是一个闭环
647
00:35:45,780 --> 00:35:49,048
刚才付村老师也提到了说闭环这一块很重要
648
00:35:49,610 --> 00:35:52,060
从这一个层面上我们再来看一看
649
00:35:52,070 --> 00:35:55,509
那我们有没有给阿狗提供大量的标注的样本
650
00:35:55,850 --> 00:35:58,688
也没有他通过小量的数据
651
00:35:58,690 --> 00:36:00,590
通过模仿的学习就可以
652
00:36:01,600 --> 00:36:04,150
我们这里面还要强调一点就是能耗
653
00:36:04,950 --> 00:36:06,408
我们跑深度学习也好
654
00:36:06,410 --> 00:36:07,823
我们消耗大量的资源
655
00:36:08,360 --> 00:36:10,169
但是这个低等的动物
656
00:36:10,420 --> 00:36:13,420
它是大脑所消耗的资源很小
657
00:36:13,600 --> 00:36:14,447
你比如说阿猫狗
658
00:36:14,450 --> 00:36:17,128
他头脑的话还不到人脑的1%
659
00:36:17,450 --> 00:36:19,805
功耗的话也只有0.1到0.2瓦
660
00:36:20,510 --> 00:36:25,386
这是我们对类似于这个芯片这个硬件设计的时候的话
661
00:36:25,400 --> 00:36:27,710
我们能不能从电脑芯片
662
00:36:28,150 --> 00:36:30,394
从仿生这个层面上我们来做
663
00:36:31,370 --> 00:36:32,550
基于这一块
664
00:36:32,720 --> 00:36:34,970
我们觉得从脑科学也好
665
00:36:36,130 --> 00:36:37,450
从心理学也好的话
666
00:36:37,450 --> 00:36:39,160
我们要获得一些启发
667
00:36:39,210 --> 00:36:43,500
你比如说这个心理学的专家贺不他有一个后不规则
668
00:36:43,770 --> 00:36:45,690
比如说同时激活的这个神经元之间
669
00:36:45,690 --> 00:36:47,580
它的连接权重会加强
670
00:36:47,940 --> 00:36:51,900
这不这也是说我们大脑的学习是以无监督学习为主
671
00:36:52,030 --> 00:36:54,718
而且我们所做的这个任务都是通用的
672
00:36:54,730 --> 00:36:58,006
这个任务我们可以做到举一反三
673
00:36:58,230 --> 00:37:01,545
甚至说我们可以来掌握一些物理的常识
674
00:37:02,160 --> 00:37:04,512
还有一些各种各样的这一个常识
675
00:37:04,690 --> 00:37:07,250
但是目前所研究这个机器人来说的话
676
00:37:07,410 --> 00:37:09,150
在这一块我们还达不到
677
00:37:09,610 --> 00:37:11,050
所以说在二十多年前
678
00:37:11,050 --> 00:37:13,974
我们就跟MICU的龚建阳教授来合作
679
00:37:13,990 --> 00:37:19,549
就提出了心智发育这么一个理论和算法
680
00:37:19,900 --> 00:37:24,489
核心的思想我们还是希望是从脑科学、认知科学、心理学
681
00:37:24,740 --> 00:37:27,580
从这些学科里面我们是能不能来受到一些启发
682
00:37:27,830 --> 00:37:29,398
来营养机器人来做
683
00:37:30,690 --> 00:37:33,209
他所给他提供的这个程序
684
00:37:33,250 --> 00:37:34,888
它是一个基因的程序
685
00:37:34,890 --> 00:37:37,360
是不是能基于这个基因的程序
686
00:37:37,360 --> 00:37:39,527
我们来做一些通用的任务
687
00:37:40,440 --> 00:37:43,290
而不是仅仅是完成一些特定的任务
688
00:37:43,380 --> 00:37:46,984
这是跟我们目前那个大模型的思路的话
689
00:37:47,000 --> 00:37:48,134
我们还是一致的
690
00:37:48,390 --> 00:37:49,070
打个比方
691
00:37:49,380 --> 00:37:51,228
比如说我们人类的这个视觉系统
692
00:37:51,230 --> 00:37:52,130
它是双通路的
693
00:37:52,280 --> 00:37:56,078
这生命科学里面已经验证了一条是腹侧流
694
00:37:56,680 --> 00:37:58,480
一条是被测流
695
00:37:58,780 --> 00:38:02,128
一条是来处理空间物体的空间位置信息的
696
00:38:02,580 --> 00:38:04,959
一条是来处理物体的轮廓信息
697
00:38:05,550 --> 00:38:07,695
我这是我们人的两只眼睛
698
00:38:07,700 --> 00:38:09,480
我们来观察世界的时段
699
00:38:09,800 --> 00:38:12,524
我们搞计算机来做网络模型
700
00:38:12,610 --> 00:38:15,551
我们能不能借鉴这个双通路是一种思想
701
00:38:16,270 --> 00:38:21,166
所以说这是当时我们设计的这个W的MWN的网络模型
702
00:38:21,180 --> 00:38:24,300
也就是说我们通过这一个还是浅层的神经网络
703
00:38:24,670 --> 00:38:30,281
通过这个浅层神经网络来训练它识别这个物体它到底是在什么样的位置
704
00:38:30,760 --> 00:38:31,964
它的特征是什么
705
00:38:32,520 --> 00:38:33,068
打个比方
706
00:38:33,170 --> 00:38:36,760
比如说我们来训练这个机器人识别复杂更难的物体
707
00:39:03,570 --> 00:39:06,766
注意力选择是人类的一种基本认知能力
708
00:39:07,300 --> 00:39:09,500
例如人要去取一个物体时
709
00:39:09,500 --> 00:39:11,735
首先要将注意力集中在这个物体上
710
00:39:12,420 --> 00:39:17,572
我们机器人是通过一种奖惩机制来让他掌握这种本领
711
00:39:17,970 --> 00:39:20,605
如果能他能将注意力集中在这个环境上
712
00:39:20,880 --> 00:39:23,535
我们就按示表示奖励
713
00:39:24,040 --> 00:39:25,720
否则予以惩罚
714
00:39:26,100 --> 00:39:27,168
通过反复学习
715
00:39:27,420 --> 00:39:30,678
机器人的注意力就会始终注视着这个玩具
716
00:39:31,480 --> 00:39:31,670
好
717
00:39:31,840 --> 00:39:34,540
这是我们2005年复旦百年校庆的时候
718
00:39:34,550 --> 00:39:36,255
推出了复旦一号这个机群
719
00:39:36,850 --> 00:39:40,950
下面我们再来结合我们这两年正在做的这个事
720
00:39:41,300 --> 00:39:42,497
其中一个事的话
721
00:39:42,500 --> 00:39:46,490
我们是来做这个显著性和伪装的这个目标来检测
722
00:39:46,630 --> 00:39:50,030
当然是从这个背景下怎么来把它在这一块的话
723
00:39:50,040 --> 00:39:51,069
我们有全监督的
724
00:39:51,540 --> 00:39:52,428
也有弱监督的
725
00:39:52,750 --> 00:39:55,228
当然还有无监督的这个学习方法
726
00:39:56,020 --> 00:39:57,604
在弱点的这一个层面
727
00:39:57,700 --> 00:40:01,287
你比如说我们有小同行他来做要做标注
728
00:40:01,390 --> 00:40:03,326
他可能把这个前景的目标
729
00:40:03,330 --> 00:40:05,570
我通过涂鸦这个方式来标注一下
730
00:40:06,630 --> 00:40:08,169
背景我们也通过这个
731
00:40:08,440 --> 00:40:12,296
但是这一块也是消耗这个标准的成本
732
00:40:12,980 --> 00:40:16,388
我们这一个博士生他所有的工作的话
733
00:40:16,400 --> 00:40:17,944
我们只要是点监督
734
00:40:18,050 --> 00:40:20,990
那我们在在目标上我们点一个点
735
00:40:21,050 --> 00:40:22,354
在背景上来点一点
736
00:40:22,720 --> 00:40:30,532
那我们把它称为单点标注来我们这里面还涉及了一些这是你的泛红填充的这种方法
737
00:40:30,810 --> 00:40:35,893
通过这种单点的标注来做了一些图像的显著目标检测
738
00:40:36,240 --> 00:40:39,240
我们也把它移植到这个呃视频里面
739
00:40:39,240 --> 00:40:43,508
因为视频的这个标注的话所消耗的时间和成本更大
740
00:40:43,710 --> 00:40:46,053
在这一块我们也做了一致
741
00:40:46,210 --> 00:40:50,844
在针对这一种无监督这一个层面
742
00:40:52,010 --> 00:40:53,250
目前
743
00:40:55,830 --> 00:41:02,196
目前的话我们还是关注到这个边缘对准确识别的重要性
744
00:41:02,619 --> 00:41:08,813
在目前我们也是把这个transformer的这个模型基础上增加了这个代理任务
745
00:41:09,060 --> 00:41:13,074
使我们这个模型同时具备检测显著性目标
746
00:41:13,310 --> 00:41:15,494
还有显著性目标边缘的这个能力
747
00:41:15,500 --> 00:41:19,954
就对整个的模型的边缘起到了一个监督
748
00:41:20,470 --> 00:41:23,428
在这一块我们也跟产业界来做进行合作
749
00:41:23,770 --> 00:41:24,598
比如说这一块
750
00:41:24,860 --> 00:41:26,570
这是上海的情感机器人
751
00:41:26,950 --> 00:41:29,174
他做这个送餐机器
752
00:41:29,330 --> 00:41:31,899
他做产品的检验
753
00:41:31,900 --> 00:41:34,120
那我们就把前面所用到的这个目标
754
00:41:34,340 --> 00:41:39,398
检测的算法来用到类似于全球很多的菜品
755
00:41:39,610 --> 00:41:41,365
包括这个产品的识别
756
00:41:41,490 --> 00:41:43,524
这是一个应用的案例
757
00:41:44,290 --> 00:41:44,890
在违章
758
00:41:44,890 --> 00:41:46,226
比如这个检测里面
759
00:41:46,230 --> 00:41:49,842
我们是我们去年发表的一篇论文
760
00:41:50,210 --> 00:41:53,390
就是我们人针对一些伪造目标的话
761
00:41:53,390 --> 00:41:56,078
那我们凑近了是不是可以看得更清楚
762
00:41:56,420 --> 00:41:57,530
基于这个想法
763
00:41:57,790 --> 00:42:03,271
我们就提出了这么一个搜索放大和识别的这么一个框架和机制
764
00:42:04,800 --> 00:42:05,950
在这一块好
765
00:42:06,250 --> 00:42:09,685
这是从非常经典的目标检测这一块
766
00:42:10,320 --> 00:42:14,962
我们还在做的另外一个就是做情感计算和情感识别
767
00:42:15,130 --> 00:42:17,209
无论是在认知心理学里面
768
00:42:17,450 --> 00:42:19,382
或者在机器人这个层面来说的话
769
00:42:19,390 --> 00:42:22,190
情感是一个非常重要的这么一个指标
770
00:42:22,600 --> 00:42:24,658
那我们在人机交互这个层面的话
771
00:42:24,660 --> 00:42:30,596
我们还是需要机器人希望能有情感来跟人来进行这个交互的时候
772
00:42:30,830 --> 00:42:32,006
就是从声音也好
773
00:42:32,360 --> 00:42:33,290
从语音也好
774
00:42:33,710 --> 00:42:35,547
从人脸这个表情也好的话
775
00:42:35,550 --> 00:42:37,981
都是以几种非常好的模态
776
00:42:39,510 --> 00:42:44,160
传统的在人脸表情识别这一块还是基于静态的做的比较多
777
00:42:44,360 --> 00:42:47,510
它只能是来表达一些现实的感情
778
00:42:48,230 --> 00:42:50,858
针对这种动态的人脸的视频
779
00:42:51,900 --> 00:42:54,019
我们怎么来判断他的这个情感
780
00:42:54,180 --> 00:42:54,639
这一块
781
00:42:54,640 --> 00:42:58,688
这是我们课题组这两年在做的这个事
782
00:42:59,120 --> 00:43:01,959
一个层面上我们怎么来构建这个数据集
783
00:43:01,969 --> 00:43:04,129
给出一个评价的指标
784
00:43:04,529 --> 00:43:05,313
在这一块
785
00:43:05,316 --> 00:43:08,844
这是我们在去年CVPR上发表了这么一篇论文
786
00:43:08,861 --> 00:43:12,491
也是针对这种动态的大场景的视频
787
00:43:13,390 --> 00:43:19,326
我们构建了这个数据集在这里面的话
788
00:43:19,330 --> 00:43:21,360
我们会针对22种场景
789
00:43:21,590 --> 00:43:24,158
接近4万个人脸的视频片段
790
00:43:24,600 --> 00:43:26,426
针对这个系统的基本表现
791
00:43:26,430 --> 00:43:28,480
我们构建了这个数据集
792
00:43:28,590 --> 00:43:30,858
这一块也有很多的单位都获得了
793
00:43:31,690 --> 00:43:33,760
让我们来进行这个应用
794
00:43:34,380 --> 00:43:35,717
下面这一块的话
795
00:43:35,720 --> 00:43:38,138
我们也是做了几个方面的工作
796
00:43:38,320 --> 00:43:40,872
时间原因我简单的等一下
797
00:43:41,100 --> 00:43:43,724
第一个就是针对这个计算资源的依赖
798
00:43:43,730 --> 00:43:49,250
我们怎么来有效的通过这个空间和通道的特征表示和失序之间的关系
799
00:43:49,650 --> 00:43:52,410
从较小的关键追踪的话
800
00:43:52,410 --> 00:43:55,946
我们怎么来学习面部表情的关键的信息
801
00:43:56,560 --> 00:43:59,486
这一块我们设计了一些网络模型
802
00:43:59,490 --> 00:43:59,770
好
803
00:43:59,940 --> 00:44:05,114
还有一块我们是怎么来设计这种显著性表层真的这个采样器
804
00:44:06,070 --> 00:44:13,076
这里面也是针对这种代表性的表情真我们怎么来面向动态人脸表情识别
805
00:44:13,880 --> 00:44:19,147
来做一些简单的可解释的、自信的表情关键帧的提取
806
00:44:19,670 --> 00:44:21,038
下面是这一块的话
807
00:44:21,040 --> 00:44:26,734
我们是基于这个语义关键帧怎么来提取动态人脸的这个表情
808
00:44:26,950 --> 00:44:27,590
这里面的话
809
00:44:27,590 --> 00:44:31,206
我们相当于说情境的这个信息很重要
810
00:44:31,540 --> 00:44:33,346
同样一个表情
811
00:44:33,410 --> 00:44:35,398
你放在不同的这个情境里面的话
812
00:44:35,620 --> 00:44:38,884
他所体现出来这个情感的话是不一样的
813
00:44:39,050 --> 00:44:40,650
我们怎么来把这个信息
814
00:44:41,590 --> 00:44:44,752
把这一部分知识的话我们添加进来来做
815
00:44:45,860 --> 00:44:46,628
这是一块
816
00:44:46,830 --> 00:44:48,046
当然还有一块的话
817
00:44:48,050 --> 00:44:51,290
我们怎么来做一些动态的人脸表情的生成
818
00:44:51,440 --> 00:44:54,224
这也是对我们整个的训练和学习的话
819
00:44:54,980 --> 00:44:56,993
也会起到非常重要的作用
820
00:44:57,120 --> 00:45:00,208
这一块我们也有小同学通过图文也好
821
00:45:00,350 --> 00:45:01,534
通过视频也好的话
822
00:45:01,670 --> 00:45:04,358
怎么来描述形成这个目标的这个人设
823
00:45:04,360 --> 00:45:05,858
还有这个数字人
824
00:45:06,230 --> 00:45:06,440
好
825
00:45:06,760 --> 00:45:08,620
这是前面这是第一部分
826
00:45:09,230 --> 00:45:12,732
后面的话我从本体发育和群体发育以来
827
00:45:14,060 --> 00:45:15,754
我们再来看一看两个视频
828
00:45:16,100 --> 00:45:17,860
这都是我从抖音上来发现
829
00:45:17,870 --> 00:45:19,900
那么这一个视频这也是
830
00:45:37,660 --> 00:45:37,970
好
831
00:45:39,210 --> 00:45:40,533
那么左边这一个视频
832
00:45:40,540 --> 00:45:42,472
这就是大家来看一看这个猫
833
00:45:42,630 --> 00:45:44,955
它要粘到这么一个狭窄的空间里面
834
00:45:45,310 --> 00:45:46,642
他这个本体这个结构
835
00:45:47,230 --> 00:45:48,979
或者说他这一个行为的话
836
00:45:49,240 --> 00:45:53,014
他这个技能或者机构还具有了这个能力
837
00:45:53,570 --> 00:45:55,838
目前的这个机型的话能不能达到这个能力
838
00:45:55,850 --> 00:45:57,280
我们也可以来反思一下
839
00:45:57,630 --> 00:46:02,520
后面这一块就是相当于说蜥蜴在碰到紧急情况了或者逃生的时候的话
840
00:46:02,540 --> 00:46:03,520
它会断尾
841
00:46:03,860 --> 00:46:07,976
它所有的这个功能的话我们these species of lizards have 
842
00:46:07,976 --> 00:46:11,504
mastered the art of escape fractures
843
00:46:11,510 --> 00:46:14,711
S on their vertebrate, allow them to detach along 
844
00:46:14,711 --> 00:46:18,069
multiple points of their tails with cracks in 
845
00:46:18,069 --> 00:46:18,872
their bones. 
846
00:46:18,892 --> 00:46:22,511
One would expect them to lose their tales often. 
847
00:46:22,518 --> 00:46:25,614
Yet, unless they're in danger, most busy tales 
848
00:46:25,614 --> 00:46:28,250
stay faithfully attached. And when their tails, 
849
00:46:28,250 --> 00:46:31,420
due to teach some lizards, can grow them back. 
850
00:46:32,570 --> 00:46:35,130
After all, the tail is an important organ, 
851
00:46:35,150 --> 00:46:39,257
helping them move and baLance with ease. Inspired 
852
00:46:39,257 --> 00:46:42,029
by the lizards crawling around their desert 
853
00:46:42,029 --> 00:46:45,210
campus, researchers that N, Y, U, adobe set out 
854
00:46:45,210 --> 00:46:48,735
to understand how lizer tails achieve the gold 
855
00:46:48,735 --> 00:46:49,560
dioxide int. 
856
00:46:50,720 --> 00:46:51,010
好
857
00:46:51,010 --> 00:46:53,060
时间原因我们就不看完
858
00:46:53,710 --> 00:46:56,826
从这一块就相当于说我们本体我们器官也好
859
00:46:56,840 --> 00:46:57,878
身体也好的话
860
00:46:58,010 --> 00:46:59,290
它也是一个发育的
861
00:47:00,030 --> 00:47:01,677
这个发育的过程的话
862
00:47:01,680 --> 00:47:04,130
跟我们心智的发育应该是对应的
863
00:47:04,600 --> 00:47:04,830
好
864
00:47:04,940 --> 00:47:06,128
在这一个层面上来说
865
00:47:06,190 --> 00:47:07,058
我们学术界也好
866
00:47:07,060 --> 00:47:07,930
产业界也好
867
00:47:08,120 --> 00:47:09,026
我们怎么来做
868
00:47:09,360 --> 00:47:10,344
从学术这个层面上
869
00:47:10,350 --> 00:47:14,220
这是从材料这一这个学科的话也有人在做
870
00:47:14,420 --> 00:47:16,193
比如说我们复旦大学
871
00:47:16,200 --> 00:47:18,300
他们也有在做这个微流控的
872
00:47:18,720 --> 00:47:20,711
就通过改变这种光照条件
873
00:47:20,720 --> 00:47:25,000
使这一个液体他运动的方向和速度来进行改变
874
00:47:26,250 --> 00:47:27,274
比如说左边这一个
875
00:47:27,280 --> 00:47:33,834
就是我记得印象中好像是MIT还是美国他们做的这种这个八爪鱼
876
00:47:34,740 --> 00:47:37,700
他是采用了这种驱动的这个方式的话
877
00:47:37,840 --> 00:47:39,840
就是采用了这种微流控
878
00:47:40,580 --> 00:47:46,054
那相当于说猫他们所所具有的这个功能
879
00:47:46,300 --> 00:47:48,400
我们整个从驱动方式这一块的话
880
00:47:48,650 --> 00:47:51,626
一定有别于传统的这种点击这种方式
881
00:47:52,060 --> 00:47:52,960
这个更简单
882
00:47:53,310 --> 00:47:57,750
这个行动记忆核心我们在医学在临床的话都有
883
00:47:58,320 --> 00:48:04,248
也就是说我这一种核心材料在低温的这个情况下的话会变形
884
00:48:04,270 --> 00:48:06,769
但是到了一定温度它它又恢复这个形状
885
00:48:06,930 --> 00:48:11,129
这一种我们可以来做一些引用他来做一件事
886
00:48:11,850 --> 00:48:16,743
这一部分这是我们研究院王明教授他们团体的话
887
00:48:16,970 --> 00:48:21,680
以此来进行仿制仿蚯蚓的一种结构
888
00:48:21,810 --> 00:48:23,147
比如说蚯蚓的话
889
00:48:23,390 --> 00:48:26,750
我怎么来通过这一种狭窄的通道
890
00:48:27,340 --> 00:48:31,216
或者前面所提到的说我们西医它还可以断尾
891
00:48:31,220 --> 00:48:32,636
在紧急情况下的话
892
00:48:32,710 --> 00:48:35,014
那相当于说我从肢体
893
00:48:35,120 --> 00:48:37,163
从机构这个层面的话
894
00:48:37,270 --> 00:48:40,000
我怎么来适应这种恶劣的环境
895
00:48:40,240 --> 00:48:42,136
比如说类似于这种通胀的话
896
00:48:42,140 --> 00:48:47,500
我们能不能这一种从防蚯蚓的这种结构来实现
897
00:48:47,760 --> 00:48:51,280
这相当于说从本体这个结构这个层面上来说了
898
00:48:51,280 --> 00:48:56,350
我们能不能借鉴一些生物的这种发育的这种思想来做一些事
899
00:48:56,550 --> 00:48:57,908
我觉得是可以的
900
00:48:58,300 --> 00:49:01,420
这一块也是我们研究院他们有教授
901
00:49:01,700 --> 00:49:03,239
我们在做三期机器人
902
00:49:03,370 --> 00:49:09,146
相当于说从针对这种空中和地面和水中的话
903
00:49:09,150 --> 00:49:10,356
我同一个本体
904
00:49:10,450 --> 00:49:13,312
我针对不同的这一种工作的这个场景的话
905
00:49:13,500 --> 00:49:16,668
我这个本体还是不是可以来做出一些应变
906
00:49:16,866 --> 00:49:20,685
这里面还有很多其他的场这个场合的话都有
907
00:49:20,689 --> 00:49:23,237
所以说在本体这个发育这个层面
908
00:49:23,238 --> 00:49:25,058
我们还要结合这个行为
909
00:49:25,241 --> 00:49:29,597
这里面就是李菲菲这个团队里面他们所做的这种事
910
00:49:29,611 --> 00:49:32,356
就相当于你说本体跟环境之间的话
911
00:49:32,440 --> 00:49:38,539
我们怎么来进行这一种这种交互来做一些事
912
00:49:38,550 --> 00:49:42,486
后面我估计错误会重点来介绍这一块
913
00:49:42,630 --> 00:49:44,879
这里面的话我就细节我也不讲
914
00:49:44,930 --> 00:49:48,098
那相当于是在前面福春老师也提到了
915
00:49:48,570 --> 00:49:52,434
当时我们怎么来通过训练这个机器人跟环境来进行交互
916
00:49:52,690 --> 00:49:55,410
来做整体的优化的发育
917
00:49:55,410 --> 00:49:59,270
也是来面对我们不同的环境有不同的挑战的话
918
00:49:59,430 --> 00:50:04,110
他在上面怎么来做出一些新的这一种改变
919
00:50:04,500 --> 00:50:07,356
所以说下面这还有一块跟本体结合起来
920
00:50:07,360 --> 00:50:09,572
我们怎么来进行这个行为的发育
921
00:50:09,580 --> 00:50:14,458
也就是说怎么来让机器人它跟物理的环境
922
00:50:14,460 --> 00:50:16,924
跟社会的环境来进行互动
923
00:50:17,600 --> 00:50:19,393
除了我感知本体这个信息
924
00:50:19,620 --> 00:50:21,780
我同时我还要感知外界的这个世界
925
00:50:22,290 --> 00:50:23,760
或者说我来做一些行为
926
00:50:24,030 --> 00:50:25,884
这里面我们也看两个
927
00:50:28,230 --> 00:50:31,910
一个是我们有研究者在做这个仿真云
928
00:50:32,230 --> 00:50:36,050
仿生鱼的话在这一个水中这么一个场景说的话
929
00:50:36,100 --> 00:50:38,410
它怎么来做一种行为的这种发育
930
00:50:38,680 --> 00:50:41,514
这一个这是一个仿生这个星星
931
00:50:42,010 --> 00:50:49,200
对这个场景里面的话也是怎么来做一些行为方面的这种发育
932
00:50:55,870 --> 00:50:56,140
好
933
00:50:56,630 --> 00:50:59,620
下面我再谈另外一个题
934
00:50:59,620 --> 00:51:01,138
就是群智智能
935
00:51:02,300 --> 00:51:04,705
单体这个机器人我可以交互了
936
00:51:04,980 --> 00:51:07,260
群体机器人的话我们也有这个需求
937
00:51:07,370 --> 00:51:13,258
这里面是国平教授他在群智计算架构里面怎么来模仿这个群智涌现的激励
938
00:51:13,510 --> 00:51:17,890
来做一些群众的协同感知数据与汇集协同计算
939
00:51:17,900 --> 00:51:19,538
还有融合的决策
940
00:51:19,630 --> 00:51:20,960
这是从架构
941
00:51:21,280 --> 00:51:25,960
下面我结合我们正在报的这个智能机器人重点专项里面
942
00:51:25,960 --> 00:51:27,652
我们面向的是工业场景里面
943
00:51:27,870 --> 00:51:30,285
你比如在新型的工业制造场景里面
944
00:51:30,290 --> 00:51:31,305
比如大飞机也好
945
00:51:31,660 --> 00:51:32,710
高铁装配也好
946
00:51:32,840 --> 00:51:34,776
大型船舶来进行焊缝也好
947
00:51:34,890 --> 00:51:37,362
都面临着这一种加工尺寸大
948
00:51:37,840 --> 00:51:38,569
任务多
949
00:51:38,920 --> 00:51:40,390
而且精度要求高
950
00:51:41,020 --> 00:51:45,525
目前市面上我们能看到的这些方式的话
951
00:51:45,530 --> 00:51:47,202
一个就是人工来进行加工
952
00:51:47,510 --> 00:51:50,916
人工加工的话它精度保证不了
953
00:51:50,920 --> 00:51:52,144
生产效率也比较低
954
00:51:52,440 --> 00:51:55,176
还有一个就是373机型来进行加工
955
00:51:55,410 --> 00:51:57,237
也受到空间限制
956
00:51:57,380 --> 00:51:59,564
也受到任务单一的这一种限制
957
00:51:59,670 --> 00:52:03,359
所以说多集成产线这一块有有有都有了
958
00:52:03,480 --> 00:52:06,210
但是他也面临着这一种协同性差
959
00:52:06,610 --> 00:52:09,503
群智智能还不足这一块儿
960
00:52:09,740 --> 00:52:11,652
你说针对这一部分
961
00:52:12,060 --> 00:52:18,601
我们在产业这里面或者说在市场这一块业务需求比较大有这么一个矛盾
962
00:52:18,790 --> 00:52:21,906
所以说在这一块我们就提出了三个科学问题
963
00:52:22,410 --> 00:52:25,754
这里面我们也提出了这个全息感知的这么一个概念
964
00:52:26,720 --> 00:52:28,449
赋予机器人在感知这个层面上
965
00:52:28,450 --> 00:52:30,100
怎么来通过全新这种方式
966
00:52:30,740 --> 00:52:33,020
下面也包含群智的涌现
967
00:52:33,160 --> 00:52:34,978
还有一些智慧的共生
968
00:52:35,630 --> 00:52:36,506
核心的这个点
969
00:52:36,510 --> 00:52:39,246
这是我们整个研究思路怎么达到闭环
970
00:52:39,420 --> 00:52:42,800
来提到说通过全系的感知和自主这个闭环的话
971
00:52:42,810 --> 00:52:44,460
能达到智能这个闭环管
972
00:52:44,970 --> 00:52:47,385
怎么来做协同的规划和群众的有限
973
00:52:47,710 --> 00:52:51,399
怎么来做一些技能的迁移和协同的进化
974
00:52:51,510 --> 00:52:53,568
包括我们知识怎么来进行的共享
975
00:52:54,340 --> 00:52:54,550
好
976
00:52:55,020 --> 00:52:57,504
主要是几个方面的研究内容
977
00:52:57,730 --> 00:53:01,993
第一个就是针对单机器人或者多机器人来说的话
978
00:53:02,010 --> 00:53:04,326
它智能闭环我们怎么来构建
979
00:53:06,210 --> 00:53:09,530
从驱动的这个自学习模型的持续学习执行机制
980
00:53:09,760 --> 00:53:14,085
到我们怎么来跟外界来进行这一个自主智能闭环的这一块
981
00:53:14,920 --> 00:53:17,571
下面这一块就是群智涌现
982
00:53:18,130 --> 00:53:19,714
这里面就包含几块
983
00:53:19,720 --> 00:53:21,700
一块是分布式的任务规划
984
00:53:21,710 --> 00:53:22,700
我们怎么来进行规划
985
00:53:23,170 --> 00:53:23,698
另外一块
986
00:53:23,700 --> 00:53:25,262
我们针对于这种任务状态
987
00:53:25,270 --> 00:53:28,534
怎么来做信息的反馈和优化机制这一块
988
00:53:29,400 --> 00:53:30,918
在分布式任务规划这一块
989
00:53:30,920 --> 00:53:33,592
我们针对复杂的任务怎么来进行分解
990
00:53:33,940 --> 00:53:36,726
这里面会提到了一些任务关联性
991
00:53:37,050 --> 00:53:40,298
怎么来做一些任务反馈和优化的调度
992
00:53:40,540 --> 00:53:41,597
细节我也不提了
993
00:53:41,940 --> 00:53:44,020
针对这种群智的话
994
00:53:44,340 --> 00:53:45,614
他怎么来做协同
995
00:53:45,780 --> 00:53:47,778
怎么来做智慧的共生
996
00:53:48,000 --> 00:53:49,728
这里面会用到几个
997
00:53:49,730 --> 00:53:51,734
比如说知识千亿的这个算法
998
00:53:52,100 --> 00:53:55,076
怎么来进行动态多目标的优化这一块
999
00:53:55,670 --> 00:53:57,720
又怎么来针对这种作业
1000
00:53:57,870 --> 00:53:59,150
针对这种工艺的话
1001
00:53:59,150 --> 00:54:02,000
我们怎么来做智慧的共生这个机制
1002
00:54:02,670 --> 00:54:04,494
这里面我们是用到两个场景
1003
00:54:04,660 --> 00:54:08,710
一个是大型船舶钢板切割面打磨这个场景
1004
00:54:08,720 --> 00:54:09,928
另外一个场景的话
1005
00:54:09,930 --> 00:54:11,298
我们针对巡检机器人
1006
00:54:11,300 --> 00:54:14,853
怎么来做到机器人装配机器人这么一个场景
1007
00:54:15,800 --> 00:54:16,090
好
1008
00:54:16,410 --> 00:54:18,210
在全职这一块的话
1009
00:54:18,210 --> 00:54:22,770
我们也是拿到了一个上海市人工智能重大专项
1010
00:54:22,780 --> 00:54:25,622
就是人机物三元协同与群众有限
1011
00:54:25,810 --> 00:54:35,071
这一个专项是由我们研究院的该中学教授来进行牵头这一个思想跟前面我所提到的说面向工业场景里面这个还是
1012
00:54:35,071 --> 00:54:35,827
相符合的
1013
00:54:36,130 --> 00:54:36,340
好
1014
00:54:36,520 --> 00:54:39,614
最后我结合时间还有也不多了
1015
00:54:39,620 --> 00:54:44,695
我就结合我们课题组来这几年我们在做的这个中医机器人
1016
00:54:46,320 --> 00:54:48,231
他怎么来跟大模型来进行结合
1017
00:54:48,240 --> 00:54:50,067
我简单做一个案例题
1018
00:54:50,240 --> 00:54:53,480
这是上海中医药大学前期主动跟我们联系
1019
00:54:53,630 --> 00:54:58,142
就相当于说我们怎么来把中医这个场景跟人工智能这个算法的话结合起来
1020
00:54:58,170 --> 00:55:00,218
我们怎么来在机器人这么一个载体上
1021
00:55:00,570 --> 00:55:03,951
或者在其他这个载体上来做我们面向的这个群体
1022
00:55:03,970 --> 00:55:08,884
也就是婴幼儿和中老年以及慢性病的管理好几块
1023
00:55:09,240 --> 00:55:10,824
那么针对医学这一块
1024
00:55:10,830 --> 00:55:13,638
我们怎么来做量化和标准化
1025
00:55:13,640 --> 00:55:16,649
比如说他怎么来判断你这个舌头舌诊的
1026
00:55:16,760 --> 00:55:18,748
怎么来判断你这个面诊的这一块
1027
00:55:19,230 --> 00:55:20,913
还有一块就是知识从哪来
1028
00:55:21,390 --> 00:55:22,440
这就是临床研究
1029
00:55:22,440 --> 00:55:25,366
我们要做一些相关性的研究工作
1030
00:55:25,370 --> 00:55:26,514
比如说他有舌裂纹
1031
00:55:26,720 --> 00:55:31,100
他跟他胃部的这个症候之间的话是不是有关系
1032
00:55:31,410 --> 00:55:33,654
这个关系我觉得就是知识
1033
00:55:34,170 --> 00:55:37,334
这一块我们需要临床医学的支持
1034
00:55:37,970 --> 00:55:41,960
当然还有一块我们要做数据的标注
1035
00:55:41,960 --> 00:55:43,630
大量数据的这一个标志
1036
00:55:43,990 --> 00:55:45,500
在这一块我们是周老师
1037
00:55:45,500 --> 00:55:47,852
我怎么来把望闻问切这一块
1038
00:55:47,860 --> 00:55:48,868
怎么来做面诊
1039
00:55:49,130 --> 00:55:51,700
在做问诊、脉诊、舌诊
1040
00:55:52,120 --> 00:55:55,486
中间这一块就是我们做手做的这个算法
1041
00:55:55,670 --> 00:55:56,755
中医有别于西医
1042
00:55:56,760 --> 00:55:58,264
它是来判断体制的
1043
00:55:58,270 --> 00:55:59,579
国家有9种体质
1044
00:55:59,810 --> 00:56:01,358
我们要形成这个闭环
1045
00:56:01,360 --> 00:56:03,241
相当于说在因为健康管理
1046
00:56:03,730 --> 00:56:05,510
在起居养生这一块的话
1047
00:56:05,510 --> 00:56:09,029
我们怎么来把这个系统来作为这个闭环
1048
00:56:10,160 --> 00:56:12,680
好在做数据采集和构建的时候的话
1049
00:56:12,690 --> 00:56:17,610
我们也用到了光学的这个镜头怎么来进行设计
1050
00:56:17,620 --> 00:56:19,336
包括光源怎么来进行设计
1051
00:56:19,470 --> 00:56:20,658
那我们在面诊这一块
1052
00:56:20,830 --> 00:56:24,574
怎么把人脸这个皮肤模型给它来构建出来
1053
00:56:24,585 --> 00:56:27,185
来判断它有光泽无光泽
1054
00:56:27,390 --> 00:56:30,090
也或者说来判断他到底有神和无神
1055
00:56:30,170 --> 00:56:32,018
这是中医来做的
1056
00:56:32,670 --> 00:56:35,498
它也有别于我们传统的人脸识别
1057
00:56:35,700 --> 00:56:36,810
它面部的话
1058
00:56:36,810 --> 00:56:39,000
它对应的这个心肝、脾、肺、肾
1059
00:56:39,500 --> 00:56:40,828
针对这个区域的话
1060
00:56:40,830 --> 00:56:42,378
我们怎么来进行识别
1061
00:56:43,430 --> 00:56:45,220
也要有相关的算法支撑
1062
00:56:45,590 --> 00:56:46,718
在舌诊这一块的话
1063
00:56:46,720 --> 00:56:49,780
我们怎么来把蛇体精确是分割出来
1064
00:56:49,860 --> 00:56:51,700
那么怎么来判断舌裂纹
1065
00:56:51,910 --> 00:56:53,870
怎么来判断它这个胎质
1066
00:56:54,270 --> 00:56:55,705
包括胎后怎么样
1067
00:56:55,850 --> 00:56:56,605
胎质怎么样
1068
00:56:56,610 --> 00:56:57,485
胎质怎么样
1069
00:56:57,490 --> 00:56:58,435
你舌胖瘦怎么样
1070
00:56:58,440 --> 00:56:59,340
有没有齿龈
1071
00:56:59,340 --> 00:56:59,928
有没有尺
1072
00:57:00,100 --> 00:57:02,592
这是相当于说中医的这一个叛徒
1073
00:57:02,690 --> 00:57:05,147
我们用人工智能这个算法的话
1074
00:57:05,150 --> 00:57:06,855
怎么来辅助它来进行实践
1075
00:57:07,930 --> 00:57:09,169
也是还有机器人
1076
00:57:09,170 --> 00:57:10,185
我们有智能问答
1077
00:57:10,510 --> 00:57:12,598
怎么来结合周一的这个场景
1078
00:57:12,680 --> 00:57:14,748
有针对性的来问这个问题
1079
00:57:14,750 --> 00:57:19,568
这是从从问诊这一个层面当然我们还也有这个脉枕
1080
00:57:19,720 --> 00:57:24,046
那脉怎么来把脉象非常准确的精确的来体现出来
1081
00:57:24,580 --> 00:57:26,725
同时还要把它来判断出来
1082
00:57:26,730 --> 00:57:28,152
这里面的话也不容易
1083
00:57:28,270 --> 00:57:29,296
当然还有声音
1084
00:57:29,990 --> 00:57:31,849
假如哮喘我们也可以听见是吧
1085
00:57:31,930 --> 00:57:33,220
那从中医这一个怎么样
1086
00:57:33,230 --> 00:57:36,477
我们通过声音是不可以来判断是于何时
1087
00:57:36,580 --> 00:57:39,089
这里面我做了我们也做了尝试
1088
00:57:39,410 --> 00:57:41,550
中医讲究这个四针和参
1089
00:57:42,260 --> 00:57:45,110
那么基于前面的这个望闻问切的话
1090
00:57:45,410 --> 00:57:48,938
我们怎么来判断后面它到底是什么样的这个体制
1091
00:57:49,070 --> 00:57:51,605
这里面的话我们知识图谱也好
1092
00:57:52,140 --> 00:57:53,445
相关的这个知识的话
1093
00:57:53,450 --> 00:57:56,336
我们可以结合起来来进行判读
1094
00:57:56,760 --> 00:57:59,344
下面就是起居养生饮食保健这一块的话
1095
00:57:59,350 --> 00:57:59,830
我们形成
1096
00:58:00,570 --> 00:58:01,746
这是16年的时候
1097
00:58:01,750 --> 00:58:04,866
我们也好做了这么一个做面试的这个机器人
1098
00:58:05,300 --> 00:58:06,236
细节的话不提
1099
00:58:06,240 --> 00:58:06,480
好
1100
00:58:06,480 --> 00:58:11,037
后面我们这刚才付春老师也提到说针对这种专才
1101
00:58:11,250 --> 00:58:14,157
针对这种特定领域里面我们大模型怎么来做
1102
00:58:14,930 --> 00:58:17,330
我们也就针对中医这个领域里面的话
1103
00:58:17,330 --> 00:58:18,968
我们怎么来把大模型结合起来
1104
00:58:19,310 --> 00:58:20,686
中医也有这个需求
1105
00:58:20,870 --> 00:58:21,920
当然也有挑战
1106
00:58:22,130 --> 00:58:25,664
比如说它的规范化和标准化这个程度还不高
1107
00:58:25,670 --> 00:58:28,577
刚才就提到了说量化标准化还不够是吧
1108
00:58:28,730 --> 00:58:29,724
那么这里面的话
1109
00:58:29,950 --> 00:58:32,537
我们怎么来应对这么一个挑战
1110
00:58:32,990 --> 00:58:38,028
但是他的中医知识的复杂性和专业性又有要求很高
1111
00:58:38,820 --> 00:58:43,945
而且要求在领对领域知识的逻辑推理能力的话要求比较强
1112
00:58:44,080 --> 00:58:47,244
那我们怎么来做在这一块来对啊
1113
00:58:47,590 --> 00:58:51,425
那么这是我们整个的技术路径
1114
00:58:51,610 --> 00:58:55,984
从推理到处方到决策
1115
00:58:55,990 --> 00:59:01,015
也就是说我们怎么来通过这个大模型怎么来做处方的生成
1116
00:59:01,020 --> 00:59:03,671
怎么来做诊断决策的生成
1117
00:59:03,876 --> 00:59:05,712
我们来做了一些评测
1118
00:59:05,916 --> 00:59:07,344
通过对ChatGPT也好
1119
00:59:07,344 --> 00:59:10,200
针对于目前市面上的这个大模型
1120
00:59:10,430 --> 00:59:12,509
也他也肯定存在一些问题
1121
00:59:12,510 --> 00:59:14,567
你比如说领域知识还不够
1122
00:59:15,040 --> 00:59:17,710
他的推理能力还比较差
1123
00:59:17,990 --> 00:59:18,180
好
1124
00:59:18,480 --> 00:59:21,876
这也是我们可以提升的空间
1125
00:59:22,220 --> 00:59:23,885
我们也做了一些测评
1126
00:59:24,020 --> 00:59:25,684
但是他比如说ChatGPT
1127
00:59:25,689 --> 00:59:29,441
它也是具备一定的中医学的常识
1128
00:59:29,445 --> 00:59:33,618
也可以来开一些中草药的处方
1129
00:59:34,130 --> 00:59:36,572
也可以来做一些药物剂量
1130
00:59:36,580 --> 00:59:37,900
我们来做一些判读
1131
00:59:37,990 --> 00:59:40,356
这里面的话有有一定的这个能力
1132
00:59:40,360 --> 00:59:42,048
当然还有一些问题
1133
00:59:42,460 --> 00:59:45,250
但准确率的话我们也做了这个测评
1134
00:59:45,460 --> 00:59:46,290
有啊
1135
00:59:46,430 --> 00:59:46,898
下面的话
1136
00:59:46,900 --> 00:59:48,517
我们基于存在的这个问题
1137
00:59:48,520 --> 00:59:49,546
我们怎么来做
1138
00:59:49,790 --> 00:59:52,750
我们就做了一些比如说常规的这个指令数据集
1139
00:59:53,020 --> 00:59:55,270
我们也是调用大语言模型知识的话
1140
00:59:55,270 --> 00:59:58,840
怎么来快速生成一些面向中医的这里面多样性的
1141
00:59:58,850 --> 01:00:01,760
还有具有创造性的一些指令这一块
1142
01:00:02,270 --> 01:00:05,486
这是我们开源的这么一个大语言模型
1143
01:00:05,500 --> 01:00:07,102
仲景中医大语言模型
1144
01:00:07,650 --> 01:00:12,754
名字处置张仲景他讲究这个方和正之间的这种对称
1145
01:00:13,470 --> 01:00:14,220
这里面的话
1146
01:00:14,220 --> 01:00:16,956
我们也做了几个模块的创新
1147
01:00:16,960 --> 01:00:19,646
比如说针对这种多场景的一个中医诊疗
1148
01:00:19,850 --> 01:00:26,816
我们怎么来做指令构建策略的这种也就构建了一些情景和故事
1149
01:00:26,840 --> 01:00:29,870
用影视编码知识信息的话我们来做
1150
01:00:29,960 --> 01:00:32,426
同时基于这个prompt这个模块的话
1151
01:00:32,520 --> 01:00:39,052
我们这里面是针对这种妇科专科来做了一些场景构建
1152
01:00:39,270 --> 01:00:43,158
也是来强化这一种大模型的逻辑推理能力
1153
01:00:43,980 --> 01:00:45,970
这里面是举了几个例子
1154
01:00:46,300 --> 01:00:48,588
时间原因也时间也差不多到了
1155
01:00:49,230 --> 01:00:52,290
这是我们在常识诊断数据治理的话
1156
01:00:52,300 --> 01:00:54,670
比如说基于中医名词这个解释的话
1157
01:00:55,240 --> 01:00:58,600
他这里面我们就要构建了8万条的这一种尝试
1158
01:00:58,610 --> 01:00:59,350
质量数据
1159
01:00:59,560 --> 01:01:03,946
我们也做了一些评估标准的构建五维度
1160
01:01:04,140 --> 01:01:08,004
从客观性、逻辑性、专业性、准确性和完整性这一个层面
1161
01:01:08,140 --> 01:01:10,802
我们来打分来判别这一块
1162
01:01:11,350 --> 01:01:13,350
针对GPD4也好
1163
01:01:13,570 --> 01:01:14,810
针对我们小同行业
1164
01:01:14,810 --> 01:01:15,668
包括文心一言
1165
01:01:15,670 --> 01:01:19,325
我们在中医这一个诊断处方这一块的话
1166
01:01:19,340 --> 01:01:21,023
我们来做了这个测评
1167
01:01:21,200 --> 01:01:27,923
当然这里面也用到了一些有临床经验的这一个医生来进行评判
1168
01:01:28,720 --> 01:01:30,295
下面也举个例子
1169
01:01:30,300 --> 01:01:33,330
比如说我们问针对于这个病情的话
1170
01:01:33,330 --> 01:01:34,870
我们有问题
1171
01:01:35,730 --> 01:01:39,820
那么我们所做的这个处方的话是什么
1172
01:01:40,220 --> 01:01:42,022
华佗的话GPT4的话
1173
01:01:42,031 --> 01:01:46,365
他们只在这一块我们也做了这个实例的测试和对比
1174
01:01:46,558 --> 01:01:49,992
当然还有一些常规症状怎么来进行咨询
1175
01:01:50,380 --> 01:01:55,755
比如说针对这种心痛戒备背痛切身这一块怎么来进行治疗
1176
01:01:55,910 --> 01:01:59,344
不同的这个大模型它体现出来也不一样
1177
01:01:59,650 --> 01:02:01,714
这里面的话我们也做了对比
1178
01:02:01,880 --> 01:02:04,300
包括一些复杂的诊疗决策
1179
01:02:04,540 --> 01:02:07,070
怎么来进行推理的这一块
1180
01:02:07,230 --> 01:02:09,120
我们也做了对比
1181
01:02:10,070 --> 01:02:12,190
我简单做一下小结
1182
01:02:12,650 --> 01:02:15,740
相当于说针对这种具体的垂直领域
1183
01:02:15,750 --> 01:02:16,590
比如说中医
1184
01:02:17,090 --> 01:02:21,374
是我们来基于一些开源的这个大模型的话
1185
01:02:21,390 --> 01:02:25,988
我们是否可以来做一些垂直领域的这边构建
1186
01:02:26,190 --> 01:02:28,519
这里面的话我们也做了一些有益的尝试
1187
01:02:28,520 --> 01:02:33,028
我们基于10B以下这种开源的模型就可以来做一些辩证的处方
1188
01:02:33,360 --> 01:02:36,550
包括决策的这方面的推理
1189
01:02:36,930 --> 01:02:39,625
这一块我们做了一些尝试
1190
01:02:39,630 --> 01:02:39,940
好
1191
01:02:39,940 --> 01:02:42,124
最后做一下总结
1192
01:02:42,470 --> 01:02:44,878
你说模型就是知识
1193
01:02:45,530 --> 01:02:47,779
尤其是大模型出来以后的说话
1194
01:02:48,430 --> 01:02:51,592
我们怎么来跟本体机器人也是一个载体
1195
01:02:51,600 --> 01:02:53,508
跟本体结合起来的时候的话
1196
01:02:53,510 --> 01:02:55,058
怎么来服务我们人类
1197
01:02:55,060 --> 01:02:57,112
在这一块我们是大有可为了
1198
01:02:57,330 --> 01:02:59,130
等于说两块
1199
01:02:59,130 --> 01:03:00,320
一块是本体
1200
01:03:00,460 --> 01:03:01,380
一块是大脑
1201
01:03:01,580 --> 01:03:02,987
这两个结合起来
1202
01:03:02,990 --> 01:03:05,897
我们是不是也可以来做一些能像人一样
1203
01:03:06,800 --> 01:03:09,775
能像低等的动物以上所具备的这个智能
1204
01:03:09,830 --> 01:03:12,870
是不是最终我们能达到这个通用智能这个水平
1205
01:03:13,090 --> 01:03:14,878
我觉得我们也可以拭目以待
1206
01:03:15,180 --> 01:03:15,460
好好
1207
01:03:16,500 --> 01:03:16,988
谢谢大家
1208
01:03:16,990 --> 01:03:18,120
我这个报告就到这里
1209
01:03:21,800 --> 01:03:22,440
好的
1210
01:03:22,760 --> 01:03:24,982
我们再次感谢张文强老师
1211
01:03:25,510 --> 01:03:26,998
然后因为时间原因
1212
01:03:27,000 --> 01:03:28,716
我看评论区里面有很多问题
1213
01:03:28,720 --> 01:03:31,180
我就留一个我就问一个问题
1214
01:03:31,410 --> 01:03:32,430
就留一个问题
1215
01:03:32,710 --> 01:03:35,175
就是看到最后我看到的最近的一个问题
1216
01:03:35,190 --> 01:03:38,102
就是问咱们的五个方面的评价
1217
01:03:38,110 --> 01:03:40,702
是不是人工来进行评价的对
1218
01:03:40,920 --> 01:03:41,328
张老师
1219
01:03:41,860 --> 01:03:42,180
对
1220
01:03:42,370 --> 01:03:49,343
我们也是他通过有经验的医生来做一些评测
1221
01:03:51,580 --> 01:03:51,870
好的
1222
01:03:52,000 --> 01:03:53,935
我们再次感谢张老师
1223
01:03:55,180 --> 01:04:07,038
我们接下来就是第三个报告第三个报告是上海交大卢策武老师带来的具身智能感知想象执行PIE方案与具身大
1224
01:04:07,038 --> 01:04:08,006
模型探索
1225
01:04:08,900 --> 01:04:11,196
也是对罗老师做一个简短的介绍
1226
01:04:11,200 --> 01:04:15,268
哈罗老师是上海交通大学教授博士生导师
1227
01:04:15,800 --> 01:04:18,568
2016年海外高层次青年引进人才
1228
01:04:18,810 --> 01:04:25,196
2018年被麻省理工科技评论评为35位35岁以下的中国科技精英
1229
01:04:26,450 --> 01:04:29,060
2019年获得求是杰出青年学者
1230
01:04:29,340 --> 01:04:31,996
2020年获上海市科技进步特等奖
1231
01:04:32,410 --> 01:04:34,594
2022年获教育部青年科技奖
1232
01:04:34,890 --> 01:04:37,520
也是RIOS最佳论文
1233
01:04:37,570 --> 01:04:41,770
2023年获机器人顶会RSS最佳论文提名奖
1234
01:04:42,070 --> 01:04:49,990
科技科学探索奖以第一作者和通讯作者也发表了在自然和自然机器智能上面发表了多篇文章
1235
01:04:50,540 --> 01:04:56,195
也是担任science正刊和nature子刊以及excel子刊等期刊的申报的人
1236
01:04:57,200 --> 01:05:00,628
卢老师的研究兴趣包括具身智能、机器视觉
1237
01:05:01,100 --> 01:05:02,760
下面有请卢老师
1238
01:05:06,990 --> 01:05:08,244
谢谢张老师介绍
1239
01:05:08,250 --> 01:05:09,630
张老师能看到我的PPT吗
1240
01:05:11,600 --> 01:05:12,188
没问题
1241
01:05:12,450 --> 01:05:12,920
好
1242
01:05:12,920 --> 01:05:13,568
那就开始
1243
01:05:14,010 --> 01:05:14,678
谢谢大家
1244
01:05:15,210 --> 01:05:18,255
然后我今天分享的聚生智能的我们的PIE方案
1245
01:05:19,080 --> 01:05:23,364
当然说我们做居然智能肯定是需要做通用机器人
1246
01:05:23,370 --> 01:05:24,870
这个也不用我们多讲了
1247
01:05:24,870 --> 01:05:29,858
如果今天过来听着报告人肯定也是知道我们想做一些什么样的事情
1248
01:05:30,370 --> 01:05:34,507
我是认为我个人认为是这部电影里面所有的性能
1249
01:05:34,510 --> 01:05:37,690
我们都能是在可见的未来能做到的
1250
01:05:38,020 --> 01:05:42,440
那么居然智能的话其实是我们的祖师爷屠灵很早就提出来了
1251
01:05:42,670 --> 01:05:49,468
他认为就是身体的所有的地球上所有的生物都是由身体作为事件而行产生的
1252
01:05:49,570 --> 01:05:52,082
所以这些智能是具有身体体验的智能
1253
01:05:52,560 --> 01:05:54,331
这也在认知学上得到印证
1254
01:05:54,340 --> 01:05:58,596
比如说这个主动的猫它在做它是有主动探索
1255
01:05:58,610 --> 01:06:00,820
这被动猫让他看到视角一样的
1256
01:06:00,820 --> 01:06:03,396
但是把它放下之后他又缺乏行为能力
1257
01:06:03,500 --> 01:06:05,463
所以说这也是居约症的重要性
1258
01:06:06,110 --> 01:06:10,114
我们从2016年之前就一直在探索一种什么样的一个通用框架
1259
01:06:10,310 --> 01:06:12,134
而不是一个任务一个是去做
1260
01:06:12,690 --> 01:06:14,080
这样的难度就会太大了
1261
01:06:14,080 --> 01:06:16,940
而是我们通用的任务可以解决大量的不一样的任务
1262
01:06:17,540 --> 01:06:19,892
就以此建立了一个叫PIE框架
1263
01:06:20,340 --> 01:06:23,348
我们这个是对应人类的多个认知模块
1264
01:06:23,350 --> 01:06:26,104
比如说我们人对首先肯定要感知这个世界
1265
01:06:26,300 --> 01:06:28,385
那么真正的感知应该是什么样子呢
1266
01:06:28,390 --> 01:06:30,850
它肯定是跟视觉是有很大的不一样
1267
01:06:31,000 --> 01:06:32,424
第二的话就是想象
1268
01:06:32,600 --> 01:06:33,410
就是想象的话
1269
01:06:33,510 --> 01:06:36,270
我们其实我们是脑子里会把这个世界想象一遍
1270
01:06:36,280 --> 01:06:37,240
大家会做什么
1271
01:06:37,370 --> 01:06:38,410
进行个规划
1272
01:06:38,410 --> 01:06:39,586
最后是拒绝执行
1273
01:06:40,040 --> 01:06:41,921
前两个是完成大脑的功能
1274
01:06:41,930 --> 01:06:43,550
是对世界的理解和抽象
1275
01:06:43,770 --> 01:06:45,360
第二个是对小脑的功能
1276
01:06:45,360 --> 01:06:48,168
是完成了执行的想象和执行
1277
01:06:48,410 --> 01:06:48,630
好
1278
01:06:48,700 --> 01:06:51,380
我们来看一下自身感知
1279
01:06:52,530 --> 01:06:54,150
首先我们自身感知的话
1280
01:06:54,150 --> 01:06:55,319
我们会感觉到说
1281
01:06:55,950 --> 01:06:57,534
感知无非就是检测的物体
1282
01:06:57,540 --> 01:06:57,820
是吧
1283
01:06:58,190 --> 01:06:59,590
那么其实不止于此
1284
01:06:59,660 --> 01:07:00,992
我们需要把这个物体
1285
01:07:01,000 --> 01:07:02,376
大家可以看到是说
1286
01:07:03,040 --> 01:07:06,940
我们我们要把一个图像来检测出来它这个盒子这远远不够的
1287
01:07:06,940 --> 01:07:09,280
我们知道这里是一个轴可以翻
1288
01:07:09,510 --> 01:07:12,569
这是可以装的这才是最完整的一个物理解释
1289
01:07:12,730 --> 01:07:15,671
这是人类天生所具有的这样的一种智能
1290
01:07:15,860 --> 01:07:18,869
而这种智能能不能被机器学习学习到呢
1291
01:07:19,280 --> 01:07:21,236
其实在在2022年的时候
1292
01:07:21,240 --> 01:07:22,056
迪克曼也说了
1293
01:07:22,060 --> 01:07:23,836
这是其实深度学习做不到的
1294
01:07:23,940 --> 01:07:26,220
因为他不能够理解那个物理的常识
1295
01:07:26,390 --> 01:07:28,028
那么我们就带来一个科学问题
1296
01:07:28,040 --> 01:07:31,145
如何通用的去估计物理的操作常识
1297
01:07:31,580 --> 01:07:32,777
它挑战在哪里呢
1298
01:07:32,780 --> 01:07:36,437
它的挑战其实第一个挑战就是大规模的数据怎么获得
1299
01:07:36,590 --> 01:07:38,750
我们光标个框都会标的累死
1300
01:07:38,750 --> 01:07:41,990
这是要在三维的情况下标注动态的这种的状态
1301
01:07:42,500 --> 01:07:43,360
这怎么做呢
1302
01:07:43,520 --> 01:07:45,830
这个东西就是一个很艰难的问题
1303
01:07:46,680 --> 01:07:48,708
为此我们就提出一种新的思路
1304
01:07:48,710 --> 01:07:51,537
叫做手和常识的对偶空间
1305
01:07:51,720 --> 01:07:54,333
就是说我们会大胆的分析发现
1306
01:07:55,340 --> 01:07:57,724
当你知道这个物体的物理常识的时候
1307
01:07:57,730 --> 01:07:59,422
我们是知道手是怎么操作的
1308
01:07:59,580 --> 01:08:00,890
而知道手是怎么操作呢
1309
01:08:01,160 --> 01:08:03,554
我们反过来知道这个物体的姿势是什么样子
1310
01:08:03,560 --> 01:08:09,586
这就是意味着说一个说我们通过这种方法可以去分析
1311
01:08:09,770 --> 01:08:13,542
通过这种手的操作能够去获得这样的一个物体的操作
1312
01:08:13,770 --> 01:08:15,218
那这有什么好处呢
1313
01:08:15,670 --> 01:08:18,114
首先就是它是能够自动标注的
1314
01:08:18,570 --> 01:08:23,819
因为我们是通过AI去分析手来去通过去去得到在上面的知识载体
1315
01:08:24,020 --> 01:08:25,298
第二它是可规模化的
1316
01:08:25,300 --> 01:08:27,556
因为手的操作它非常可扩展
1317
01:08:28,030 --> 01:08:30,578
第三就是说的数据是很自然的
1318
01:08:30,590 --> 01:08:31,910
手的操作数据是很自然的
1319
01:08:31,910 --> 01:08:33,174
它一定是最准确的
1320
01:08:33,320 --> 01:08:36,057
你比在三维试验区模型都是比较准确的
1321
01:08:36,340 --> 01:08:36,521
好
1322
01:08:36,521 --> 01:08:40,157
为此我们就做出了一系列的视觉上的工作
1323
01:08:40,158 --> 01:08:41,244
当然时间有限
1324
01:08:41,249 --> 01:08:42,878
我们就不一一列举了
1325
01:08:42,885 --> 01:08:48,691
我们分别在ICCV、ICCPR和i CVPR等等一系列的文章发表了这样的一个内容
1326
01:08:48,890 --> 01:08:50,290
这是我们的基本的成果
1327
01:08:50,290 --> 01:08:54,358
就是我们能够去解析这个物体的操作常识
1328
01:08:56,820 --> 01:08:58,640
我们来看看我们做的一个系统
1329
01:08:58,790 --> 01:09:00,558
这个系统是一套软硬件接系统
1330
01:09:01,000 --> 01:09:02,988
这个系统的话我们能够通过把人
1331
01:09:03,130 --> 01:09:04,330
通过那个行
1332
01:09:04,330 --> 01:09:05,308
通过视觉系统
1333
01:09:05,310 --> 01:09:07,411
我把它解析成为一个人体
1334
01:09:07,500 --> 01:09:08,646
还有这些物体
1335
01:09:08,650 --> 01:09:10,510
最后我们把它变成个灵巧手
1336
01:09:10,750 --> 01:09:12,379
通过这灵巧手的操作
1337
01:09:12,380 --> 01:09:14,424
我们能够推断出这个物体的概念
1338
01:09:14,430 --> 01:09:15,753
这样形成一个概念流
1339
01:09:15,930 --> 01:09:19,374
也就是说我们可以很轻易的去标注大规模的数据
1340
01:09:19,620 --> 01:09:22,164
在这样的大规模的知识数据的加持下
1341
01:09:22,170 --> 01:09:23,958
我们就能够训出一个大模型
1342
01:09:24,120 --> 01:09:25,587
这个模型是这个物体
1343
01:09:25,720 --> 01:09:27,178
然后就是物体的点云
1344
01:09:27,180 --> 01:09:29,140
然后我们就能够知道物体的知识
1345
01:09:29,320 --> 01:09:30,400
这是操作知识
1346
01:09:30,400 --> 01:09:33,408
而不是是六维模六维六维六六维姿态
1347
01:09:34,050 --> 01:09:35,842
大家可能会看起来像个六维姿态
1348
01:09:35,850 --> 01:09:36,640
其实不是的
1349
01:09:36,830 --> 01:09:38,447
你看这个红色的是表示轴
1350
01:09:38,930 --> 01:09:41,723
这个红色的这个上上下下表示它是可以装的
1351
01:09:41,730 --> 01:09:42,430
它是空的
1352
01:09:43,410 --> 01:09:45,566
不知道能看到我的鼠标吗
1353
01:09:45,780 --> 01:09:48,190
大家可以的
1354
01:09:48,190 --> 01:09:49,414
罗老师能看到那就好
1355
01:09:50,300 --> 01:09:53,520
你看这个事情就是是可以拉起来
1356
01:09:53,650 --> 01:09:55,582
其实我们是不只标注物体的姿态
1357
01:09:55,590 --> 01:09:57,493
我们标注它如何被操作的
1358
01:09:58,230 --> 01:10:00,630
然后的话我们会讲一个更难的问题
1359
01:10:00,640 --> 01:10:02,200
就是你刚讲的我们都是钢体
1360
01:10:02,330 --> 01:10:03,482
那么柔性体怎么办呢
1361
01:10:03,770 --> 01:10:05,370
你看这个事情的柔性体
1362
01:10:05,370 --> 01:10:09,129
我们照样可以用VR眼镜和AR眼镜来去做操作
1363
01:10:09,130 --> 01:10:11,944
但是唯一不同的是我们是用一个非常好的仿真器
1364
01:10:12,080 --> 01:10:13,411
这个仿真器能做到这一点
1365
01:10:13,420 --> 01:10:14,950
能够非常柔性的操作
1366
01:10:15,830 --> 01:10:19,460
这个事情的话就是说我们给一个柔性的物体做操作
1367
01:10:19,880 --> 01:10:21,856
我们就可以一点一点的叠叠叠
1368
01:10:21,990 --> 01:10:22,890
然后叠完之后
1369
01:10:23,000 --> 01:10:25,686
我放在一个真机里面去蝶那就会很舒服
1370
01:10:25,870 --> 01:10:31,078
等于说我们是在第一个阶段用人的操作去去获取这个物体的知识
1371
01:10:31,090 --> 01:10:33,685
这个物体的知识指导我们的做操作
1372
01:10:33,690 --> 01:10:36,461
然后我们的真机上去调整就能做出来了
1373
01:10:36,590 --> 01:10:41,140
这是我们的第一次我们能做到对任意的物体任意的换位置
1374
01:10:41,160 --> 01:10:42,750
那位置要去做
1375
01:10:42,750 --> 01:10:43,893
因为这个难度是什么
1376
01:10:43,900 --> 01:10:46,070
因为他的知识是更加的不确定性
1377
01:10:46,170 --> 01:10:46,918
为什么呢
1378
01:10:47,040 --> 01:10:50,528
因为你想想他的姿态是完全不一样的
1379
01:10:50,540 --> 01:10:52,286
然后你是一糊作一团
1380
01:10:52,470 --> 01:10:59,517
我们是能对任意的物体任意的任意的物体和任意的角度任意的衣服
1381
01:10:59,530 --> 01:11:03,556
任意的角度和任意的折叠那个混乱程度从零开始叠
1382
01:11:03,880 --> 01:11:06,630
这个事情是目前是第一次
1383
01:11:06,640 --> 01:11:08,620
第一可能是第一次能做到这样水平
1384
01:11:10,040 --> 01:11:13,175
那么我们有了我们有了这样的一个模型之后
1385
01:11:13,380 --> 01:11:17,538
我们就可以来上面去做交互的进一步的交互感知
1386
01:11:17,550 --> 01:11:21,110
因为我们觉得具身感知它一定要包括交互感知
1387
01:11:21,560 --> 01:11:25,214
首先我们是有一个这样的一个初步的模型
1388
01:11:25,430 --> 01:11:26,774
这个模型我们会有一个action
1389
01:11:26,970 --> 01:11:28,586
那个action我们通过仿真汽车
1390
01:11:28,590 --> 01:11:30,410
仿真它下一帧应该长什么样子
1391
01:11:30,800 --> 01:11:32,896
当然说你会问这个仿真器不好怎么样
1392
01:11:33,070 --> 01:11:34,522
这是我们下一个阶段要讲的
1393
01:11:34,530 --> 01:11:36,030
我们一个很好的仿真器
1394
01:11:37,120 --> 01:11:40,639
我们我们是能够预知下个阶段的点源应该什么样子的
1395
01:11:40,830 --> 01:11:42,656
如果这两个一剪剪完之后
1396
01:11:42,660 --> 01:11:44,250
我们就可以对他知道一个loss
1397
01:11:44,260 --> 01:11:47,236
这个loss我们就可以求他的发展
1398
01:11:47,740 --> 01:11:50,022
等于说一个正确的好的一个物体
1399
01:11:50,030 --> 01:11:51,625
一定会是两个是很接近的
1400
01:11:51,630 --> 01:11:53,996
如果这两个不接近证明什么呢
1401
01:11:54,280 --> 01:11:57,244
如果这两个不接近证明说你这个物体做错了
1402
01:11:57,260 --> 01:11:58,898
如果我们去压缩这个loss
1403
01:11:59,400 --> 01:12:00,380
压缩他们的误差
1404
01:12:00,380 --> 01:12:02,539
最后这个物体就不可以变成被我们翻看
1405
01:12:02,550 --> 01:12:04,404
就是越越算是越准的
1406
01:12:05,790 --> 01:12:06,969
这是我们的中间结果
1407
01:12:07,310 --> 01:12:09,676
就是你看一下我们的测试的操作
1408
01:12:09,820 --> 01:12:11,206
我们的这个情况的话
1409
01:12:11,210 --> 01:12:12,450
我们是会越来越准
1410
01:12:13,030 --> 01:12:14,240
这是我们的中间的结果
1411
01:12:14,240 --> 01:12:16,220
一开始我们不知道这是个什么东西
1412
01:12:16,430 --> 01:12:19,038
然后通过我们的那个我们的操作之后
1413
01:12:19,050 --> 01:12:20,670
慢慢知道这是一个什么东西
1414
01:12:20,670 --> 01:12:21,820
包括拉柜子
1415
01:12:22,070 --> 01:12:24,484
他一开始也不知道柜子是怎么个操作法
1416
01:12:24,750 --> 01:12:26,622
他一开始可能估计是很糟糕的
1417
01:12:26,630 --> 01:12:27,568
比如像这个样子
1418
01:12:27,700 --> 01:12:28,606
但他一拉之后
1419
01:12:28,610 --> 01:12:30,122
马上它就是很稳定了
1420
01:12:30,130 --> 01:12:31,775
因为它的轴方向
1421
01:12:31,780 --> 01:12:36,200
它它的怕方向马上就被被通过这样的一个操作把它切割对了
1422
01:12:37,780 --> 01:12:38,290
好
1423
01:12:38,560 --> 01:12:40,296
这是我们的这一个进一步的工作
1424
01:12:40,300 --> 01:12:41,460
我们做了个更难的东西
1425
01:12:41,460 --> 01:12:41,990
穿针
1426
01:12:42,200 --> 01:12:49,400
我们的procession和这感知和交互是融为一体的这是一个一个一个相当于一个是我们的看各个角度
1427
01:12:49,410 --> 01:12:51,867
你看穿针我们是各个角不能穿
1428
01:12:51,870 --> 01:12:53,575
为什么要各个角度能穿呢
1429
01:12:54,280 --> 01:12:55,720
因为它这样会挡住
1430
01:12:55,800 --> 01:12:57,360
你必须找到最好的孔位
1431
01:12:57,360 --> 01:12:58,688
你才能把它穿上去
1432
01:12:59,040 --> 01:13:03,115
这篇论文也获得了机器人顶会ISS的最佳系统论文提名
1433
01:13:03,280 --> 01:13:08,900
这是唯一的当年唯一的华人团队获得ISS最佳系统论文提名
1434
01:13:11,830 --> 01:13:12,610
好
1435
01:13:15,460 --> 01:13:18,038
就是下一步我们要讲那个巨声想象了
1436
01:13:18,520 --> 01:13:19,744
具体想象是这样子的
1437
01:13:20,400 --> 01:13:22,220
我们其实要引入一个科学问题
1438
01:13:22,220 --> 01:13:26,996
就是说如何引入物理常识来降低开盘绩效的机器人决策
1439
01:13:27,210 --> 01:13:29,134
那么我们就需要机器人的仿真
1440
01:13:29,140 --> 01:13:29,640
为什么呢
1441
01:13:29,640 --> 01:13:32,565
因为它会把它抽象到一个机器
1442
01:13:32,570 --> 01:13:34,073
抽象到它的物理知识
1443
01:13:34,080 --> 01:13:34,848
通过仿真
1444
01:13:35,360 --> 01:13:38,148
其实这是个全新的科学和工程复合问题
1445
01:13:38,350 --> 01:13:39,510
它的是物理
1446
01:13:39,510 --> 01:13:40,894
它需要它为什么难
1447
01:13:40,900 --> 01:13:41,878
它需要物理转
1448
01:13:42,170 --> 01:13:44,186
同时它又耦合人工智能系统
1449
01:13:44,190 --> 01:13:44,928
还有快
1450
01:13:46,650 --> 01:13:48,126
那么我们来看看仿真
1451
01:13:48,270 --> 01:13:50,082
现在世界上仿真有有两大类
1452
01:13:50,090 --> 01:13:51,238
一类是工业仿真
1453
01:13:51,410 --> 01:13:53,450
工业仿真的特点是它比较慢
1454
01:13:53,460 --> 01:13:55,649
但是它的它虽然是比较慢
1455
01:13:55,650 --> 01:13:56,520
但是它很准
1456
01:13:56,520 --> 01:13:57,900
物理上很准
1457
01:13:58,270 --> 01:14:00,448
另一个是他的速度非常快
1458
01:14:00,450 --> 01:14:01,486
但是物理上不准
1459
01:14:02,010 --> 01:14:03,844
在这种情况下就存在一个生态位
1460
01:14:03,850 --> 01:14:07,750
就是物理机器人的AI仿真IS university
1461
01:14:08,120 --> 01:14:09,682
这是我们开发的一个软件
1462
01:14:09,960 --> 01:14:11,874
这个事情也欢迎大家使用
1463
01:14:11,880 --> 01:14:12,920
我们是开源的
1464
01:14:15,060 --> 01:14:15,530
好
1465
01:14:16,710 --> 01:14:21,678
那么它的我们来看它的难度就是如果你要是从从游戏仿真软件
1466
01:14:21,680 --> 01:14:24,423
它是把刚体和柔性体分开建模
1467
01:14:24,430 --> 01:14:25,669
为什么这样会快
1468
01:14:25,830 --> 01:14:27,294
那它就很容易穿模
1469
01:14:27,430 --> 01:14:28,486
而我们是不一样的
1470
01:14:28,610 --> 01:14:34,217
我们是把柔性体、液体、气体、惯性体全部写成个大方程
1471
01:14:34,430 --> 01:14:36,922
然后再产生了非常多的近似算法
1472
01:14:36,930 --> 01:14:38,862
再用机器学习算它的初始值
1473
01:14:39,350 --> 01:14:44,802
这个事情的话你看一下就能够刚体柔性体能够很好的去联合的优化
1474
01:14:46,140 --> 01:14:46,620
好
1475
01:14:46,900 --> 01:14:51,436
的结果是我们比那个最好的工业引擎在毫米级别的误差
1476
01:14:51,440 --> 01:14:53,130
而我们的速度能达到400米
1477
01:14:53,140 --> 01:14:55,129
每能够提升400倍
1478
01:14:55,130 --> 01:14:57,130
实在真正的实现了实时
1479
01:14:58,390 --> 01:14:59,760
这是我们的一个中间结果
1480
01:15:05,630 --> 01:15:06,568
大家可以看一下
1481
01:15:07,020 --> 01:15:08,268
这我们的软件架构
1482
01:15:08,270 --> 01:15:10,762
我们这边录发在ISS2023
1483
01:15:11,310 --> 01:15:14,996
这是我们的整个家用环境下的一个光学模拟
1484
01:15:15,230 --> 01:15:17,518
然后我们有很多的快速的仿真
1485
01:15:17,530 --> 01:15:18,460
高速的仿真
1486
01:15:18,620 --> 01:15:20,570
比如这个柔性物体的高速仿真
1487
01:15:21,060 --> 01:15:23,160
比如说这个书本的柔性仿真
1488
01:15:23,380 --> 01:15:27,043
还有就比如说什么呢
1489
01:15:27,050 --> 01:15:30,209
比如说那个湿润漏透气度一些
1490
01:15:30,380 --> 01:15:32,060
包括我们能够对于柔性
1491
01:15:32,060 --> 01:15:33,229
对于流体的仿真
1492
01:15:33,230 --> 01:15:36,524
比如说我们用力学去去仿真这个粘性物体
1493
01:15:36,730 --> 01:15:40,594
真实的情况下和虚拟的情况下的差误差在十分之以内
1494
01:15:40,780 --> 01:15:42,570
这难度是相当是很大的
1495
01:15:42,780 --> 01:15:45,855
包括我们能够仿真各种真实的触觉
1496
01:15:46,860 --> 01:15:48,260
他的比如说他会有三次
1497
01:15:48,260 --> 01:15:50,492
比如说第一个首先是光照了
1498
01:15:50,700 --> 01:15:52,765
然后下面是点云
1499
01:15:53,120 --> 01:15:56,294
就等于说在你在机械系统里面完全可以做自己的实验
1500
01:15:58,870 --> 01:15:59,930
然后还有是触觉
1501
01:16:03,540 --> 01:16:04,090
力觉
1502
01:16:08,260 --> 01:16:08,770
好
1503
01:16:09,370 --> 01:16:11,479
然后我们还有非常好的interface
1504
01:16:12,010 --> 01:16:13,326
我们是一个python版本的
1505
01:16:13,330 --> 01:16:14,188
欢迎大家使用
1506
01:16:14,190 --> 01:16:16,444
现在要有十多家高校在上面使用
1507
01:16:16,770 --> 01:16:18,618
包括康奈尔等高效
1508
01:16:19,250 --> 01:16:20,858
然后还有VR眼镜
1509
01:16:20,860 --> 01:16:23,387
这直接可以把你们带上VR眼镜就可以用了
1510
01:16:23,590 --> 01:16:24,619
这是我们的融化
1511
01:16:24,620 --> 01:16:26,840
然后这是一个风洞实验
1512
01:16:27,410 --> 01:16:31,210
就比如说我们我们甚至能够跟真实的风去比较
1513
01:16:31,410 --> 01:16:33,608
我们记录他真实的风的吹吹情况
1514
01:16:33,790 --> 01:16:34,600
这是真实的
1515
01:16:34,600 --> 01:16:35,350
这是仿真的
1516
01:16:35,350 --> 01:16:38,280
我们的区别是其实是几乎很小
1517
01:16:40,680 --> 01:16:41,200
好
1518
01:16:42,590 --> 01:16:46,090
这个事情就是说我们的我们在公开的论文表明
1519
01:16:46,480 --> 01:16:48,628
我们比现在的那些仿真引擎
1520
01:16:48,630 --> 01:16:51,444
在以下的那个事情是我们做到了
1521
01:16:51,450 --> 01:16:52,330
率先做到的
1522
01:16:52,330 --> 01:16:53,474
也是现在我们有的
1523
01:16:53,670 --> 01:16:55,263
他们可能还暂时没有
1524
01:16:55,740 --> 01:16:57,000
比如固态项
1525
01:16:57,520 --> 01:16:59,305
那个那个固液相
1526
01:16:59,310 --> 01:16:59,930
还有气固相
1527
01:17:00,090 --> 01:17:01,560
还有热传导等等
1528
01:17:03,640 --> 01:17:06,472
我来讲一个这个东西为什么那么重要
1529
01:17:06,480 --> 01:17:08,895
就是说我们现在的一个机器人的话
1530
01:17:08,900 --> 01:17:14,450
我们是希望对未来的物体能够我们的知道认知它它的所有的物理属性
1531
01:17:14,840 --> 01:17:16,452
那么我们就需要一个仿真引擎
1532
01:17:16,460 --> 01:17:18,272
比如说我们现在去抓这个布
1533
01:17:18,430 --> 01:17:20,734
我们可以对这个布有一个初步的仿真
1534
01:17:21,100 --> 01:17:22,915
但这个仿真肯定是不准的
1535
01:17:23,320 --> 01:17:25,288
因为我们不知道它的整个的物理参数
1536
01:17:25,300 --> 01:17:27,196
就比如说它的材质
1537
01:17:27,380 --> 01:17:29,900
它往这边拉的做杨氏模量压
1538
01:17:29,900 --> 01:17:30,650
这个坡松量
1539
01:17:30,650 --> 01:17:31,856
还有它的高度
1540
01:17:31,860 --> 01:17:32,408
还有切度
1541
01:17:32,410 --> 01:17:36,610
就比如说你拿块纱布和拿一块那个塑料布是完全不一样的
1542
01:17:37,050 --> 01:17:38,370
那么没关系
1543
01:17:38,370 --> 01:17:39,420
我们去仿真
1544
01:17:39,420 --> 01:17:43,956
然后同时我们这个点云这个仿真结果点云它之间是有个误差的
1545
01:17:44,270 --> 01:17:46,178
如果我们把它最小化成误差
1546
01:17:46,180 --> 01:17:48,064
我们就能够去估计这些参数
1547
01:17:48,070 --> 01:17:48,550
对吧
1548
01:17:48,550 --> 01:17:56,324
这些关系都是可以微分去求求导的那这整个东西都可微的这就要求深度学习和仿真的速度应该是得一样的
1549
01:17:56,450 --> 01:17:59,276
不然的话你仿真跟不上深度学习的学习器
1550
01:18:00,330 --> 01:18:00,975
这是为什么
1551
01:18:00,980 --> 01:18:03,394
我们需要一个快一个物流准的仿真结果
1552
01:18:04,820 --> 01:18:05,999
这是我们的中间结果
1553
01:18:06,000 --> 01:18:07,338
下面是甄姬的
1554
01:18:07,340 --> 01:18:08,366
上面是仿真的
1555
01:18:08,370 --> 01:18:10,818
也就是说他在真机做什么事情的时候
1556
01:18:10,830 --> 01:18:12,110
仿真是一模一样的
1557
01:18:12,110 --> 01:18:13,058
能够刻画出来
1558
01:18:14,090 --> 01:18:16,588
这是我们估计出来的杨氏模量、泊松比
1559
01:18:16,760 --> 01:18:18,764
还有千刃钢度等等它的材质
1560
01:18:18,770 --> 01:18:20,759
这些材质你看误差是比较小的
1561
01:18:21,670 --> 01:18:23,344
这证明我们估的很准
1562
01:18:23,870 --> 01:18:24,240
好
1563
01:18:24,520 --> 01:18:26,010
我们还要再做一个事情
1564
01:18:26,010 --> 01:18:28,110
就是说如果在开放行为下面
1565
01:18:28,120 --> 01:18:34,521
我们很多的行为是需要希望能够开放的去学的那就是我们需要积累了海量的数据之后
1566
01:18:34,530 --> 01:18:36,462
我们通过视频能不能自学习
1567
01:18:36,470 --> 01:18:37,595
比如说你现在去做饭
1568
01:18:37,600 --> 01:18:37,780
以后
1569
01:18:37,780 --> 01:18:39,715
下载一堆做饭的数据
1570
01:18:39,900 --> 01:18:41,286
你可以去去学习
1571
01:18:42,910 --> 01:18:45,220
我们就采集了大量的人脸的数据
1572
01:18:45,230 --> 01:18:47,150
我们构成一个lobo two的一个数据集
1573
01:18:47,340 --> 01:18:49,580
这次也采用了大量的人手的操作
1574
01:18:50,030 --> 01:18:51,334
我们就通过看视频
1575
01:18:51,340 --> 01:18:53,100
然后在仿真上任务去检验
1576
01:18:53,110 --> 01:18:54,307
最后推一道真机
1577
01:18:54,450 --> 01:18:59,846
这些工作发表在22老colon 202
1578
01:19:01,020 --> 01:19:01,220
好
1579
01:19:01,220 --> 01:19:02,749
我们最后讲一个具体执行
1580
01:19:02,900 --> 01:19:04,804
具体执行就是另一个科学问题了
1581
01:19:04,810 --> 01:19:06,592
就是我们人类经常忽略的
1582
01:19:06,790 --> 01:19:08,891
其实我们大脑规划完之后
1583
01:19:08,900 --> 01:19:12,045
我们是一个很强的小脑和下意识神经的
1584
01:19:12,060 --> 01:19:14,174
这个东西是其实是很难被刻画的
1585
01:19:14,180 --> 01:19:16,039
也就是我们经常会被忽略
1586
01:19:16,040 --> 01:19:16,976
但是没有这个东西
1587
01:19:16,980 --> 01:19:18,450
他机器人就经常坐不稳
1588
01:19:18,600 --> 01:19:22,608
那么我们的科学问题是如何去学习将人类鲁棒的下意识
1589
01:19:22,870 --> 01:19:25,600
因为我们机器会存在的管制噪声
1590
01:19:25,790 --> 01:19:27,250
还有误差、仿真误差
1591
01:19:27,250 --> 01:19:28,475
还有机械误差等
1592
01:19:30,130 --> 01:19:31,300
为什么我们能做到呢
1593
01:19:31,300 --> 01:19:33,289
因为是如果我们把一个程序流
1594
01:19:33,740 --> 01:19:34,700
把它的物理流
1595
01:19:34,700 --> 01:19:38,690
我们还分解为一步步的来拆解为原子的那些操作
1596
01:19:38,930 --> 01:19:42,910
这些原子操作的话就是都是像拉马拧插这样的
1597
01:19:43,030 --> 01:19:44,190
那么每个原子操作
1598
01:19:44,310 --> 01:19:46,290
其实我们可以学出一个鲁棒的模型
1599
01:19:46,700 --> 01:19:49,607
比如说这种翻开这种动作虽然不同不一样的
1600
01:19:49,730 --> 01:19:52,790
但它都会归结为这个轴负转轴这样的一个过程
1601
01:19:53,230 --> 01:19:54,544
我们是可以被学习的
1602
01:19:54,550 --> 01:19:54,980
好
1603
01:19:55,460 --> 01:19:58,218
为了解决问题抓那个那个原操作
1604
01:19:58,230 --> 01:20:00,814
我们第一个要解决的困境是抓取的困境
1605
01:20:01,420 --> 01:20:02,040
抓取的话
1606
01:20:02,040 --> 01:20:04,086
它首先它数据是一个问题
1607
01:20:04,620 --> 01:20:06,360
如何去产生这样大量的数据
1608
01:20:06,370 --> 01:20:07,200
其实很难的
1609
01:20:07,200 --> 01:20:09,240
之前是没有很好的解决方法
1610
01:20:09,330 --> 01:20:12,256
因为这个点云上面要标注几千个密集的数据
1611
01:20:12,260 --> 01:20:14,630
我们需要几十万个这样的点云数据
1612
01:20:14,770 --> 01:20:16,220
那其实难度是非常大的
1613
01:20:16,450 --> 01:20:19,168
我们也发觉用机器人蒸汽抓又抓了几十年
1614
01:20:20,210 --> 01:20:22,626
因此我们就开发了一套半自动的系统
1615
01:20:22,830 --> 01:20:26,070
这套半系统能自己扫描物体并且摆放场景
1616
01:20:26,290 --> 01:20:36,817
这办系统上只能产生一个场景和物体的数字孪生这个标准速率能提高1万倍
1617
01:20:37,780 --> 01:20:38,190
好
1618
01:20:38,630 --> 01:20:40,000
有了这样我们真实点云
1619
01:20:40,000 --> 01:20:44,316
我们就可以通过数字孪生这个的物体能够产生很多抓取点位
1620
01:20:44,410 --> 01:20:47,561
这些抓取点位的点云就形成了大规模规模的点云匹配
1621
01:20:48,070 --> 01:20:49,360
那这个大于匹配对的话
1622
01:20:49,460 --> 01:20:50,820
我们用很短的时间
1623
01:20:50,820 --> 01:20:52,156
用几周时间能收敛
1624
01:20:52,160 --> 01:20:54,960
能够收集到21亿个有效数据量
1625
01:20:55,150 --> 01:20:58,737
然后有这标注的效率扩大会到10万倍
1626
01:20:58,950 --> 01:21:02,499
那么我们通用抓取大模型的话就是可以在这样做
1627
01:21:02,500 --> 01:21:05,100
包括我们采用了贝叶斯G参数
1628
01:21:05,110 --> 01:21:06,446
贝叶斯分解的网络
1629
01:21:06,450 --> 01:21:07,297
这是我们的网络
1630
01:21:07,300 --> 01:21:10,578
等于说我们把它肢解为几个机械参数的贝叶斯关系
1631
01:21:10,840 --> 01:21:13,540
最后去学这样的网络就能实现通用的结果
1632
01:21:14,070 --> 01:21:17,206
就比如说这个可能很多同学看过
1633
01:21:17,390 --> 01:21:20,018
就是如果我们能够把这个PA子敲碎之后
1634
01:21:20,020 --> 01:21:21,856
我们每一块都能够被抓起来
1635
01:21:22,050 --> 01:21:24,418
那要证明说我们今天都是没有见过的
1636
01:21:24,420 --> 01:21:26,610
那么我们就是证明我们这是个通用的
1637
01:21:28,740 --> 01:21:33,570
然后我们也在第一次做到了在在移动的在这个移动的环境下能够抓取
1638
01:21:33,570 --> 01:21:38,654
这也发表了TIO它的速度是它的是能够在没有见过的动态物体都能做
1639
01:21:39,570 --> 01:21:41,970
然后我们也第一次超过了人类的水平
1640
01:21:42,390 --> 01:21:44,289
在准确率和速度上面
1641
01:21:45,120 --> 01:21:48,524
这是我们抓几千个没见过的物体都能够很稳定的抓取
1642
01:21:48,810 --> 01:21:55,488
还能够抓取那些非常细小的东西非常细小的东西
1643
01:21:55,790 --> 01:21:56,240
好
1644
01:21:56,530 --> 01:22:01,630
我们这个工作也是得到了斯坦福在机器人的抓取的影响力
1645
01:22:01,650 --> 01:22:04,866
排行榜面也是近十年的前到排前十名
1646
01:22:05,000 --> 01:22:05,940
排到第二名
1647
01:22:08,260 --> 01:22:10,456
我们刚刚提出的方案的话
1648
01:22:11,200 --> 01:22:12,520
我们干脆抓取
1649
01:22:12,520 --> 01:22:15,176
那我们对于大量的任务怎么去学习呢
1650
01:22:15,330 --> 01:22:16,398
我们需要一个
1651
01:22:16,400 --> 01:22:18,061
我们来看看谷歌怎么学的
1652
01:22:18,310 --> 01:22:21,280
它是RT two是把一个视觉进去之后
1653
01:22:21,280 --> 01:22:22,730
我们就是产生它的位置
1654
01:22:22,940 --> 01:22:26,626
但是位置这个东西它只能做一些简单的工作
1655
01:22:26,630 --> 01:22:29,446
如果需要很多的力的接触它就不行了
1656
01:22:29,460 --> 01:22:32,196
所以说我们需要做的是一个利蔚混合的操作
1657
01:22:32,570 --> 01:22:35,270
等于说刻画了逆位混合的表征特征
1658
01:22:35,390 --> 01:22:40,083
然后提出了位置引导下的力觉单元模型建模
1659
01:22:40,540 --> 01:22:41,608
以及回归参数
1660
01:22:41,610 --> 01:22:46,146
等于说我们的大模型是能够去输出力觉和位置同时的一个反馈
1661
01:22:46,320 --> 01:22:46,590
对的
1662
01:22:46,830 --> 01:22:48,858
那这东西就需要力觉的反馈了
1663
01:22:50,340 --> 01:22:54,199
比如说我们去刮这个气球高的需求的话
1664
01:22:54,210 --> 01:22:56,604
我们是需要在表面稳定的五牛顿
1665
01:22:56,610 --> 01:22:58,538
然后倾斜多少牛顿
1666
01:22:58,650 --> 01:22:59,910
这都是一个力学问题
1667
01:23:00,030 --> 01:23:04,293
所以我们这个模型需要输出很好的秘诀才能做到这样的一个稳定性
1668
01:23:04,550 --> 01:23:06,160
这个和以前完全不一样
1669
01:23:06,250 --> 01:23:08,902
只要整个模型的设计也会完全的不一样
1670
01:23:08,970 --> 01:23:11,298
因为时间关系我就不展开了
1671
01:23:11,300 --> 01:23:16,292
我们做的方案甚至能够PK过人类的瓜呱呱气球的能力
1672
01:23:17,970 --> 01:23:19,907
当然我们也可以真实的刮胡子
1673
01:23:19,910 --> 01:23:21,926
只是我们觉得真实的刮胡子的话
1674
01:23:23,220 --> 01:23:26,036
没有气球来的有可显示度
1675
01:23:26,270 --> 01:23:31,652
包括我们能够做的非常复杂的位置的操作
1676
01:23:32,040 --> 01:23:33,776
力觉的相关的操作
1677
01:23:34,450 --> 01:23:38,174
为此我们也构建了全球最大的力觉的数据集
1678
01:23:38,550 --> 01:23:41,973
我们是已经有100TB是GP3的两倍数据集
1679
01:23:41,990 --> 01:23:44,003
这篇文章也发表在ISS
1680
01:23:44,250 --> 01:23:45,812
希望大家能学我们的数据
1681
01:23:45,820 --> 01:23:47,287
我们数据也是开源的
1682
01:23:47,870 --> 01:23:53,030
然后的话我们也是参与了位置的数据库的位置为中心的数据库的构建
1683
01:23:53,560 --> 01:23:59,200
Deep mind stands华盛理工之间没有一起构建这样的一个大规模的数据集
1684
01:23:59,210 --> 01:24:00,800
当然这是一是为中心的
1685
01:24:00,800 --> 01:24:05,189
我们是强调和位置和力觉为力位混合还不太一样
1686
01:24:05,690 --> 01:24:07,838
我们也是唯一的国内的单位
1687
01:24:07,840 --> 01:24:09,600
并且数据量排名是第二
1688
01:24:10,090 --> 01:24:13,492
然后为此我们构建了一个整体的一个机器人系统
1689
01:24:13,660 --> 01:24:15,178
把我们的技术都放在一起
1690
01:24:15,310 --> 01:24:16,588
包括如图所示
1691
01:24:16,870 --> 01:24:20,860
我们能够不停的动态的抓不同的没有见过的物体
1692
01:24:22,050 --> 01:24:23,942
同时我们能够擦这种动作
1693
01:24:23,950 --> 01:24:25,762
其实是很少能做到这样子的
1694
01:24:25,960 --> 01:24:29,560
然后这样我们能够对这个柔性物体的理解
1695
01:24:30,250 --> 01:24:31,710
柔性物体充分的理解它
1696
01:24:31,710 --> 01:24:34,130
柔性物体能够挂起来等等
1697
01:24:35,630 --> 01:24:37,718
为了进一步的做深刻的研究
1698
01:24:37,720 --> 01:24:43,376
我们也在大脑和身体之间研究一个叫做自身行为研究的一个项目
1699
01:24:43,870 --> 01:24:44,758
因为时间有限
1700
01:24:44,760 --> 01:24:45,993
我这里就不展开讲了
1701
01:24:46,000 --> 01:24:47,920
这些工作发表在nature上面
1702
01:24:48,310 --> 01:24:49,838
这是我的代表工作
1703
01:24:50,240 --> 01:24:55,415
申请人的我们是在科学探索奖上面
1704
01:24:55,420 --> 01:24:58,812
也是肯定了我在自身智能方面的贡献
1705
01:24:59,450 --> 01:25:02,234
我们在人工智能领域发表了一些论文
1706
01:25:02,240 --> 01:25:05,104
包括机器人的两大顶刊TROIJR
1707
01:25:05,770 --> 01:25:11,223
然后还有三次获得国际机器人论文的顶会好
1708
01:25:11,230 --> 01:25:12,130
谢谢大家
1709
01:25:12,420 --> 01:25:13,040
张老师
1710
01:25:17,530 --> 01:25:18,500
我讲完了
1711
01:25:18,500 --> 01:25:18,930
谢谢
1712
01:25:18,930 --> 01:25:22,110
好的好的
1713
01:25:22,110 --> 01:25:22,920
谢谢罗老师
1714
01:25:23,090 --> 01:25:24,782
感谢罗老师的这个精彩报告
1715
01:25:25,000 --> 01:25:27,640
然后我看到了也是时间原因
1716
01:25:27,790 --> 01:25:30,520
我就留那个选一个问题
1717
01:25:31,820 --> 01:25:33,550
就是有人问这个一块儿
1718
01:25:33,560 --> 01:25:35,040
咱们建模的那个柔性布
1719
01:25:35,480 --> 01:25:37,304
用这个神经网络去建模的话
1720
01:25:37,310 --> 01:25:38,700
他要大概有多少的参数
1721
01:25:43,680 --> 01:25:46,752
说一定就是有点像有线人法那种感觉
1722
01:25:46,910 --> 01:25:49,346
建模一块大概是几千几千个参数
1723
01:25:49,350 --> 01:25:50,134
大概是这样子的
1724
01:25:50,460 --> 01:25:50,680
好
1725
01:25:50,680 --> 01:25:51,150
谢谢
1726
01:25:51,920 --> 01:25:52,200
好的
1727
01:25:52,700 --> 01:25:53,420
感谢罗老师
1728
01:25:53,900 --> 01:25:57,321
也是因为整体的时间原因
1729
01:25:57,540 --> 01:26:03,722
我们接下来就进入到嘉宾的访谈和交流的这个环节
1730
01:26:04,270 --> 01:26:09,430
然后我们也是准备了大概五个问题
1731
01:26:09,490 --> 01:26:11,778
然后也请三位老师
1732
01:26:11,910 --> 01:26:13,770
麻烦三位老师能否开一下摄像头
1733
01:26:18,060 --> 01:26:19,150
好的
1734
01:26:21,360 --> 01:26:21,830
对
1735
01:26:22,400 --> 01:26:25,667
我们这边第一个问题是啊
1736
01:26:25,670 --> 01:26:27,720
因为这三个报告听下来
1737
01:26:27,820 --> 01:26:29,490
是我自己也是受益匪浅
1738
01:26:31,380 --> 01:26:34,860
这边第一个问题是就是在大模型技术出现之前
1739
01:26:35,380 --> 01:26:39,436
因为我们虽然咱们今天的主题是这个狙神智能和大模型
1740
01:26:39,980 --> 01:26:43,070
那我们就在想这个大模型出现之前
1741
01:26:43,780 --> 01:26:46,789
聚神智能就有了一定的前期研究和进展
1742
01:26:47,350 --> 01:26:49,975
包括卢老师和孙老师其实也提到了
1743
01:26:51,210 --> 01:26:53,298
其实居人智能这个概念起源的还是挺早的
1744
01:26:53,310 --> 01:26:54,438
上个世纪60年代
1745
01:26:55,150 --> 01:26:56,850
那么在这个大模型时代
1746
01:26:56,850 --> 01:26:58,722
这个巨神智能的研究和应用
1747
01:26:59,210 --> 01:27:05,585
它面临哪些就跟原来的这个研究相比有哪些的机遇和挑战
1748
01:27:06,830 --> 01:27:11,250
就是请三位老师可以分别发表一下各自的看法
1749
01:27:13,410 --> 01:27:16,500
要不要不孙老师您先行
1750
01:27:16,840 --> 01:27:18,790
其实我今天也谈到了一个问题
1751
01:27:19,250 --> 01:27:21,170
就是机器人的应用过程里面
1752
01:27:21,710 --> 01:27:24,258
早先的工作基本上就是就事论事
1753
01:27:24,850 --> 01:27:27,634
就是机器人面向特定的任务
1754
01:27:28,170 --> 01:27:29,170
特定的场景
1755
01:27:30,260 --> 01:27:33,260
我们要找具体的算法让工程去调参
1756
01:27:33,710 --> 01:27:36,170
最后把这个任务做的特别好
1757
01:27:37,560 --> 01:27:38,784
现在大模型来以后
1758
01:27:38,980 --> 01:27:42,363
大家就想说这大模型能不能面向多任务
1759
01:27:42,560 --> 01:27:43,498
就像我们人一样
1760
01:27:43,850 --> 01:27:45,440
可以做很多很多的事情
1761
01:27:46,370 --> 01:27:47,070
而大模型
1762
01:27:47,170 --> 01:27:49,328
我们今天我其实也谈到了一点
1763
01:27:49,330 --> 01:27:52,534
就是像现在微软现在在用的
1764
01:27:52,960 --> 01:27:56,128
我认为主要的它是一个通用的任务规划器
1765
01:27:57,540 --> 01:28:00,884
具体的在用在某一个场景
1766
01:28:01,010 --> 01:28:03,110
某一个任务下面
1767
01:28:04,090 --> 01:28:11,972
他要去具体的这种我刚才讲到了就是场景库、技能库和资源库必须建立
1768
01:28:12,160 --> 01:28:15,240
否则的话他难以做到提升
1769
01:28:15,680 --> 01:28:18,752
这第一个方面我在问我说这个场景库
1770
01:28:19,180 --> 01:28:22,106
这个技能库是这个中间件
1771
01:28:23,040 --> 01:28:27,408
第二个我觉得比较重要的就是因为机器人在应用过程里
1772
01:28:27,670 --> 01:28:29,190
大家在用大模型的时候
1773
01:28:29,320 --> 01:28:34,598
场景肯定不是说大家都是一个统一的这里面一个问题就是感知问题
1774
01:28:35,310 --> 01:28:40,185
我觉得今天场内非常有趣的就是我们能不能像做像哈工大
1775
01:28:40,880 --> 01:28:44,408
未来你们在做这个叫节点式
1776
01:28:44,410 --> 01:28:44,980
对吧
1777
01:28:45,120 --> 01:28:46,605
过去我们都有个平台
1778
01:28:47,140 --> 01:28:48,508
后来说咱不用平台了
1779
01:28:48,930 --> 01:28:51,758
咱们把传感器放在这个导弹上面
1780
01:28:52,100 --> 01:28:53,680
放在这个运载火箭上面
1781
01:28:54,850 --> 01:28:56,920
把它构造一个数学平台
1782
01:28:57,520 --> 01:28:58,900
这里面有一个很重要的一点
1783
01:28:58,900 --> 01:29:02,148
就是我能不能把再有传在机器人上的
1784
01:29:02,160 --> 01:29:04,736
包括不在机器人上传感器的感知信息
1785
01:29:05,020 --> 01:29:08,520
能不能把它投影到一个公共空间
1786
01:29:08,520 --> 01:29:09,704
我们叫隐空间里去
1787
01:29:10,610 --> 01:29:12,348
来我们在这里有一套方法
1788
01:29:12,350 --> 01:29:15,262
比如说从数据空间到特征空间
1789
01:29:15,270 --> 01:29:16,716
再到概念空间
1790
01:29:16,720 --> 01:29:17,686
再到知识空间
1791
01:29:18,900 --> 01:29:25,708
其实这样很重要的一点就是因为最后据说里面很重要的一点就是如何像人一样去决策
1792
01:29:26,610 --> 01:29:30,660
而决策里面我们需要在精神场景的知识去做这样一个事情
1793
01:29:31,190 --> 01:29:35,324
再有一个就是我跟武测入伍测观点比较一致的一个接入错误
1794
01:29:35,960 --> 01:29:38,660
最后一个就是我叫的fair shot learning
1795
01:29:38,670 --> 01:29:39,558
就是聚身优化
1796
01:29:39,790 --> 01:29:41,030
这个是比较重要的
1797
01:29:41,200 --> 01:29:48,038
但是这个据说优化每个人用的时候都希望不需要太多的学习
1798
01:29:48,060 --> 01:29:51,434
就能很快的能够把参数调到最优
1799
01:29:52,430 --> 01:29:54,050
这里面我们就提到一个概念
1800
01:29:54,060 --> 01:29:56,892
就是从实力到模板这个过程
1801
01:29:57,340 --> 01:30:01,102
如何通过知识推理实现这种模板之间的转换
1802
01:30:01,300 --> 01:30:02,115
实现多任务
1803
01:30:03,210 --> 01:30:05,680
我希望从这几个过程角度来做
1804
01:30:06,420 --> 01:30:08,774
希望大模型能够在机器人
1805
01:30:08,780 --> 01:30:10,070
在各种场景
1806
01:30:10,190 --> 01:30:14,576
各种任务下都能的话实现这样一个提升
1807
01:30:15,280 --> 01:30:19,732
当然的话有还有一些单个集成的方面的一些例子
1808
01:30:19,750 --> 01:30:24,027
比如说像通过交互去实现感知
1809
01:30:24,200 --> 01:30:25,872
这个路测也谈到一个问题
1810
01:30:25,880 --> 01:30:27,884
我们也做了一些类似的工作
1811
01:30:27,890 --> 01:30:32,306
比如说我们做的是固定试点的传感器和移动视线传感器
1812
01:30:32,470 --> 01:30:35,870
如何实现对常见的感知和认知过程
1813
01:30:37,940 --> 01:30:39,798
我想回答大概是这些
1814
01:30:40,710 --> 01:30:41,110
好的
1815
01:30:41,110 --> 01:30:41,910
谢谢孙老师
1816
01:30:42,620 --> 01:30:46,272
张老师您这边有没有什么看有没有相应的这个看法
1817
01:30:47,480 --> 01:30:48,446
那我就接着负责
1818
01:30:48,450 --> 01:30:50,640
老师刚才讲了
1819
01:30:50,820 --> 01:30:53,956
我还是觉得从两块那么一块的话
1820
01:30:54,310 --> 01:30:59,934
就相当于说他怎么能把这个任务更好的完成
1821
01:30:59,940 --> 01:31:03,459
就从从本体这个层面
1822
01:31:03,620 --> 01:31:05,195
你比如说从营销手段
1823
01:31:05,590 --> 01:31:10,098
从精细的这种累人的这种结构了
1824
01:31:10,300 --> 01:31:16,420
那这一块我们怎么能达到像那个人或者像低等动物一样
1825
01:31:16,440 --> 01:31:18,375
还是从机构这个层面
1826
01:31:18,880 --> 01:31:20,826
从行为这一块做
1827
01:31:21,200 --> 01:31:23,191
但刚才付春老师也提到了
1828
01:31:23,200 --> 01:31:25,900
说我们在针对一些通用的这个任务
1829
01:31:26,410 --> 01:31:28,880
原来机器人我们就是就事论事
1830
01:31:28,890 --> 01:31:30,267
针对于这种特定场景
1831
01:31:30,490 --> 01:31:32,218
特定任务我们来做
1832
01:31:32,740 --> 01:31:34,500
如果说针对某一个行业
1833
01:31:34,500 --> 01:31:35,140
某一个场景
1834
01:31:35,140 --> 01:31:37,337
我们有些通用的这个任务的话
1835
01:31:37,570 --> 01:31:39,883
那么机器人怎么来做
1836
01:31:39,990 --> 01:31:43,866
这里面的话我觉得大模型可以提供一些机会
1837
01:31:44,270 --> 01:31:48,870
而这个机会的话就可以在感知层在认知这一块
1838
01:31:48,870 --> 01:31:49,860
甚至在决策
1839
01:31:50,010 --> 01:31:54,265
我们怎么来通过这一种大模型给他来进行赋能这一块
1840
01:31:54,280 --> 01:31:57,647
我觉得针对这种研究这一部分
1841
01:31:57,770 --> 01:31:58,832
当然我们是有压力的
1842
01:31:58,960 --> 01:32:01,255
你比如说传统的我们在做目标检测
1843
01:32:01,390 --> 01:32:02,350
目标分割的话
1844
01:32:02,730 --> 01:32:03,826
大模型出来了以后
1845
01:32:03,920 --> 01:32:06,230
我们总是要跟大模型来进行对比
1846
01:32:07,440 --> 01:32:10,320
能不能达到他这么一个水平
1847
01:32:10,630 --> 01:32:14,268
这一这对我们学术研究是有一定压力的
1848
01:32:14,620 --> 01:32:16,171
当然对于产业界来说的话
1849
01:32:16,280 --> 01:32:24,557
我们怎么来能开发出这种适合于这种场景里面有点通用这个任务的同时
1850
01:32:24,560 --> 01:32:29,530
又能把这个任务能做的做精、做细、做巧的这一个层面
1851
01:32:30,090 --> 01:32:32,746
在这一块觉得在应用这一块来说的话
1852
01:32:32,820 --> 01:32:38,668
实际上我们也有有比较好的什么机会了
1853
01:32:38,810 --> 01:32:41,197
那么是到底是哪一个行业
1854
01:32:41,200 --> 01:32:42,916
哪一个产业能抓住这个机会
1855
01:32:43,310 --> 01:32:46,480
这一块我觉得大家也可以拭目以待
1856
01:32:48,840 --> 01:32:49,990
谢谢张老师
1857
01:32:50,820 --> 01:32:52,140
罗老师您看看
1858
01:32:57,320 --> 01:32:57,860
好好
1859
01:32:57,860 --> 01:32:58,350
谢谢
1860
01:32:58,720 --> 01:33:02,096
前面的两位老师讲的已经非常全面了
1861
01:33:02,560 --> 01:33:03,715
对我我非常认同
1862
01:33:04,450 --> 01:33:06,210
之前的话是都是一个任务
1863
01:33:06,210 --> 01:33:07,398
就之前都是一个任务
1864
01:33:07,400 --> 01:33:09,144
比如说我做一个任务就能发一篇论文
1865
01:33:09,150 --> 01:33:10,640
这篇论文就不用发论文
1866
01:33:10,640 --> 01:33:12,512
然后我告诉你很特定的解法
1867
01:33:12,860 --> 01:33:14,589
其实我觉得在大模型时代的话
1868
01:33:14,590 --> 01:33:16,766
我们应该思考是一个怎么样统一的模型
1869
01:33:16,770 --> 01:33:18,318
是一个统一的方法论
1870
01:33:18,790 --> 01:33:22,130
就是说我们这个模型的话是要带来很大的机会
1871
01:33:22,130 --> 01:33:25,132
所以你看现在的研究就不再是发论文的时候
1872
01:33:25,140 --> 01:33:29,142
很多人不会说我提出一个什么什么样的一个解决的一个什么的应用
1873
01:33:29,200 --> 01:33:32,698
我都说我提出一种方法能解决若干个一系列的应用
1874
01:33:32,920 --> 01:33:34,396
因为它都有一个共性
1875
01:33:35,170 --> 01:33:37,868
就是我们甚至希望说将来做到一个新的场景
1876
01:33:38,280 --> 01:33:41,336
机器人只有一套统一的方法论去学习
1877
01:33:41,460 --> 01:33:42,824
这套学习的方法只有一个
1878
01:33:42,830 --> 01:33:44,920
普通人都能教他去去学
1879
01:33:44,920 --> 01:33:46,570
然后他就能够落地应用
1880
01:33:47,050 --> 01:33:48,086
因为你去到餐厅
1881
01:33:48,090 --> 01:33:50,385
去到餐去到那个医院
1882
01:33:50,740 --> 01:33:52,390
那我们就简单的教他一下
1883
01:33:52,390 --> 01:33:53,240
他就能够用
1884
01:33:54,290 --> 01:33:56,704
这个事情我觉得是一个很大的一个机会
1885
01:33:56,710 --> 01:33:59,380
等于说整个研究的方法论都不一样
1886
01:33:59,380 --> 01:34:02,180
所以你看发论文的要发有一派的论文
1887
01:34:02,190 --> 01:34:04,888
都不再说我的某个应用做solo就结束了
1888
01:34:05,000 --> 01:34:07,440
而是你这个东西benefit的一大批应用
1889
01:34:07,820 --> 01:34:09,393
这是一种很大的一种变化
1890
01:34:09,400 --> 01:34:09,960
这是第一
1891
01:34:09,960 --> 01:34:10,520
第二点的话
1892
01:34:10,520 --> 01:34:12,320
我觉得一个大模型时代
1893
01:34:12,320 --> 01:34:13,970
一个机身智能大模型的话
1894
01:34:14,050 --> 01:34:15,274
我觉得很关键的数据
1895
01:34:16,140 --> 01:34:21,405
其实在在大机身智能之前不是说比如说语言和图像他们讲数据
1896
01:34:21,610 --> 01:34:22,880
但是他们讲数据的时候
1897
01:34:23,350 --> 01:34:25,646
他们更多是讲我有多少的体力活
1898
01:34:25,660 --> 01:34:27,793
就是我拥有多少数据
1899
01:34:27,800 --> 01:34:29,228
是拥有这个词
1900
01:34:29,390 --> 01:34:31,015
但是说我觉得到居然智能的话
1901
01:34:31,020 --> 01:34:31,944
他就不只是拥有
1902
01:34:31,950 --> 01:34:35,670
因为我觉得是一种你能不能去产生数据的能力
1903
01:34:35,790 --> 01:34:37,737
以及能规模化数据的能力
1904
01:34:37,970 --> 01:34:39,608
因为在今天的话
1905
01:34:39,610 --> 01:34:41,090
数据暨科研
1906
01:34:41,530 --> 01:34:42,982
基田数据的话就是体力活
1907
01:34:43,840 --> 01:34:44,827
因为你就去下载
1908
01:34:44,970 --> 01:34:47,562
但是现在的话居然智能数据去哪里下载呢
1909
01:34:47,570 --> 01:34:48,860
这个问题好
1910
01:34:48,860 --> 01:34:50,100
那你就必须得生成
1911
01:34:50,270 --> 01:34:51,026
所以生成的话
1912
01:34:51,030 --> 01:34:52,779
那你生成的有效性怎么样
1913
01:34:52,780 --> 01:34:54,639
你生成的数据是不是噪声很多
1914
01:34:55,190 --> 01:34:56,065
这是第二个问题
1915
01:34:56,070 --> 01:34:56,510
第三个问题
1916
01:34:56,510 --> 01:34:58,050
你生成的成本是不是很高
1917
01:34:58,420 --> 01:34:59,920
就像我们自己做抓取的时候
1918
01:35:00,030 --> 01:35:02,638
我觉得如果我们用机器去生成也可以
1919
01:35:02,650 --> 01:35:03,786
但这个效率太低了
1920
01:35:03,790 --> 01:35:04,900
能搞三十多年
1921
01:35:05,050 --> 01:35:08,482
那么有没有办法折中的方法使用它的生存的时间非常短
1922
01:35:08,660 --> 01:35:09,776
可以被规模化
1923
01:35:09,780 --> 01:35:10,716
然后它的效率
1924
01:35:10,720 --> 01:35:12,403
它的质量又能够可用
1925
01:35:12,710 --> 01:35:17,450
所以这里面就会非常都非常多的对于数据这样的一个科学问题的探讨
1926
01:35:17,590 --> 01:35:19,138
所以数据的科学问题
1927
01:35:19,140 --> 01:35:21,648
它既是对这个问题的理解
1928
01:35:21,770 --> 01:35:24,425
所以它是一个只是一个偶合的问题
1929
01:35:24,430 --> 01:35:32,216
就算法和数据在居然智能的时代是一个大特别是大数据是一个全新的科研问题
1930
01:35:32,710 --> 01:35:32,920
好
1931
01:35:32,920 --> 01:35:33,760
我就讲这两点
1932
01:35:35,970 --> 01:35:36,280
好的
1933
01:35:36,280 --> 01:35:37,220
谢谢卢老师
1934
01:35:37,730 --> 01:35:40,982
这里面刚才几位老师的回答
1935
01:35:41,200 --> 01:35:43,225
我又突然有一个想法
1936
01:35:43,450 --> 01:35:45,958
就是我刚才提到了我们说的
1937
01:35:46,800 --> 01:35:53,974
其实我们的这个聚生智能实际上是面向在真实的复杂开放环境的这个过程之中
1938
01:35:53,980 --> 01:35:57,628
其实他的少样本学习的能力其实是很重要的
1939
01:35:58,630 --> 01:35:59,870
那么我们我就在想
1940
01:35:59,870 --> 01:36:04,468
就是我们说后面我们有没有可能让这个智能体也好
1941
01:36:04,470 --> 01:36:05,527
或者机器人也好
1942
01:36:05,740 --> 01:36:07,868
在真实的环境之中
1943
01:36:08,160 --> 01:36:10,160
让他采集真实的数据去进行一个学习
1944
01:36:10,170 --> 01:36:16,773
而不是我们准备的这个标注的这种有监督标注的这种数据集让他去学习
1945
01:36:17,420 --> 01:36:20,156
不知道在聚生智能的后续发展的这个阶段
1946
01:36:20,170 --> 01:36:23,628
这种数据的就是从真实的场景中
1947
01:36:23,640 --> 01:36:25,089
我们实时的获取数据
1948
01:36:25,090 --> 01:36:26,467
然后实时的进行学习
1949
01:36:27,410 --> 01:36:29,594
包括跟场景之间的交互和反馈
1950
01:36:29,930 --> 01:36:34,078
然后来让整个的模型在训练的过程之中
1951
01:36:34,090 --> 01:36:36,680
形成这样的一个数据的一个闭环
1952
01:36:37,340 --> 01:36:39,740
我不知道这样的想法是不是正确的
1953
01:36:39,900 --> 01:36:41,180
或者是这个样的一个方向
1954
01:36:46,140 --> 01:36:50,223
要不孙老师您看看我这个问题
1955
01:36:50,560 --> 01:36:52,800
对我是突然刚才想到的这个问题
1956
01:36:52,810 --> 01:36:55,066
对我觉得这个问题问的很好
1957
01:36:55,540 --> 01:36:59,195
其实的话就是我们过去比如说深度学习
1958
01:37:00,040 --> 01:37:00,850
做人脸识别
1959
01:37:00,850 --> 01:37:03,655
它用到了仅仅用到了样本里的视觉特征
1960
01:37:06,290 --> 01:37:09,158
具身的概念就想要身临其中
1961
01:37:09,270 --> 01:37:11,475
就是沉浸和作用
1962
01:37:11,750 --> 01:37:12,580
就是reaction
1963
01:37:12,660 --> 01:37:15,300
就是说我碰了他一下我的一个反应
1964
01:37:16,190 --> 01:37:18,635
这个是具身的两个比较重要的特点
1965
01:37:19,280 --> 01:37:21,352
所以说的话其实我是搞机器人的
1966
01:37:21,360 --> 01:37:26,066
最早因为以前我们做机器人用的学习数据都是真实场景下的
1967
01:37:28,290 --> 01:37:31,638
机器人在小模型叫few shot learning里面
1968
01:37:31,760 --> 01:37:35,672
我觉得比较重要的就是大模型的作用主要体现在哪里呢
1969
01:37:36,030 --> 01:37:37,395
主要是产生数据
1970
01:37:38,790 --> 01:37:41,338
就是它有很多数据是生成出来的
1971
01:37:41,980 --> 01:37:48,055
这样就弥补了基层学习过程中对大量真实数据的这种需要
1972
01:37:48,470 --> 01:37:51,116
觉得今天路测也提到了这个问题
1973
01:37:51,970 --> 01:37:54,146
所以我今天也其实我的报告里也谈到了
1974
01:37:54,160 --> 01:37:56,799
就是大模型跟扩散模型的结合
1975
01:37:57,060 --> 01:37:58,971
实际上就是产生多模态的数据
1976
01:37:59,940 --> 01:38:02,423
第二个就是跟知识图谱的结合
1977
01:38:02,430 --> 01:38:04,075
就要增强它推理
1978
01:38:04,080 --> 01:38:07,095
就是多任务这一块比较重要的一块
1979
01:38:08,320 --> 01:38:09,314
小样本学习里面
1980
01:38:09,420 --> 01:38:11,290
还有一部分就来自于知识
1981
01:38:12,160 --> 01:38:15,220
还有一部分知识来自的话迁移学习
1982
01:38:15,990 --> 01:38:17,664
这个你比如车谈到的
1983
01:38:17,670 --> 01:38:19,190
他就说叫虚实迁移
1984
01:38:19,600 --> 01:38:21,076
这是一部分很重要的
1985
01:38:21,690 --> 01:38:22,734
另外还有一部分的话
1986
01:38:23,270 --> 01:38:24,495
其实我觉得的话
1987
01:38:24,580 --> 01:38:28,825
就是通过跨模态去去做小样本学习
1988
01:38:28,830 --> 01:38:31,330
就通过这些手段来提高
1989
01:38:31,680 --> 01:38:36,848
通过few shot learning来使机器人在很快的实现这种能力
1990
01:38:36,863 --> 01:38:40,767
你比如说我今天谈到的就是叫preference learning
1991
01:38:40,773 --> 01:38:41,858
叫偏好学习
1992
01:38:41,930 --> 01:38:45,596
就是通过人在某些特定场景下
1993
01:38:45,600 --> 01:38:48,602
比如说某些人某些做某些事的技能就特别好
1994
01:38:48,930 --> 01:38:52,141
我能不能把这部分用小样本模型把它建出来
1995
01:38:52,810 --> 01:38:54,314
然后再通过生成式
1996
01:38:54,320 --> 01:38:55,848
你看跟最大熵结合
1997
01:38:56,130 --> 01:38:57,860
再跟这个对抗学习结合
1998
01:38:58,010 --> 01:39:00,152
产生数据让他去学习
1999
01:39:00,350 --> 01:39:02,840
这也是跟这个虚实迁移
2000
01:39:02,840 --> 01:39:07,226
跟对抗学习结合来减少样本的一个方法
2001
01:39:08,350 --> 01:39:10,580
总之就是大模型的出现
2002
01:39:11,270 --> 01:39:14,998
使得的具身感知这块就是通过多模态
2003
01:39:15,840 --> 01:39:20,220
它可以增强交互增强的多模态感知的这种能力
2004
01:39:20,470 --> 01:39:29,770
再一个就是通过这种趋势迁移来提高学习这个学习的效果
2005
01:39:30,100 --> 01:39:36,915
总之我觉得大模型是未来机器人走向巨深非常重要的意义一种方法
2006
01:39:37,170 --> 01:39:39,920
而且我觉得前景也是很广
2007
01:39:40,230 --> 01:39:43,783
我团队现在已经在四个场景下做出来了
2008
01:39:43,790 --> 01:39:44,934
就是做了一个实验
2009
01:39:45,110 --> 01:39:47,020
就在这个手机装配里面
2010
01:39:47,910 --> 01:39:50,170
发现效果还是比较好的
2011
01:39:50,170 --> 01:39:51,802
你比如说我们还做过跟体感
2012
01:39:52,990 --> 01:39:55,837
就watch感知的这种结合
2013
01:39:56,450 --> 01:39:59,245
通过体感来增强这种感知效果
2014
01:39:59,630 --> 01:40:03,550
我们也研究比如说通过反馈怎么样增强注意力
2015
01:40:04,030 --> 01:40:07,292
类似这些来提高这个交互的效果
2016
01:40:08,100 --> 01:40:14,964
总之我觉得大模型来做这个巨升也是一个刚开始的过程
2017
01:40:15,430 --> 01:40:19,772
还有很多问题可能留在后面大家在做的过程中不断的去解决
2018
01:40:20,230 --> 01:40:22,280
但是我觉得这是一个非常有前景的方向
2019
01:40:24,510 --> 01:40:25,490
谢谢孙老师
2020
01:40:26,490 --> 01:40:30,362
张老师您看呢嗯呃我同意
2021
01:40:31,330 --> 01:40:33,585
的确因为比如我打个比方
2022
01:40:33,590 --> 01:40:37,782
比如说我们也在前面做那个情感计算
2023
01:40:38,190 --> 01:40:39,450
这个情感识别
2024
01:40:39,450 --> 01:40:43,450
那里面我们也构建了比较大的数据集
2025
01:40:43,620 --> 01:40:45,000
这个是从网上
2026
01:40:45,820 --> 01:40:46,830
从视频里面
2027
01:40:46,830 --> 01:40:48,190
从电影里面
2028
01:40:48,190 --> 01:40:51,590
从各种反正从互联网上面我们办下来这个数据
2029
01:40:52,130 --> 01:40:55,479
但是这个数据我们要用在这个机型上面
2030
01:40:56,030 --> 01:40:58,032
那可能这个场景就要变了
2031
01:40:58,490 --> 01:41:01,074
就相当于说我们要以机器人这一个视角
2032
01:41:01,670 --> 01:41:07,058
这个视角来看它是适不适合
2033
01:41:07,560 --> 01:41:10,164
我觉得这就是加上自身以后
2034
01:41:10,360 --> 01:41:13,450
就对我们这一个研究又提出了挑战
2035
01:41:13,450 --> 01:41:16,926
相当于说以人这个视角或者说以机器人的一个视角
2036
01:41:16,930 --> 01:41:18,310
我再来观察你这个对象
2037
01:41:18,310 --> 01:41:20,442
你这个对象的这个情感是在哪
2038
01:41:20,450 --> 01:41:23,867
那你这个所所做的这个数据集合不合适
2039
01:41:24,440 --> 01:41:25,952
那就不一就不一样了
2040
01:41:27,370 --> 01:41:29,879
这里面就提到了我们有姿态了
2041
01:41:30,780 --> 01:41:32,499
就是从视角这个层面
2042
01:41:32,840 --> 01:41:33,946
所以说反过头来
2043
01:41:34,000 --> 01:41:41,735
我同意刚才就是我们怎么怎么来构建更适合于这个应用场景里面这个数据来训练
2044
01:41:43,080 --> 01:41:44,080
这个层面
2045
01:41:44,610 --> 01:41:46,073
那个大模型出来以后的话
2046
01:41:46,080 --> 01:41:50,640
的确是能给我能辅助我们来做这方面的工作
2047
01:41:50,830 --> 01:41:53,030
无论是针对这个视觉也好
2048
01:41:53,570 --> 01:41:55,480
针对这个语音也好的话
2049
01:41:56,590 --> 01:42:03,506
这几个层面我们可以结合进来来来更容易把这个事给做起来
2050
01:42:03,810 --> 01:42:08,310
核心还是相当于说人与物之间他们有这个交互
2051
01:42:08,500 --> 01:42:10,856
交互的这个过程就相当于你那个视角不一样
2052
01:42:11,300 --> 01:42:13,850
你看这个问题的这个角度也不一样
2053
01:42:15,170 --> 01:42:18,850
你这个输入输出的话还是我觉得可能也有变化
2054
01:42:19,210 --> 01:42:21,240
这一块的确是有挑战的
2055
01:42:21,240 --> 01:42:23,280
但反过头来也是有有机遇的
2056
01:42:24,600 --> 01:42:25,545
我只是补充一下
2057
01:42:25,550 --> 01:42:28,610
刚才不付成老师所讲的这个
2058
01:42:30,840 --> 01:42:31,210
好的
2059
01:42:31,210 --> 01:42:31,960
谢谢张老师
2060
01:42:32,550 --> 01:42:34,760
罗老师您看这种环境中的数据
2061
01:42:34,790 --> 01:42:37,559
两位老师讲的已经非常全面了
2062
01:42:37,750 --> 01:42:41,958
我觉得跟我想的是就是我也是很同意
2063
01:42:42,260 --> 01:42:44,620
就可能有一点点补充的
2064
01:42:44,620 --> 01:42:49,456
就是说我觉得我们肯定是未来这个居然智能肯定是个开环的
2065
01:42:49,600 --> 01:42:50,328
肯定是个闭环的
2066
01:42:50,330 --> 01:42:51,398
就是你要反馈
2067
01:42:51,640 --> 01:42:54,700
不能再像那个图像语言那种是闭环
2068
01:42:54,700 --> 01:42:56,316
它是死的不能动的
2069
01:42:56,450 --> 01:42:58,518
但是这东西也是相辅相成
2070
01:42:58,520 --> 01:43:01,308
我们也不能够过分的强调互动这个事情
2071
01:43:01,500 --> 01:43:04,866
就是人为什么能在互动中学会这种行为
2072
01:43:04,880 --> 01:43:09,155
是因为他本身的DNA就是我们的父母传给我们这个能力
2073
01:43:09,170 --> 01:43:11,770
已经能够足够一个很好的平台
2074
01:43:12,130 --> 01:43:13,750
就是这个基础来做这个事情
2075
01:43:13,910 --> 01:43:17,344
你就想想如果是一个一只猴子或者只猫
2076
01:43:18,010 --> 01:43:19,484
哪怕他有很好的机械能力
2077
01:43:19,490 --> 01:43:21,548
他也不能做到人这样很多的操作
2078
01:43:21,550 --> 01:43:22,090
为什么呢
2079
01:43:22,160 --> 01:43:24,816
因为它的智能的基建基准就不是很高
2080
01:43:25,030 --> 01:43:26,948
所以说我的东西是个相辅相成的
2081
01:43:26,960 --> 01:43:28,890
说我们要让让他有互动
2082
01:43:29,030 --> 01:43:31,440
可以去吸收反馈的消息
2083
01:43:31,440 --> 01:43:35,680
那我们先得把这个智能的初始值提到一定程度
2084
01:43:35,680 --> 01:43:37,534
这才能会更大的闭环
2085
01:43:37,540 --> 01:43:39,870
不然就是它的周期很长
2086
01:43:40,070 --> 01:43:41,966
比如说我们之前的增强学习
2087
01:43:42,070 --> 01:43:47,844
就是喜欢连装的东西都装个几万次、几十万次才学到
2088
01:43:47,850 --> 01:43:49,943
其实这东西是完全是有点浪费
2089
01:43:49,950 --> 01:43:53,875
我们其实我们发现就是说我们能够学一个很好的监督学习
2090
01:43:53,890 --> 01:43:55,738
然后把它抓取到一定的程度
2091
01:43:55,850 --> 01:43:57,730
然后再他去迁移就很快
2092
01:43:57,870 --> 01:43:59,911
所以说我们在抢救反馈的时候
2093
01:44:00,030 --> 01:44:04,846
也不要忘了这个基准线的初始这个智能的一个这样的一个基准线
2094
01:44:04,860 --> 01:44:05,640
这也很重要
2095
01:44:05,640 --> 01:44:07,850
不然它的交互是一个非常耗时
2096
01:44:07,860 --> 01:44:08,896
非常浪费资源的
2097
01:44:09,180 --> 01:44:09,450
好
2098
01:44:09,450 --> 01:44:10,128
我就讲这一点
2099
01:44:12,350 --> 01:44:14,410
接着罗老师这个想法说
2100
01:44:14,720 --> 01:44:20,656
那就是说这个其实跟大模型的这个预训练和我们说的这个自适应
2101
01:44:20,670 --> 01:44:23,766
或者叫做有经有烟度的金条
2102
01:44:23,770 --> 01:44:29,440
其实还是比较类似的这样的一个技术的一个路线
2103
01:44:30,030 --> 01:44:30,210
对
2104
01:44:30,210 --> 01:44:33,312
也是能够做到像孙老师说的这种多任务的这种方式
2105
01:44:34,690 --> 01:44:35,020
行
2106
01:44:35,140 --> 01:44:37,444
那也是感谢三位老师
2107
01:44:37,600 --> 01:44:39,680
然后我们我这还有下一个问题
2108
01:44:40,450 --> 01:44:45,264
下一个问题是就是因为我本身是做自然源处理的和那个人机对话的
2109
01:44:45,320 --> 01:44:48,363
然后也非常自然的切到大模型这块来了
2110
01:44:49,350 --> 01:44:53,662
我们做自然语言处理其实都感觉到就是从去年的11月底的时候
2111
01:44:53,840 --> 01:44:56,722
就迎来了这种技术的革新
2112
01:44:56,730 --> 01:45:02,427
就是好多的这种任务就被这个大模型这样的一个技术给取代了
2113
01:45:02,440 --> 01:45:04,351
就是好多的任务就中间的任务
2114
01:45:04,360 --> 01:45:07,390
中间的一些技术可能就不需要做了
2115
01:45:08,730 --> 01:45:13,750
但是其实我们我通过今天的听三位老师的报告
2116
01:45:13,760 --> 01:45:18,377
我发现这个巨人智能其实它比自然语言处理
2117
01:45:18,390 --> 01:45:21,280
或者比传统的这种人机对话的这种任务
2118
01:45:21,930 --> 01:45:24,538
它是相对来说它的技术链条会比较长
2119
01:45:25,780 --> 01:45:27,180
包括他从前端的
2120
01:45:27,180 --> 01:45:29,260
就是我记得孙老师是有一个图
2121
01:45:29,730 --> 01:45:32,800
前面从语音语义的理解
2122
01:45:33,160 --> 01:45:34,616
然后动作的拆分
2123
01:45:34,620 --> 01:45:35,520
指令的规划
2124
01:45:35,520 --> 01:45:36,486
然后策略学习
2125
01:45:36,660 --> 01:45:38,640
然后到跟控制系统的交互
2126
01:45:39,020 --> 01:45:39,980
整个过程之中
2127
01:45:39,980 --> 01:45:41,650
它的技术链条特别的长
2128
01:45:42,900 --> 01:45:45,132
我就在想请几位老师看一看
2129
01:45:45,140 --> 01:45:52,884
就分析一下大模型在后续的一段时间内能不能像在自然处理这边带来突破
2130
01:45:52,910 --> 01:45:56,860
这样去带来具身智能这块的大跨度的一个进展
2131
01:46:00,400 --> 01:46:00,710
对
2132
01:46:00,710 --> 01:46:03,471
那请孙老师还是请孙老师
2133
01:46:03,480 --> 01:46:05,838
我是我是比较乐观的
2134
01:46:07,310 --> 01:46:11,704
就是说把大模型跟机器人结合
2135
01:46:12,440 --> 01:46:15,556
在带动机器人的具身智能方面我是比较乐观
2136
01:46:16,120 --> 01:46:16,948
因为为什么呢
2137
01:46:17,320 --> 01:46:18,848
我们经常说机器人
2138
01:46:19,740 --> 01:46:21,567
机器人它是机器加人
2139
01:46:22,450 --> 01:46:24,250
第一个我们要做好一个机器
2140
01:46:24,260 --> 01:46:26,630
这个机器的话像人的四肢一样发达
2141
01:46:27,070 --> 01:46:28,348
它的平衡能力
2142
01:46:28,820 --> 01:46:30,449
包括它的这种灵巧性
2143
01:46:31,040 --> 01:46:33,009
第二个就是人怎么去做它
2144
01:46:33,440 --> 01:46:39,351
而大模型的话恰恰是很接近人脑的这样一个通用智能
2145
01:46:39,630 --> 01:46:44,076
就像我们刚才这个错误的报告的最后一页
2146
01:46:44,110 --> 01:46:45,000
讲的很清楚
2147
01:46:45,630 --> 01:46:48,066
通用机械是人工智能的终极形态
2148
01:46:48,320 --> 01:46:50,420
具身智能最核心的灵魂
2149
01:46:51,020 --> 01:46:53,960
大模型在这里面我觉得起一个非常重要的作用
2150
01:46:54,370 --> 01:46:57,090
因为我刚才其实我最后也谈到了一点
2151
01:46:57,380 --> 01:47:00,818
就是其实对话大模型就是大模型中的一种
2152
01:47:01,560 --> 01:47:05,124
我们也谈到了将来也会有认知大模型和行为大模型
2153
01:47:05,960 --> 01:47:13,608
这些共同作用最后就给机器人装上一个跟人的大脑最接近的这么一个大脑
2154
01:47:13,610 --> 01:47:18,335
然后跟我们把我们人的这种行为控制的很多方法
2155
01:47:18,350 --> 01:47:19,727
通过大模型沟通起来
2156
01:47:20,540 --> 01:47:22,472
而且我今天特别强调了一点
2157
01:47:22,480 --> 01:47:27,356
就是场景库、技能库和资源库的建立是一个重要的中间件
2158
01:47:27,880 --> 01:47:31,699
如果再把车务说的跟数据的利用有机的结合
2159
01:47:32,510 --> 01:47:36,528
去形成知识加数据这样一个过程
2160
01:47:36,710 --> 01:47:40,886
我我我我很觉得就是说未来3到5年
2161
01:47:41,810 --> 01:47:43,768
机器人整个的这个灵巧性
2162
01:47:44,580 --> 01:47:46,699
我觉得会达到一个很高的程度
2163
01:47:47,030 --> 01:47:49,286
将比如说我今天一个博士生
2164
01:47:50,140 --> 01:47:53,260
通过这个人的抓取不同物体的实验
2165
01:47:54,370 --> 01:47:55,497
他让机器去学习
2166
01:47:55,500 --> 01:47:58,339
也能达到人的甚至超过人的这个灵巧性
2167
01:47:58,630 --> 01:47:59,750
这个是比较单一
2168
01:47:59,750 --> 01:48:01,028
是行为层面的
2169
01:48:01,410 --> 01:48:02,814
那么机器如何去规划
2170
01:48:03,350 --> 01:48:04,470
像类似这些方面
2171
01:48:04,470 --> 01:48:08,958
我就觉得需要把知识跟数据学习的结合
2172
01:48:09,920 --> 01:48:11,936
而这个方面我想在未来3到5年
2173
01:48:11,940 --> 01:48:13,284
随着大模型的应用
2174
01:48:13,670 --> 01:48:18,070
尤其是大模型在机器人多任务聚生方面的应用
2175
01:48:18,520 --> 01:48:23,560
我想会带动机器人的具身智能迈向一个新的高度
2176
01:48:23,570 --> 01:48:24,830
这是我个人的看法
2177
01:48:27,070 --> 01:48:28,000
谢谢孙老师
2178
01:48:28,160 --> 01:48:31,840
然后我在张老师发表了这个观点之前
2179
01:48:31,850 --> 01:48:34,259
我还是想请教一下孙老师
2180
01:48:34,630 --> 01:48:37,136
因为孙老师您这个报告中也提到
2181
01:48:38,230 --> 01:48:39,550
我们说的这种知识
2182
01:48:39,810 --> 01:48:43,285
或者我也看到您展现出来了几种不同形态的这种知识图谱
2183
01:48:44,540 --> 01:48:45,476
我在我的看来
2184
01:48:45,480 --> 01:48:50,900
在我看来是不是说这种我们说的这种符号化的
2185
01:48:50,900 --> 01:48:55,918
以语言的这种知识图谱而符号化的这种知识图谱的这种形式
2186
01:48:56,650 --> 01:49:04,264
能够指导这个机器人来在真实的场景中进行一定的场景的理解
2187
01:49:04,610 --> 01:49:05,709
那这个过程之中
2188
01:49:05,710 --> 01:49:15,034
其实有点像是我们说的传统的这种符号主义和我们最终的行为主义之间的一个连接
2189
01:49:15,480 --> 01:49:16,880
那这种过程之中
2190
01:49:18,120 --> 01:49:25,611
那您认为是不是这种知识的就是这种符号化的这种知识能能够能够怎么说呢
2191
01:49:25,800 --> 01:49:30,683
能够在有限的机器人在真实场景的探索之中
2192
01:49:30,940 --> 01:49:32,370
能给他们进行一定的指导
2193
01:49:32,370 --> 01:49:39,552
并减少他们在学习过程之中的这种模型学习的这种收敛的快慢
2194
01:49:39,790 --> 01:49:40,130
对
2195
01:49:40,300 --> 01:49:40,610
是的
2196
01:49:40,610 --> 01:49:43,214
我我大概我是赞成你这个观点的
2197
01:49:43,220 --> 01:49:49,824
其实咱们的预训练模型最早的也是把这个卷积网络把它分化
2198
01:49:51,180 --> 01:49:55,080
跟ASTN就是循环网络结合
2199
01:49:55,480 --> 01:49:57,448
最后通过一个很重要的部分
2200
01:49:57,450 --> 01:50:03,417
就是每一个图像块的patch的词和向量的这种token
2201
01:50:03,560 --> 01:50:04,049
对对对
2202
01:50:04,250 --> 01:50:06,378
token的这种嵌入来实现的
2203
01:50:06,900 --> 01:50:08,935
其实我们这个技能库里面
2204
01:50:09,760 --> 01:50:12,905
技能方面那个机缘实际上就是个词嵌入
2205
01:50:13,550 --> 01:50:15,670
它既有表示动作去
2206
01:50:15,890 --> 01:50:17,980
它也有属性参数向量表示
2207
01:50:18,130 --> 01:50:20,650
比如说往哪个方向去
2208
01:50:21,140 --> 01:50:22,150
速度是多少
2209
01:50:23,650 --> 01:50:26,710
这个也是有一个向量跟它嵌入在一起的
2210
01:50:26,980 --> 01:50:28,510
所以他也用到了词嵌入
2211
01:50:28,910 --> 01:50:32,570
这样就把数据学习跟知识融为一体
2212
01:50:32,900 --> 01:50:38,246
再通过注意力机制来提高刚才您谈到的学习的效率
2213
01:50:38,670 --> 01:50:40,938
就是我通过知识我就告诉机器人
2214
01:50:41,110 --> 01:50:42,540
你要做就应该这么去做
2215
01:50:42,880 --> 01:50:44,788
特别是我们刚才谈到了一个
2216
01:50:44,790 --> 01:50:47,640
就是我有大量的样例如何形成模板
2217
01:50:47,860 --> 01:50:50,794
而这个模板实际上就是我们能做事情当中
2218
01:50:50,990 --> 01:50:55,796
大家遵循的一个最捷径的最优的一个突破
2219
01:50:56,420 --> 01:50:57,380
我按照他去做
2220
01:50:57,690 --> 01:51:01,090
所以我就能够实现即便是场景有所改变
2221
01:51:01,500 --> 01:51:04,019
我也只要通过少样本学习
2222
01:51:04,200 --> 01:51:06,070
就可以实现这样一个矩阵优化
2223
01:51:09,190 --> 01:51:09,740
好的
2224
01:51:09,740 --> 01:51:10,590
谢谢孙老师
2225
01:51:11,450 --> 01:51:13,672
张老师您接着上一个问题
2226
01:51:13,680 --> 01:51:16,480
就是您觉得这个大模型在后续的发展
2227
01:51:16,490 --> 01:51:23,082
会不会是将在自然语言处理这边带来的这种颠覆式的或者是跨越式的提升
2228
01:51:23,750 --> 01:51:27,062
我刚才我就是郑老师讲的已经挺好的
2229
01:51:27,820 --> 01:51:34,237
你的确是你想一想我们我们还是看我觉得还是要看就大家不理解的时候
2230
01:51:34,260 --> 01:51:35,324
我们这个来看人
2231
01:51:35,500 --> 01:51:36,643
我们人是怎么来做呢
2232
01:51:36,650 --> 01:51:38,558
我们怎么来让小孩子来学的
2233
01:51:38,800 --> 01:51:39,620
就学知识
2234
01:51:40,350 --> 01:51:42,781
如果说当模型出来了以后的话
2235
01:51:42,790 --> 01:51:45,706
我们说知识是不是获取知识的这一种途径
2236
01:51:45,930 --> 01:51:48,178
我们更效率更高了
2237
01:51:48,340 --> 01:51:49,390
途径更多了
2238
01:51:50,370 --> 01:51:53,313
这一种手段更丰富了
2239
01:51:53,840 --> 01:51:56,128
所以说从这个层面上来说
2240
01:51:56,380 --> 01:51:59,513
就是训练这个机器人这个手段
2241
01:51:59,520 --> 01:52:04,680
我们大模型完全是可以在这里面发挥非常重要的作用的
2242
01:52:04,960 --> 01:52:07,600
甚至于刚才提到说符号主义
2243
01:52:08,140 --> 01:52:09,058
逻辑主义也好
2244
01:52:09,330 --> 01:52:10,248
社会主义也好
2245
01:52:10,250 --> 01:52:16,830
我们是实际上是可以把这几个是不是融合起来
2246
01:52:16,830 --> 01:52:19,350
我们来训练这个机器人
2247
01:52:19,500 --> 01:52:24,379
是他掌握所我们这里面所谓的这个知识
2248
01:52:25,360 --> 01:52:25,640
对
2249
01:52:25,960 --> 01:52:31,592
还是希望他能能通过这种知识的灌输
2250
01:52:31,720 --> 01:52:35,318
来转换成技能的提升这一个层面
2251
01:52:35,850 --> 01:52:40,972
我觉得尤其是生成式的这个大模型是可以来发挥重要的作用
2252
01:52:41,810 --> 01:52:43,721
不光是在你们自然员处理身上
2253
01:52:43,730 --> 01:52:45,826
在CV这个领域里
2254
01:52:46,380 --> 01:52:47,416
甚至于说机器人
2255
01:52:47,420 --> 01:52:50,048
因为机器人我们的确是第一个是场景理解
2256
01:52:50,230 --> 01:52:52,990
第二个是我们要做行为反馈
2257
01:52:53,200 --> 01:52:56,780
这个行为我们也有别于传统的自动化这个专业
2258
01:52:56,880 --> 01:52:59,220
咱们这里面传统的这种方法
2259
01:52:59,390 --> 01:53:03,030
我们是不是这个行为也可以生成
2260
01:53:03,840 --> 01:53:12,656
而且这一个层面我们也明白张老师就是从孙老师讲的那个动作机缘的那个角度来说的话
2261
01:53:12,930 --> 01:53:14,330
确实是可以生成的
2262
01:53:14,510 --> 01:53:16,406
我认为个人认为对
2263
01:53:16,410 --> 01:53:18,217
甚至于说我们来做出一些东西
2264
01:53:18,220 --> 01:53:21,195
相当于说我们人不是来规划的这个行为
2265
01:53:22,660 --> 01:53:25,850
我们是不是用非常经济的
2266
01:53:25,850 --> 01:53:28,300
比如说搜索这一种方式
2267
01:53:28,300 --> 01:53:31,657
我们跟行为结合起来
2268
01:53:32,040 --> 01:53:36,200
而是机器人所做的这个动作更更经济
2269
01:53:36,200 --> 01:53:37,320
性价比更高
2270
01:53:37,610 --> 01:53:38,935
这个怎么样
2271
01:53:38,970 --> 01:53:45,028
我就觉得这个大模型可以起到一个一尤其是预训练这个模型
2272
01:53:45,040 --> 01:53:46,305
我们觉得是可以发挥作用
2273
01:53:49,260 --> 01:53:49,720
好的
2274
01:53:49,720 --> 01:53:50,410
谢谢张老师
2275
01:53:50,560 --> 01:53:51,316
我也总结一下
2276
01:53:51,440 --> 01:54:01,890
张老师大概的意思就是其实大模型张老师想开始表达的意思就是大模型其实它能作为无论是数据的提供的一个比较
2277
01:54:01,890 --> 01:54:03,353
便利的一个方式
2278
01:54:03,390 --> 01:54:06,170
或者是知识的一个演示
2279
01:54:06,170 --> 01:54:08,498
知识的这种知识库这种方式
2280
01:54:08,500 --> 01:54:13,330
其实它都能够在机器人训练的和学习的过程之中
2281
01:54:13,600 --> 01:54:15,810
提供很好的经验和知识
2282
01:54:17,850 --> 01:54:18,030
对
2283
01:54:18,030 --> 01:54:18,260
行
2284
01:54:18,890 --> 01:54:23,450
卢老师您看会不会到一个GP3时刻
2285
01:54:23,460 --> 01:54:24,699
或者在GP时刻
2286
01:54:24,700 --> 01:54:25,796
我也是比较乐观的
2287
01:54:25,800 --> 01:54:28,096
我也是跟孙老师还有张老师一样
2288
01:54:28,100 --> 01:54:29,000
是比较乐观的
2289
01:54:29,730 --> 01:54:31,615
这个是因为技术也到了这一步
2290
01:54:31,620 --> 01:54:32,726
大模型也贪通了
2291
01:54:32,730 --> 01:54:33,810
无非就是一种表征
2292
01:54:34,370 --> 01:54:35,318
然后它的空间
2293
01:54:35,320 --> 01:54:37,324
因为机械还有物理约束空间
2294
01:54:37,330 --> 01:54:40,435
它其实比语言更加的不会那么复杂
2295
01:54:40,440 --> 01:54:41,420
我是比较相信的
2296
01:54:41,420 --> 01:54:43,250
而且又我们发现了一点
2297
01:54:43,470 --> 01:54:45,090
就现在做的越来越多了
2298
01:54:45,330 --> 01:54:48,795
这个其实速度跟你的科研人员的数量什么比例的
2299
01:54:48,920 --> 01:54:52,587
我感觉现在从北美的话基本就是好多人在做
2300
01:54:52,600 --> 01:54:54,056
然后中国很多在做
2301
01:54:54,860 --> 01:54:57,400
所以这么大的科研体量
2302
01:54:57,400 --> 01:54:59,276
再加上经费投入
2303
01:54:59,280 --> 01:55:01,403
再加上这个科学上的问题
2304
01:55:01,410 --> 01:55:05,723
我觉得不是那么就不不是那么不可跨过去的
2305
01:55:05,740 --> 01:55:07,570
所以我也是比较乐观的
2306
01:55:07,570 --> 01:55:09,532
对于三五年这个事情
2307
01:55:09,540 --> 01:55:11,780
我也是跟孙老师的看法是一样的
2308
01:55:12,300 --> 01:55:13,440
或者是两三年
2309
01:55:13,440 --> 01:55:15,225
所以说就明年我也不知道anyway
2310
01:55:15,400 --> 01:55:18,580
这个事情是相对是乐观的
2311
01:55:21,270 --> 01:55:22,080
好的
2312
01:55:22,080 --> 01:55:22,880
谢谢罗老师
2313
01:55:23,570 --> 01:55:24,476
因为时间原因
2314
01:55:24,660 --> 01:55:26,493
然后我们也是留最后一个问题
2315
01:55:26,800 --> 01:55:27,436
最后一个问题
2316
01:55:27,440 --> 01:55:33,742
其实我是替在座的各位研究生观众们提来的提出来的
2317
01:55:34,370 --> 01:55:39,254
因为如果我们在座的有研究生正在等待这个选方向
2318
01:55:39,270 --> 01:55:42,960
或者是未来的研究的这个方向的话
2319
01:55:42,960 --> 01:55:46,200
那几位老师看看在具身智能这个领域的话
2320
01:55:46,880 --> 01:55:49,985
更加推荐哪些切入点
2321
01:55:49,990 --> 01:55:52,234
或者是哪些比较有意思的
2322
01:55:52,240 --> 01:55:54,892
或者比较前沿的一些研究方向
2323
01:55:55,010 --> 01:55:59,403
对那那个还是请孙老师来给我们这帮学生来解答一下
2324
01:55:59,420 --> 01:56:02,500
我的学生现在在做这么两个方向
2325
01:56:02,770 --> 01:56:04,466
第一个就是跨模态
2326
01:56:05,000 --> 01:56:10,512
就是我们人我们人无论是交互还是行为都通过跨模态实现的
2327
01:56:10,950 --> 01:56:14,044
那么人的这种跨模态对行为的这种影响
2328
01:56:14,550 --> 01:56:17,385
所以也是我们团队现在在做的一个很重要的方向
2329
01:56:17,400 --> 01:56:18,424
我觉得这个很重要
2330
01:56:18,760 --> 01:56:19,210
第二个
2331
01:56:19,300 --> 01:56:25,306
我个人觉得就是要建立大型场景这个技能和机缘库
2332
01:56:25,940 --> 01:56:28,976
这个是我觉得是对接未来
2333
01:56:28,980 --> 01:56:29,870
就是把符号
2334
01:56:29,870 --> 01:56:34,570
就刚才您提到的和数据学习结合起来的一个重要的中间件
2335
01:56:34,580 --> 01:56:36,920
也是大模型应用的一个重要中间件
2336
01:56:37,320 --> 01:56:39,840
而这里问题不像我们想象那么简单
2337
01:56:40,310 --> 01:56:42,410
刚才我们谈到的不光是有运动技能
2338
01:56:42,420 --> 01:56:44,220
还有情感各种技能
2339
01:56:44,220 --> 01:56:46,407
如何表示这个结合点
2340
01:56:46,410 --> 01:56:48,785
我觉得是未来大模型的比较重要的一个方向
2341
01:56:49,550 --> 01:56:55,320
也是具身智能面向多任务我觉得比较重要的一个桥梁
2342
01:56:57,360 --> 01:56:57,890
好的
2343
01:56:57,890 --> 01:56:58,199
孙老师
2344
01:56:58,200 --> 01:57:05,508
那您是否认为将来会有一个universal的一个机缘的这种动作库的出现呢
2345
01:57:05,790 --> 01:57:07,770
就是适应全程所有的场景
2346
01:57:08,390 --> 01:57:10,147
他要他要建出来
2347
01:57:10,330 --> 01:57:12,838
要考虑所有的场景是这样
2348
01:57:14,130 --> 01:57:15,600
当然这里面他也会迁移
2349
01:57:16,360 --> 01:57:17,998
你比如说我们有个学生也在做
2350
01:57:18,010 --> 01:57:20,290
就是说我以前没见过的目标
2351
01:57:21,100 --> 01:57:22,269
我怎么能判断出
2352
01:57:22,520 --> 01:57:25,666
我们就经常讲的叫感知新事物
2353
01:57:25,670 --> 01:57:26,650
适应新环境
2354
01:57:27,130 --> 01:57:29,341
像这个大模型就非常有用
2355
01:57:30,130 --> 01:57:31,178
其他没有
2356
01:57:32,650 --> 01:57:33,000
好的
2357
01:57:33,000 --> 01:57:33,640
谢谢孙老师
2358
01:57:34,350 --> 01:57:37,486
张老师您给这个研究生的选题的建议
2359
01:57:37,490 --> 01:57:38,526
对我刚才是这样
2360
01:57:38,530 --> 01:57:41,233
我刚才也提到了第一个怎么来识别场景
2361
01:57:41,830 --> 01:57:42,740
对场景理解
2362
01:57:43,310 --> 01:57:45,350
刚才他也提到了那个场景里
2363
01:57:45,350 --> 01:57:50,010
我当然第一个是我们怎么识别这种动态的场景
2364
01:57:50,150 --> 01:57:52,049
包括场景里面这个人
2365
01:57:52,600 --> 01:57:54,628
我们课题组这里面这两年聚焦
2366
01:57:54,870 --> 01:57:56,256
包括就像人的话
2367
01:57:56,260 --> 01:57:59,665
我们怎么来情进行情感计算那场景
2368
01:57:59,670 --> 01:58:03,014
然后机器人的话怎么来理解这里面这个场景
2369
01:58:03,270 --> 01:58:04,758
包括动的这个物体
2370
01:58:05,490 --> 01:58:06,210
这一个层面
2371
01:58:06,410 --> 01:58:08,543
我们可以是跨模态了
2372
01:58:08,670 --> 01:58:14,478
甚至于说可以把语言和视觉结合起来
2373
01:58:15,090 --> 01:58:16,128
可以做一些事
2374
01:58:16,680 --> 01:58:17,480
另外一块
2375
01:58:17,480 --> 01:58:23,171
我觉得因为聚生针对如果说这把弄弄到机器人上
2376
01:58:23,180 --> 01:58:33,580
就相当于机器人那个行为怎么来进行这个计算这一块我我觉得我们也是在布局这方面的工作
2377
01:58:34,790 --> 01:58:39,430
就像机器人行为我们怎么来进行表征和和学习
2378
01:58:41,420 --> 01:58:41,740
好
2379
01:58:43,430 --> 01:58:43,859
张老师
2380
01:58:44,160 --> 01:58:44,988
我也补充一下
2381
01:58:44,990 --> 01:58:46,028
补充问您一下
2382
01:58:46,030 --> 01:58:48,142
就是您说的机器人的行为
2383
01:58:48,810 --> 01:58:54,058
包括我们说的比如说一个机械臂也好
2384
01:58:54,070 --> 01:58:55,430
或者它的一个躯干也好
2385
01:58:55,700 --> 01:59:00,980
它的整体的所有的关节的的一个向量化的一个表征
2386
01:59:01,000 --> 01:59:02,356
这块不是不算
2387
01:59:02,470 --> 01:59:05,846
这我觉得这就是这一传统的这种思路
2388
01:59:07,240 --> 01:59:12,595
就相当于说我们如果说针对还拿这个机械臂的话
2389
01:59:12,600 --> 01:59:21,280
那如果说机械臂来前面有物体或者是前面有有要做这个行为的话
2390
01:59:21,280 --> 01:59:25,150
那么这个行为怎么来来生成和表达
2391
01:59:25,330 --> 01:59:28,788
我觉得这里面有一个新的想法
2392
01:59:29,590 --> 01:59:30,976
有别于说我们每一个关节
2393
01:59:30,980 --> 01:59:33,564
我们都要计算它的这个运动学和动力学
2394
01:59:34,690 --> 01:59:36,890
虽然是先不按照这一个来
2395
01:59:38,860 --> 01:59:39,670
明白了
2396
01:59:40,200 --> 01:59:40,520
好的
2397
01:59:40,520 --> 01:59:41,160
谢谢张老师
2398
01:59:41,680 --> 01:59:42,400
卢老师
2399
01:59:46,110 --> 01:59:49,710
两位老师已经把关键的内容都说了
2400
01:59:50,140 --> 01:59:51,364
我觉得这边差不多
2401
01:59:52,090 --> 01:59:54,169
然后我其实这边有些题目
2402
01:59:54,410 --> 01:59:57,794
但其实我也不想去过度说是我们做的题目
2403
01:59:59,720 --> 02:00:00,518
就很建议大家
2404
02:00:00,700 --> 02:00:02,247
我意思是想我想表达一个观点
2405
02:00:02,250 --> 02:00:05,358
就是居身智能是一个拓荒区
2406
02:00:05,360 --> 02:00:07,153
就是到处都是荒芜的土地
2407
02:00:07,480 --> 02:00:11,286
其实你可以去就包括前面两位老师也做建议的也好
2408
02:00:11,300 --> 02:00:13,748
或者说你可以把每年的论文的列表看一下
2409
02:00:13,760 --> 02:00:17,022
我觉得每个领域都值得去投入做
2410
02:00:17,230 --> 02:00:18,634
然后的话你只要进去
2411
02:00:18,640 --> 02:00:20,875
很多时候都能成为这个领域的先驱
2412
02:00:21,080 --> 02:00:26,248
就是我我觉得这个特别有意思的问题就是之前我们做一些相对比较传统的方向
2413
02:00:26,280 --> 02:00:29,262
几个学员都会打架说我们想到一个idea去了
2414
02:00:29,680 --> 02:00:32,800
但是当我们在军人智能发现我们学生随便想个idea
2415
02:00:33,070 --> 02:00:33,638
想完之后
2416
02:00:33,830 --> 02:00:35,205
这个东西好像是都没人做
2417
02:00:35,210 --> 02:00:36,380
连reference都没有
2418
02:00:36,380 --> 02:00:37,588
那我就第一个做了
2419
02:00:37,790 --> 02:00:41,358
所以说我觉得这里面是我我我的看法
2420
02:00:41,370 --> 02:00:43,830
居然智能它和其他的这些不太一样
2421
02:00:43,840 --> 02:00:45,086
它是造一个火箭
2422
02:00:45,340 --> 02:00:47,188
他这个东西的要求的东西特别多
2423
02:00:47,200 --> 02:00:50,658
各式各样的技术技能都要去做
2424
02:00:50,670 --> 02:00:51,998
所以他是个全站式
2425
02:00:53,000 --> 02:00:54,452
或者说我们说更像造一辆车
2426
02:00:54,890 --> 02:00:57,338
就是每一个零部件都有很solid
2427
02:00:57,630 --> 02:00:59,240
你才能够整辆车跑起来
2428
02:00:59,410 --> 02:01:01,880
所以导致到要做的东西太多了
2429
02:01:01,890 --> 02:01:04,998
所以我是想是我也不说具体哪个
2430
02:01:05,600 --> 02:01:07,019
不会去具体列出一些方向
2431
02:01:07,020 --> 02:01:11,020
其实我觉得现在我是我的看法是现在就相当于像早期一样
2432
02:01:11,700 --> 02:01:15,960
你随便进入哪个领域都能成为这个领域的先驱
2433
02:01:15,960 --> 02:01:18,584
就像早年我们其实比如说在视觉早期
2434
02:01:18,590 --> 02:01:23,756
我们随便做一个跟踪分割或者是目标检测
2435
02:01:24,040 --> 02:01:26,728
甚至是那种简单的OKA flow
2436
02:01:26,760 --> 02:01:27,348
都能闪现
2437
02:01:27,350 --> 02:01:28,138
是非常高
2438
02:01:28,140 --> 02:01:28,830
都能上
2439
02:01:28,830 --> 02:01:31,448
所以说我是我我就表达一个观点
2440
02:01:31,450 --> 02:01:33,889
就是现在其实都很好
2441
02:01:33,890 --> 02:01:37,330
每个领域都很大的空间
2442
02:01:37,840 --> 02:01:41,044
就是前面两位老师讲的那个那个各个领域
2443
02:01:41,140 --> 02:01:42,757
各个几个关键问题也好说
2444
02:01:42,960 --> 02:01:44,208
你可以拉一下列表
2445
02:01:46,520 --> 02:01:49,446
都有很多这样的也可以做
2446
02:01:49,450 --> 02:01:50,660
而且做完之后都有贡献
2447
02:01:50,660 --> 02:01:52,616
它会组成一辆统一的一辆车
2448
02:01:52,620 --> 02:01:54,156
就是会像坐车一样
2449
02:01:54,160 --> 02:01:56,972
每个零件组装起来都为这个东西会产生贡献
2450
02:01:57,320 --> 02:01:58,504
我只表达这个观点
2451
02:01:58,510 --> 02:02:01,270
具体的那个方向我也觉得不用推荐
2452
02:02:01,270 --> 02:02:03,646
其实做哪个方向都挺好的
2453
02:02:03,650 --> 02:02:05,750
因为这个不存在说哪个更好
2454
02:02:05,760 --> 02:02:07,090
因为它太早期了
2455
02:02:07,090 --> 02:02:07,430
现在
2456
02:02:07,850 --> 02:02:10,049
好好的
2457
02:02:10,050 --> 02:02:10,840
谢谢罗老师
2458
02:02:11,240 --> 02:02:17,420
也再次感谢孙老师和张老师能在百忙之中给我们带来这么精彩的报告
2459
02:02:17,440 --> 02:02:21,490
和我们这么精彩的观点的一个分享
2460
02:02:22,710 --> 02:02:24,657
今天我们由于时间的原因
2461
02:02:24,660 --> 02:02:27,737
我们今天的c talk就到此为止了
2462
02:02:28,710 --> 02:02:30,899
也非常感谢咱们三位嘉宾
2463
02:02:31,430 --> 02:02:38,064
为我们带来这个狙神智能及大模型相关的这个概念的多角度的一个解读
2464
02:02:38,570 --> 02:02:44,940
也非常感谢一直在线上收看到现在的这各位朋友然后我看了一下刚才的后台数据
2465
02:02:44,940 --> 02:02:48,156
可能咱们的这个观看人数应该过万了
2466
02:02:48,160 --> 02:02:48,440
快
2467
02:02:48,860 --> 02:02:49,210
对
2468
02:02:49,810 --> 02:02:51,070
当然也是时间的原因
2469
02:02:51,070 --> 02:02:53,369
有一些观众提出来的问题
2470
02:02:53,830 --> 02:03:01,350
我们不能一一拿来提问和讨论了哈那那最后也是感谢各位老师和各位观众
2471
02:03:01,740 --> 02:03:02,937
我们下周的同一时间
2472
02:03:03,050 --> 02:03:05,130
也欢迎继续来到咱们的直播间
2473
02:03:05,400 --> 02:03:09,579
关注其他的计算机领域的术语解读和相关的知识
2474
02:03:10,200 --> 02:03:11,772
今天的这个活动就到这里了
2475
02:03:12,250 --> 02:03:12,890
谢谢大家
2476
02:03:13,120 --> 02:03:13,720
大家再见
2477
02:03:14,350 --> 02:03:14,750
好
2478
02:03:15,080 --> 02:03:15,968
再见文兰老师
2479
02:03:16,130 --> 02:03:17,300
再见孙老师
2480
02:03:17,680 --> 02:03:19,318
再见再见再见孙老师
2481
02:03:19,320 --> 02:03:20,310
再见张老师
2482
02:03:20,870 --> 02:03:22,110
还有伟楠
2483
02:03:22,110 --> 02:03:22,660
谢谢
2484
02:03:23,120 --> 02:03:23,340
好
2485
02:03:23,340 --> 02:03:24,508
谢谢各位两位老师
2486
02:03:25,560 --> 02:03:26,550
谢谢孙老师
2487
02:03:27,860 --> 02:03:28,350
再见
2488
02:03:29,610 --> 02:03:32,080
如果老师保重身体真的感冒了
2489
02:03:32,260 --> 02:03:33,415
有这效果不太好
2490
02:03:33,420 --> 02:03:33,790
抱歉
2491
02:03:35,160 --> 02:03:36,039
没挺好
2492
02:03:37,300 --> 02:03:37,660
好
2493
02:03:38,390 --> 02:03:38,690
拜拜
2494
02:03:38,690 --> 02:03:39,040
再见
