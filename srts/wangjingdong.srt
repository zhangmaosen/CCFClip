1
00:00:00,170 --> 00:00:05,846
下面我们以热烈的掌声来欢迎中国计算机学会计算机视觉专委会的常务委员
2
00:00:06,190 --> 00:00:09,670
百度计算机视觉首席科学家王景东
3
00:00:09,670 --> 00:00:14,470
给大家带来自监督与训练在工业视觉中应用的主提报告
4
00:00:14,480 --> 00:00:15,070
大家欢迎
5
00:00:36,630 --> 00:00:37,990
大家上午好
6
00:00:38,670 --> 00:00:46,626
今天跟大家汇报的工作是自建陆域训练或者视觉大模型在工业视觉里面的应用
7
00:00:48,570 --> 00:00:55,131
我们首先看一些在工业场景里面跟视觉相关的一些典型的例子
8
00:00:55,480 --> 00:00:58,604
第一个是工业质检相关的
9
00:00:58,900 --> 00:01:00,583
比如说有半导体芯片
10
00:01:01,910 --> 00:01:03,515
汽车零部件
11
00:01:04,150 --> 00:01:07,462
然后下面还有比如说产品的
12
00:01:07,470 --> 00:01:10,530
还有医学诊断等等相关的纺织品相关的
13
00:01:10,770 --> 00:01:13,500
这些都涉及到工业质检的一些问题
14
00:01:17,960 --> 00:01:22,890
然后第二个一个典型的场景是安全巡检
15
00:01:23,320 --> 00:01:26,035
这里面可以看到有的是跟人相关的
16
00:01:26,570 --> 00:01:30,476
你的这个比如说你的工着装是不是很规范
17
00:01:31,270 --> 00:01:31,530
对吧
18
00:01:31,530 --> 00:01:33,650
你的操作是不是很规范
19
00:01:33,950 --> 00:01:39,830
当然还有仪表的这个指标状态等等
20
00:01:40,250 --> 00:01:43,851
右边是跟环境相关的一些东西
21
00:01:43,860 --> 00:01:48,700
比如说有没有烟雾、明火等等这样子的东西
22
00:01:49,640 --> 00:01:50,770
这是安全与巡检
23
00:01:54,070 --> 00:01:58,095
这里面是给的一些通道可视化巡检
24
00:01:58,450 --> 00:02:02,989
比如说书电线路通道的环境和状态等等
25
00:02:03,590 --> 00:02:08,360
这些都是对我们安全非常重要的一些场景
26
00:02:08,540 --> 00:02:10,124
下面大家就看到一些例子
27
00:02:10,800 --> 00:02:13,768
有的是比如说吊车撞上了高压线
28
00:02:13,770 --> 00:02:17,289
那我们能不能够通过视觉的方法去预警
29
00:02:17,970 --> 00:02:20,810
或者是火灾、烟雾等等
30
00:02:25,000 --> 00:02:29,500
然后还有一个是无人机的本体缺陷的检测
31
00:02:29,770 --> 00:02:31,066
大家有看到这些例子
32
00:02:31,070 --> 00:02:36,467
典型的例子在这个人员高压线上面等等这样子的
33
00:02:36,480 --> 00:02:39,840
比如说有没有绝缘面的破损
34
00:02:40,030 --> 00:02:43,638
以往我们要通过人去检测
35
00:02:43,790 --> 00:02:44,708
现在今天来讲
36
00:02:44,830 --> 00:02:48,158
通过无人机去拍摄这样的图片
37
00:02:48,160 --> 00:02:52,280
能够通过计算机视觉的算法去自动的去检测
38
00:02:56,230 --> 00:02:59,988
还有前面也提到了设备的状态的巡检
39
00:03:00,500 --> 00:03:05,610
这些都是可以通过视觉的方法去快速的去检测
40
00:03:08,570 --> 00:03:13,210
然后还有一个跟工业视觉相关的就是遥感测绘
41
00:03:14,320 --> 00:03:15,370
大家看可以看到
42
00:03:15,370 --> 00:03:19,530
比如说左边的一些机场的飞机的检测
43
00:03:20,090 --> 00:03:22,394
中间有变化的检测
44
00:03:22,400 --> 00:03:24,597
比如说建筑的变化等等这样子
45
00:03:25,260 --> 00:03:27,930
最右边的是地块的分割
46
00:03:28,640 --> 00:03:30,570
刚才讲的这些例子里面
47
00:03:30,770 --> 00:03:34,228
其实涉及到计算机视觉的算法
48
00:03:34,230 --> 00:03:36,288
主要是两个大类
49
00:03:36,290 --> 00:03:37,610
当然还有其他的一些算法
50
00:03:37,620 --> 00:03:39,060
主要这里面主要两大的
51
00:03:39,060 --> 00:03:39,960
一个是检测
52
00:03:39,960 --> 00:03:41,010
一个是分割
53
00:03:41,470 --> 00:03:42,998
是计算机视觉里面
54
00:03:43,000 --> 00:03:46,800
特别是计算机视觉视觉识别重要的两个任务
55
00:03:46,990 --> 00:03:50,509
左边我们给了一些目标检测的一些例子
56
00:03:50,960 --> 00:03:58,435
目标检测主要是说把目标它的这个所在位置的框给找出来
57
00:03:59,060 --> 00:04:01,748
右边是给些语义分割的一些例子
58
00:04:02,250 --> 00:04:02,680
对吧
59
00:04:04,270 --> 00:04:08,722
这是在工业视觉里面遇到常见的两个基本的问题
60
00:04:09,660 --> 00:04:11,494
在分割领域里面
61
00:04:14,100 --> 00:04:17,568
我们早先大概2020年做了一个算法
62
00:04:17,570 --> 00:04:23,520
现在在这整个计算机视觉领域里面用的比较多的一类算法
63
00:04:24,090 --> 00:04:26,870
称之为OCR net
64
00:04:27,070 --> 00:04:32,866
这个OCR实际上在这里面代表的意思是这个object complete repetition
65
00:04:33,810 --> 00:04:40,506
这样的一个算法是跟今天大家谈的比较多的transformer算法是相关的
66
00:04:41,220 --> 00:04:50,020
这个是第一个去利用这个transformer decoder去做这个语音分割的那这个算法是刚才提到的
67
00:04:50,020 --> 00:04:57,060
是2020年的那同时2020年里面有一个来自于当时还是facebook的一个算法
68
00:04:57,100 --> 00:05:00,556
叫data右边的桌目标检测的一个算法
69
00:05:01,230 --> 00:05:02,328
那就今天来讲
70
00:05:02,500 --> 00:05:04,852
在整个计算机世界领域里面
71
00:05:05,230 --> 00:05:06,950
这个算法是用的最多的
72
00:05:07,090 --> 00:05:09,158
然后慢慢的也走向产业化
73
00:05:09,300 --> 00:05:10,749
走向实际的应用场景
74
00:05:11,860 --> 00:05:16,021
我今天跟大家主要汇报的是关于检测相关的
75
00:05:16,150 --> 00:05:23,648
以及如何利用这个预训练去帮助做检测的说一些工作
76
00:05:25,010 --> 00:05:27,700
首先我们看看目标检测和预训练
77
00:05:30,610 --> 00:05:38,230
这个是个典型的视觉识别的这个是我们称之为流程
78
00:05:38,440 --> 00:05:42,085
通常它也包含这个编码器和解码器
79
00:05:43,350 --> 00:05:46,146
这个编码器大家看看左下角
80
00:05:46,250 --> 00:05:49,715
实际上对应的早先有这个卷积网络
81
00:05:52,060 --> 00:05:56,515
最近有基于这个transformer的这样一个网络结构
82
00:05:57,900 --> 00:05:59,748
右边有解码器
83
00:05:59,940 --> 00:06:00,900
这个非常的多
84
00:06:01,020 --> 00:06:04,890
从这个RCN113年14年开始
85
00:06:04,890 --> 00:06:06,394
RCN到今天来讲
86
00:06:06,600 --> 00:06:09,564
就是基于这个传说的这种方案
87
00:06:10,160 --> 00:06:11,258
包括刚才的data
88
00:06:11,530 --> 00:06:13,077
包括这个OCR net等等
89
00:06:13,080 --> 00:06:15,285
这样子都是基于informal
90
00:06:16,150 --> 00:06:18,910
这是通用的识别的框架
91
00:06:19,030 --> 00:06:21,956
检测相对来说结果出来的不一样
92
00:06:22,140 --> 00:06:25,251
结果最右边的结果就是比如这一条小狗
93
00:06:25,260 --> 00:06:28,608
我们需要把它的这个狗所在的框给框出来
94
00:06:28,610 --> 00:06:29,180
对吧
95
00:06:32,560 --> 00:06:37,505
这个是在过去大概十年左右的时间里面
96
00:06:37,930 --> 00:06:41,398
这个检测解码器的代表方法
97
00:06:42,390 --> 00:06:52,346
刚才提了一个RCN到后来的这个比如说在工业场景里面用的比较多的是这个euro
98
00:06:52,370 --> 00:06:53,650
2015年的euro
99
00:06:53,650 --> 00:06:56,170
现在这个在实际场景用的非常多
100
00:06:56,340 --> 00:06:57,796
但现在有一种趋势
101
00:06:57,800 --> 00:07:03,988
它会逐步的会被劝说这样的方法做代替
102
00:07:05,170 --> 00:07:06,930
刚才提到2020年开始
103
00:07:08,000 --> 00:07:11,094
就是主要是基于facebook这么一个data的工作
104
00:07:11,100 --> 00:07:13,260
慢慢的关注的越来越多
105
00:07:13,260 --> 00:07:15,825
这里面有the formal data
106
00:07:16,620 --> 00:07:19,698
然后还有我们自己的工作肯定性的data
107
00:07:20,590 --> 00:07:25,882
包括后来这个idea做的这个demo
108
00:07:26,180 --> 00:07:28,380
或者我们做的global data等等这样子
109
00:07:29,710 --> 00:07:32,758
这个是基本的代表性的方法
110
00:07:34,150 --> 00:07:40,810
这里面会稍微简单讲一讲为什么这样一个data的方案现在大家关注的越来越多呢
111
00:07:41,890 --> 00:07:44,326
一方面是有正正是因为这个传说
112
00:07:44,650 --> 00:07:48,560
最近几年基本上这里不仅是计算机视觉
113
00:07:48,640 --> 00:07:52,441
当然也包括NOP都是广泛的使用传输摸的方法
114
00:07:52,460 --> 00:07:53,636
号称大一统的方法
115
00:07:56,060 --> 00:07:57,985
这个结构我们会展开去讲
116
00:07:58,260 --> 00:08:04,010
今天这里面在传说里面有一个比较有意思的一件事情
117
00:08:04,270 --> 00:08:05,684
两个应该两个点
118
00:08:05,690 --> 00:08:06,986
第一个所谓的auto query
119
00:08:07,980 --> 00:08:09,900
那我就query什么样的意思呢
120
00:08:09,910 --> 00:08:13,100
如果说你要想检测这个框
121
00:08:13,410 --> 00:08:17,700
那你可以把框定位成定义成一个query的形式
122
00:08:17,720 --> 00:08:21,256
实际相当于你想要搜寻的目标
123
00:08:21,260 --> 00:08:22,664
就相当于我们做搜索的时候
124
00:08:22,670 --> 00:08:27,248
在搜索框里面搭建一个query能够得到结果
125
00:08:27,260 --> 00:08:29,500
那在检测里面也可以实现这样的问题
126
00:08:30,090 --> 00:08:31,340
在检测里面另外一个点
127
00:08:31,460 --> 00:08:35,887
就是我们不知道在一个图片里面有多少物体
128
00:08:36,450 --> 00:08:38,950
这个时候就涉及到这个one one Simon去做
129
00:08:41,120 --> 00:08:44,558
我我我自己觉得这个比较有意思的这个data
130
00:08:44,560 --> 00:08:48,754
或者说能够被大家广为接受人员重要原因
131
00:08:48,770 --> 00:08:50,219
大家看右边的这个图
132
00:08:50,930 --> 00:08:52,378
大家可以想象一下
133
00:08:52,860 --> 00:08:57,708
假如这个图里面有这桶里面有两只牛
134
00:08:57,710 --> 00:08:58,330
对吧
135
00:08:58,800 --> 00:09:04,938
我们去如果就需要把这个牛所在的这个框给标出来
136
00:09:04,940 --> 00:09:05,990
我们会怎么做呢
137
00:09:06,560 --> 00:09:10,697
我们肯定是找最这个牛的最上部是点在哪个地方
138
00:09:10,710 --> 00:09:13,850
最左边、最右边、最下面在什么地方
139
00:09:14,040 --> 00:09:16,560
大家看看右边图的那个最下面一列
140
00:09:17,460 --> 00:09:20,700
大家可这个是在屏幕上还挺清楚的
141
00:09:21,080 --> 00:09:30,470
大家可以看到最下面的前四个图里面有那个红橙色亮出来那几个位置
142
00:09:30,480 --> 00:09:36,118
大家看正好他是在找这个物体的最左边、最上面、最右边、最下面
143
00:09:36,960 --> 00:09:40,528
这也是为什么传说比较吸引人的地方
144
00:09:41,030 --> 00:09:42,533
它的可解性性非常强
145
00:09:42,540 --> 00:09:45,510
它跟我们人类去标这个框是一样的
146
00:09:45,520 --> 00:09:47,698
真的是找四个点对吧
147
00:09:47,940 --> 00:09:49,851
那这样子找到4个点出来以后
148
00:09:50,210 --> 00:09:52,058
你就可以把这个框给弄出来
149
00:09:52,610 --> 00:09:54,573
那最下面一行最右边的那个图
150
00:09:55,190 --> 00:09:58,046
因为我们在检测里面不仅要定位这个框
151
00:09:58,520 --> 00:10:01,352
而且要知道这个物体是什么
152
00:10:01,360 --> 00:10:04,240
他真的是在找这个框里面找一个位置
153
00:10:04,250 --> 00:10:07,865
然后这个位置的特征去帮去做识别
154
00:10:09,000 --> 00:10:12,960
这个是data为什么为大家所喜欢的一个原因
155
00:10:18,070 --> 00:10:18,410
对
156
00:10:18,620 --> 00:10:20,220
然后预训练
157
00:10:20,220 --> 00:10:21,903
因为为什么要做预训练呢
158
00:10:21,910 --> 00:10:25,258
因为刚才提到的工业视觉的这些任务里面
159
00:10:25,260 --> 00:10:34,500
大家可以想象我们通常的标注数据都是非常少的那预训练跟今天大模型比较相关
160
00:10:34,500 --> 00:10:36,480
我后面会简单解释一下
161
00:10:36,890 --> 00:10:39,436
这里面就训练了我们去年做了这么一个工作
162
00:10:39,450 --> 00:10:42,810
就是把刚才那个检测的流程图里面
163
00:10:43,830 --> 00:10:48,709
它的编码器和这个解码器多做了预训练
164
00:10:48,900 --> 00:10:49,716
那做了预训练以后
165
00:10:49,840 --> 00:10:58,498
我们去年应该是11月份首次在这个计算机四原理里面的一个benchmark数据上
166
00:10:58,530 --> 00:11:02,210
客户上面首次达到了60.5这样的一个结果
167
00:11:07,230 --> 00:11:11,400
下面谈一谈其实刚才整个框架里面
168
00:11:11,540 --> 00:11:13,465
编码器与训练是怎么去做
169
00:11:13,720 --> 00:11:16,410
这是我今天给大家汇报的一个重点内容
170
00:11:19,590 --> 00:11:23,007
也就是现在大家谈大模型谈的非常的多
171
00:11:23,720 --> 00:11:29,539
然后其实大家不见得有一个或者还没有达成一个共识
172
00:11:29,540 --> 00:11:30,848
什么叫大模型
173
00:11:31,290 --> 00:11:38,358
这里面我试图去从视觉的角度去给出大模型是涉及到的几有重要的因素
174
00:11:39,340 --> 00:11:41,518
第一个算法依然非常重要
175
00:11:41,750 --> 00:11:46,419
你需要什么样的这个比如说刚才编码器解码器的结构
176
00:11:46,860 --> 00:11:49,484
你去怎么去做训练
177
00:11:49,650 --> 00:11:53,122
这是跟算法相关的那第二点就是算力
178
00:11:53,430 --> 00:11:55,530
今天来讲大家觉得这个非常的重要
179
00:11:56,000 --> 00:11:59,927
你有多少的卡你才能把这个模型训练好
180
00:12:00,940 --> 00:12:01,498
第三点
181
00:12:01,620 --> 00:12:06,686
其实可能大家谈的没有算你谈的那么多
182
00:12:06,690 --> 00:12:08,550
但实际上是非常重要的
183
00:12:08,550 --> 00:12:10,250
而且可能是最为重要的
184
00:12:10,630 --> 00:12:11,530
就大数据
185
00:12:12,040 --> 00:12:17,224
你要从大量的数据里面去挖掘有用的信息
186
00:12:17,230 --> 00:12:18,476
来帮助你去解决
187
00:12:18,480 --> 00:12:19,920
比如这里面的检测任务
188
00:12:20,450 --> 00:12:21,980
或者是这个分割任务
189
00:12:22,400 --> 00:12:27,748
谈到数据就是说大家可能最近听到那个语言模型里面谈的比较多
190
00:12:28,290 --> 00:12:29,780
有一个说法
191
00:12:30,140 --> 00:12:33,617
这个互联网上的公开的数据都已经被用完了
192
00:12:34,180 --> 00:12:34,720
换句话讲
193
00:12:34,720 --> 00:12:36,350
围绕训练这么一个模型
194
00:12:36,450 --> 00:12:37,220
大语言模型
195
00:12:37,220 --> 00:12:41,224
比如说这个GPD或者说我们这个百度的文心一言
196
00:12:41,540 --> 00:12:44,032
实际上是利用大量的数据去产量
197
00:12:44,040 --> 00:12:45,876
能得到很好的这么一个模型
198
00:12:46,420 --> 00:12:47,776
这个上面三点
199
00:12:48,060 --> 00:12:48,740
下面三点
200
00:12:48,740 --> 00:12:51,140
还有就是说你训练这个模型出来以后
201
00:12:51,900 --> 00:12:55,275
实际上你是希望他来解决很多任务
202
00:12:55,360 --> 00:12:57,582
而不仅仅是解决一个任务
203
00:12:58,380 --> 00:13:00,189
然后中间这个大参数
204
00:13:00,190 --> 00:13:03,016
其实大家可能对大模型有有这么一个理解
205
00:13:03,030 --> 00:13:06,342
以为大模型仅仅是说把参数
206
00:13:06,600 --> 00:13:07,648
把这个模型的规模
207
00:13:07,650 --> 00:13:08,560
模型的参数变大
208
00:13:09,040 --> 00:13:10,150
其实是不够的
209
00:13:10,320 --> 00:13:12,244
光有参数其实结果是不够好的
210
00:13:12,920 --> 00:13:13,718
除了这些以外
211
00:13:13,850 --> 00:13:16,118
还有一个我把这个大平台列出来
212
00:13:16,130 --> 00:13:17,549
为什么要把这个列出来呢
213
00:13:18,070 --> 00:13:22,894
原因是说当我们要去训练这么大量的数据
214
00:13:23,500 --> 00:13:25,775
然后这个模型规模很大的时候
215
00:13:26,360 --> 00:13:28,090
我们要很多的卡去训练
216
00:13:28,860 --> 00:13:30,948
几千块甚至上万块还是训练
217
00:13:31,690 --> 00:13:35,590
大家可能用过8卡或者用过六四卡
218
00:13:36,220 --> 00:13:41,386
如果大家有机会去训练用上千块卡去训练的时候
219
00:13:41,940 --> 00:13:45,569
就会遇到发现今天这个卡掉今天掉了一块卡
220
00:13:45,580 --> 00:13:48,352
明天又掉了块卡卡经常会出问题
221
00:13:48,710 --> 00:13:51,031
还有不同机器之间的通信
222
00:13:51,320 --> 00:13:54,108
这会涉及到你只使用什么样的一个框架
223
00:13:54,120 --> 00:13:56,800
什么样的一个平台就很好的支撑这样的训练
224
00:14:01,700 --> 00:14:04,467
刚才提到就是在工业场景里面
225
00:14:04,470 --> 00:14:06,520
实际上我们面临的问题
226
00:14:08,010 --> 00:14:09,165
方法是一个因素
227
00:14:09,170 --> 00:14:11,050
更重要的是数据的问题
228
00:14:11,050 --> 00:14:14,410
我们的数据往往是非常少的
229
00:14:14,670 --> 00:14:16,126
或者讲准确的讲
230
00:14:16,130 --> 00:14:18,340
有标注的数据能够得到
231
00:14:18,340 --> 00:14:21,281
比如说刚才那个框的数据还是非常少的
232
00:14:21,290 --> 00:14:22,250
原因很简单
233
00:14:22,830 --> 00:14:26,958
就是我们去标注这样的数据需要花费大量的人力和财力
234
00:14:27,260 --> 00:14:31,212
但是我们有大量的工业场景的数据不是没有
235
00:14:31,410 --> 00:14:33,170
没有标注的数据非常的多
236
00:14:33,170 --> 00:14:33,820
那这个时候
237
00:14:33,920 --> 00:14:38,288
我们实际上采用了这个与训练的方案
238
00:14:38,430 --> 00:14:40,006
今天跟大家分享的
239
00:14:40,010 --> 00:14:41,243
因为在工业场景里面
240
00:14:41,250 --> 00:14:44,040
我们通常用的就是自监督与训练这么一个方案
241
00:14:46,450 --> 00:14:50,177
这里面就是在视觉里面的自监督预训练
242
00:14:50,350 --> 00:14:52,856
其实最近几年其实研究的非常多
243
00:14:53,270 --> 00:14:56,078
实际上从上个世纪90年代
244
00:14:56,080 --> 00:14:58,100
hinton, 也就图灵奖获得者hinton
245
00:14:58,610 --> 00:15:04,130
在91年92年已经做了这样的早期的预训练的工作
246
00:15:05,450 --> 00:15:12,490
这里面我把它最近几年在做预训练大数据预训练这方面的工作分成三大类
247
00:15:12,490 --> 00:15:15,334
第一类其实称之为早期方法
248
00:15:15,340 --> 00:15:16,474
之所以称为早期方法
249
00:15:16,480 --> 00:15:21,079
就是大家没有一个统一的一个是统一的方案去做
250
00:15:21,090 --> 00:15:25,732
有的比如说把一幅黑白的图片变成一个彩色的图片
251
00:15:25,750 --> 00:15:27,406
它都可以去做预训练
252
00:15:27,410 --> 00:15:27,880
对吧
253
00:15:28,300 --> 00:15:31,884
那中间的对比与训练是用的比较多的
254
00:15:32,320 --> 00:15:35,840
也是两三年前非常流行的
255
00:15:36,150 --> 00:15:38,925
你这个算法最右边是掩码图像建模
256
00:15:38,930 --> 00:15:44,850
跟这个自然语言里面做这个bert就是google的bert很相像的一个方案
257
00:15:45,690 --> 00:15:50,754
这里面就是把最下面我就谈谈我个人的自监督和无监督
258
00:15:51,210 --> 00:15:54,616
这两个东西实际上是不一样的
259
00:15:54,620 --> 00:15:57,774
有的时候大家会把自监督和无监督等价起来
260
00:15:57,780 --> 00:15:59,094
无监督就是没有监督
261
00:15:59,400 --> 00:16:01,160
自监督实际上是有监督的
262
00:16:01,160 --> 00:16:04,257
只不过是说你要设计一个很好的优训练方案
263
00:16:04,580 --> 00:16:06,370
把这个监督信息给挖掘出来
264
00:16:08,420 --> 00:16:10,290
掩码图像建模这个概念非常简单
265
00:16:10,290 --> 00:16:11,424
大家看左边的图
266
00:16:11,880 --> 00:16:16,140
我随机的把五个patch给抹掉
267
00:16:16,550 --> 00:16:20,388
然后任务是把这五个patch给预测出来
268
00:16:21,070 --> 00:16:22,946
我们当我们解决这个任务的时候
269
00:16:23,470 --> 00:16:25,303
我们需要设计了一个网络结构
270
00:16:25,310 --> 00:16:27,460
我们希望通过解决任务
271
00:16:27,460 --> 00:16:30,234
然后学到的网络结构去帮助这个下游的任务
272
00:16:32,030 --> 00:16:34,440
我们的方法叫做contest auto encode
273
00:16:36,450 --> 00:16:41,891
这个核心思想就是因为我们这个编码器编码
274
00:16:41,900 --> 00:16:47,300
所谓编码就是那个图像编码成一个我们称作一个向量或者一个embedding
275
00:16:47,300 --> 00:16:47,760
对吧
276
00:16:48,580 --> 00:16:50,387
那实际上换句话讲这是个表征
277
00:16:50,990 --> 00:16:52,490
我们既然要学这个表征
278
00:16:52,590 --> 00:16:58,715
我们就希望解决这么一个刚才这个野马图像建模这个任务
279
00:16:58,840 --> 00:17:01,584
我们是在这个表征空间里面去做
280
00:17:02,210 --> 00:17:04,160
在表征空间里面做的这个思想
281
00:17:04,280 --> 00:17:05,664
其实用的非常的多
282
00:17:06,280 --> 00:17:08,184
然后比如说生成
283
00:17:08,350 --> 00:17:12,277
去年可能大家关注比较多的这个纹身图
284
00:17:12,470 --> 00:17:12,900
对吧
285
00:17:13,070 --> 00:17:18,613
基本都是在那个表征空间里面去做的这是第一个思想
286
00:17:19,490 --> 00:17:20,420
第二个思想
287
00:17:21,210 --> 00:17:26,110
我们是去解决刚才野马图像建模预测五个patch的这个任务
288
00:17:26,570 --> 00:17:29,508
这个本身不是我们想要的东西
289
00:17:29,510 --> 00:17:32,790
所以说我们在设计预训练这个网络结构的时候
290
00:17:32,920 --> 00:17:35,800
希望预训练的这个部分
291
00:17:36,580 --> 00:17:38,836
解决任务的部分跟学习编码基本分离
292
00:17:38,970 --> 00:17:39,838
基于这两个原则
293
00:17:39,840 --> 00:17:41,250
我们设计了这么一个方法
294
00:17:43,760 --> 00:17:46,870
这个我就不展开去讲这些整体的网络结构
295
00:17:48,850 --> 00:17:50,830
这里面在我们这个工作之前
296
00:17:50,840 --> 00:17:56,291
其实有有这么两个比较相特别相关就比较重要的工作
297
00:17:57,310 --> 00:17:59,900
一个是来自微软的这么一个工作
298
00:18:00,110 --> 00:18:05,929
这个工作他直接用一个结构直接去学解决运行的任务
299
00:18:06,090 --> 00:18:07,542
这个结构就跟我刚才说的
300
00:18:07,550 --> 00:18:12,390
这个结构实际上一方面去学这个表征怎么表达图像
301
00:18:12,400 --> 00:18:14,363
另外一方面也去做预测的任务
302
00:18:14,810 --> 00:18:16,493
换句话讲他做了两件事情
303
00:18:16,500 --> 00:18:20,070
但是这两件事情不是我们想要的其中的一件事情
304
00:18:20,350 --> 00:18:23,716
所以他的整个出来的结果就不是那么的好
305
00:18:25,030 --> 00:18:28,390
第二个是来自于facebook新的meta的这么一个结构
306
00:18:28,400 --> 00:18:30,649
它在B的基础上往前走了一步
307
00:18:31,740 --> 00:18:34,044
这里面他把这个in提出来
308
00:18:34,050 --> 00:18:36,712
所谓的encode decode这么两个步骤
309
00:18:37,380 --> 00:18:44,484
这个decode在这里面它实际上可能也在做编码图像的这么一个任务
310
00:18:44,500 --> 00:18:44,960
换句话讲
311
00:18:44,960 --> 00:18:46,193
它分离的不是很清楚
312
00:18:46,750 --> 00:18:50,800
它encode这里面不是完全去学习这个表征的
313
00:18:54,280 --> 00:18:56,726
这个是一个比较了三个方面的去比较
314
00:18:58,150 --> 00:18:59,320
这里面要提到的一点
315
00:18:59,400 --> 00:19:01,644
就是我们这个CIE的工作
316
00:19:01,770 --> 00:19:05,550
最近这个图灵奖获得的一样的空
317
00:19:05,550 --> 00:19:08,330
他也有一篇类似的工作
318
00:19:08,330 --> 00:19:11,540
也是在隐含空间里面去做的这么一件事情
319
00:19:15,350 --> 00:19:17,042
然后这里再比较一下
320
00:19:17,050 --> 00:19:20,410
就是这个mask的image model
321
00:19:20,420 --> 00:19:22,656
我同样建模跟这个对比预训练
322
00:19:23,070 --> 00:19:28,898
我刚才提到的在9191年92年hinton也做了这个类似的工作
323
00:19:29,570 --> 00:19:33,430
就是做这个对比预训练的那我们看到今天来讲
324
00:19:33,440 --> 00:19:40,104
为什么对比预训练跟这个图像建模的方法比较起来没有图像建模方法那么好啊
325
00:19:40,110 --> 00:19:41,856
这是我们的一个分析
326
00:19:44,030 --> 00:19:51,272
首先这个对比优训练它是怎么去工作的那这一点其实大家没有搞得那么的清楚
327
00:19:51,600 --> 00:19:54,400
即便是在hinton这个91年的文章里面
328
00:19:54,400 --> 00:19:55,880
他讲的也没有那么清楚
329
00:19:55,880 --> 00:20:01,644
他说有一个潜在的东西使得某两个patch相似
330
00:20:02,110 --> 00:20:02,992
或者某两个view
331
00:20:03,000 --> 00:20:04,521
那两个view是什么意思呢
332
00:20:04,530 --> 00:20:07,218
在这个地方就是输入的最左边的图片
333
00:20:07,350 --> 00:20:09,800
我在里面抠出两个patch来
334
00:20:09,810 --> 00:20:11,028
这个两个patch是view
335
00:20:11,540 --> 00:20:16,562
我我通过这个网络结构使得这两个view得到了embedding
336
00:20:16,570 --> 00:20:17,390
尽量相像
337
00:20:17,710 --> 00:20:19,070
但是大家可以想象
338
00:20:19,330 --> 00:20:21,686
大家看看这2个VU2个端的v view
339
00:20:21,700 --> 00:20:23,527
一个是狗的前半部分
340
00:20:23,530 --> 00:20:24,546
一个狗的后半部分
341
00:20:24,550 --> 00:20:27,878
其实这两个没有大家想象是不一样的
342
00:20:28,320 --> 00:20:30,570
如果我们有一个网络结构让他学出来一样
343
00:20:30,580 --> 00:20:31,804
这是有问题的对吧
344
00:20:32,200 --> 00:20:35,260
所以说我们其实后来想了一件事情
345
00:20:35,270 --> 00:20:35,990
是什么事情呢
346
00:20:35,990 --> 00:20:40,820
就是说我们收银通过这个encode去把这个表征很好的表达出来
347
00:20:40,820 --> 00:20:42,150
这两个表征是不一样的
348
00:20:42,150 --> 00:20:43,150
在这两个vu里面
349
00:20:43,800 --> 00:20:47,070
后来通过project做什么呢
350
00:20:47,430 --> 00:20:49,770
比如说我们看看上面狗的前半部分
351
00:20:49,990 --> 00:20:53,239
首先从那部分我们把这个狗的整体预测出来
352
00:20:53,580 --> 00:20:57,150
猜出来下面也是把狗的整体拆出来
353
00:20:57,650 --> 00:21:01,730
那这两个没有虽然是对应狗的不同部分
354
00:21:01,740 --> 00:21:03,357
但它都是来自于同一条狗
355
00:21:03,850 --> 00:21:07,116
所以通过这个project把狗的整体拆出来以后
356
00:21:07,660 --> 00:21:08,906
它就可以相像了
357
00:21:09,170 --> 00:21:10,310
这个就是我们给的一个解释
358
00:21:10,320 --> 00:21:15,830
后来其实这个是大概也是同期这个养老控跟他的同事一起做的工作
359
00:21:15,850 --> 00:21:17,687
也有类似的这么一个发现
360
00:21:18,120 --> 00:21:20,472
也真的会发现这个编码就出来的
361
00:21:20,480 --> 00:21:24,920
是能够很好的表征这个狗的这个前半部分、后部分
362
00:21:24,920 --> 00:21:26,698
换句话就是这个part的概念
363
00:21:27,090 --> 00:21:28,807
然后这个所谓的project的后面
364
00:21:29,330 --> 00:21:32,410
他真的是对part没有那么敏感的
365
00:21:35,560 --> 00:21:37,999
这部分就是我可能不展开去讲
366
00:21:38,000 --> 00:21:40,224
就对比与训练实际上最终非常有意思
367
00:21:40,720 --> 00:21:45,070
因为我们在这个领域里面用的比较多的是移民积累的这么一个数据集
368
00:21:45,410 --> 00:21:47,295
这本书已经有有这么一个特点
369
00:21:47,300 --> 00:21:51,640
它的物体基本是在中间的那这个是他设计图像
370
00:21:52,560 --> 00:21:54,523
斯坦福的李菲菲教授设计图像
371
00:21:54,530 --> 00:21:58,402
但是可能刻意要为之的这么一个结果
372
00:21:58,600 --> 00:21:59,953
但是正好在对比有限里面
373
00:21:59,960 --> 00:22:02,040
他就会把中间的这个物体学的非常好
374
00:22:04,140 --> 00:22:06,337
这个也是来自于我们下面的图
375
00:22:06,340 --> 00:22:08,100
可以看出来来自于什么呢
376
00:22:08,110 --> 00:22:11,118
当我们去在一个图片里面随机去抠出
377
00:22:11,120 --> 00:22:12,463
我们称之为random crops
378
00:22:13,240 --> 00:22:15,760
如果这个crop的大小不是特别小的话
379
00:22:15,760 --> 00:22:22,624
就会发现这个图片的中间的这个像素点很容易被落到这个random cross里面
380
00:22:22,790 --> 00:22:23,980
这个其实是很简单
381
00:22:27,160 --> 00:22:27,870
这样子一来
382
00:22:27,990 --> 00:22:29,394
就是一个简单的比较
383
00:22:29,560 --> 00:22:35,499
比如大家看看前三行里面的第二列、第二行这个是对于训练的方法
384
00:22:35,500 --> 00:22:38,254
它真的是基本学到的是物体中间的信息
385
00:22:38,670 --> 00:22:40,506
第三行是我们的方法
386
00:22:40,890 --> 00:22:43,370
包括其他的这个MM的方法也是类似
387
00:22:43,630 --> 00:22:45,262
他可以学到整个图片的信息
388
00:22:45,270 --> 00:22:45,738
换句话讲
389
00:22:45,740 --> 00:22:48,210
整个图片里面信息不仅仅物体的信息有作用
390
00:22:48,210 --> 00:22:50,498
其实其他的信息也是很有作用
391
00:22:51,580 --> 00:22:54,040
下面三行也是类似的这么一个观察
392
00:22:55,700 --> 00:22:57,119
后来我们就做了一些评测
393
00:22:58,370 --> 00:23:02,507
这个评测我可以跳过去讲这个不具体讲这个结果
394
00:23:02,520 --> 00:23:04,679
但是大家都看到就是我们的方案会更好
395
00:23:05,130 --> 00:23:09,915
这里面一个非常有意思的就是大家在做这方面的探索的时候都会说
396
00:23:09,990 --> 00:23:13,429
以前都是用这个nata process
397
00:23:13,430 --> 00:23:15,852
就是倒数第二列的一个去做评测
398
00:23:15,860 --> 00:23:17,630
就会发现MRA的方法
399
00:23:18,070 --> 00:23:20,604
就是图像里掩码图像建模的方法
400
00:23:20,610 --> 00:23:22,650
会比这个对比训练方法的结果要差
401
00:23:23,310 --> 00:23:26,239
所以说有人提出来这个时候应该用所谓的fine tuning
402
00:23:26,250 --> 00:23:29,799
就是今天大家谈的比较多的这个微调的方案去做
403
00:23:30,960 --> 00:23:32,220
其实这里面是有问题的
404
00:23:32,880 --> 00:23:37,630
所以我们提出来应该用最右边的attending plopping去做这个对比
405
00:23:37,650 --> 00:23:38,010
原因很简单
406
00:23:40,080 --> 00:23:43,352
我们在这边可以看到
407
00:23:44,640 --> 00:23:46,110
比如还是第二行第三行
408
00:23:47,030 --> 00:23:48,500
第二行是这个对比训练
409
00:23:48,500 --> 00:23:49,967
第三行是MM的方法
410
00:23:50,690 --> 00:23:52,586
我们在传统做模识别的时候
411
00:23:52,590 --> 00:23:55,671
实际上会包含两个基本的步骤
412
00:23:55,680 --> 00:23:58,205
第一个是就是fish extraction特征提取
413
00:23:58,850 --> 00:24:01,694
提取完了以后还要做selection
414
00:24:01,880 --> 00:24:06,848
所谓selection就是选择对你任务有帮助的那个信息
415
00:24:07,020 --> 00:24:11,508
大家可以看到第二行和第三行对比预训练已经选出来了
416
00:24:11,520 --> 00:24:14,112
这在这个页面阶段上做分类
417
00:24:14,120 --> 00:24:15,737
重要的信息就是物体信息
418
00:24:16,470 --> 00:24:17,766
这个掩码图像的方法
419
00:24:18,020 --> 00:24:20,594
它实际上学的整个图片的信息
420
00:24:21,590 --> 00:24:22,598
除了物体以外的信息
421
00:24:22,600 --> 00:24:24,658
实际上是对这个分类没有帮助的
422
00:24:25,600 --> 00:24:27,784
所以说我们提出来就是用attending problem
423
00:24:28,040 --> 00:24:30,440
就是有一个额外的一个选择的一步骤
424
00:24:30,450 --> 00:24:31,953
大家看到最右边一列
425
00:24:32,080 --> 00:24:36,611
其实结果都很结果就发现对比一群人的结果变化很小
426
00:24:36,740 --> 00:24:39,730
但是MM的方法结果变化很大
427
00:24:39,730 --> 00:24:41,180
这也说明了为什么要做
428
00:24:41,440 --> 00:24:45,010
为什么刚才跟前面那个解释就是类似的
429
00:24:45,010 --> 00:24:46,594
一个是highlight的中间的物体
430
00:24:46,600 --> 00:24:48,420
一个是学到整个图片所有的信息
431
00:24:49,880 --> 00:24:55,380
这里面其实在下一个任务就是公开的数据里面
432
00:24:56,080 --> 00:24:57,900
一个是分割的一个检测的结果
433
00:24:57,910 --> 00:25:01,490
它都会比这个对比变量都比其他的范围好一些
434
00:25:04,040 --> 00:25:08,513
但是质检度这个实际上是一个很大的一个点问题
435
00:25:09,360 --> 00:25:10,584
所以他的训练特别慢
436
00:25:10,890 --> 00:25:13,702
其实我们没有给他人为的给他一些监督信息
437
00:25:13,710 --> 00:25:14,898
让他自己去学
438
00:25:14,950 --> 00:25:16,644
那当然很困难的一件事情
439
00:25:17,130 --> 00:25:21,288
所以说我们后来想办法去加速这么一个训练的过程
440
00:25:21,810 --> 00:25:27,318
提出了利用现有的别的模型来帮助训练
441
00:25:27,710 --> 00:25:33,683
我们用clip去帮助这么一个我们的预训实验一的范围去做快速的预训练
442
00:25:33,690 --> 00:25:34,986
实际上这样的大家可以看到
443
00:25:36,550 --> 00:25:43,976
我们比如说去看第一组的这个CIE跟CIEV two 1600个apple和300个apple 
444
00:25:43,976 --> 00:25:45,556
1600个和300个
445
00:25:45,600 --> 00:25:49,274
意味着你的计算量下降了五分下降到5分之1左右
446
00:25:49,650 --> 00:25:50,982
但他的结果反馈更好
447
00:25:51,360 --> 00:25:56,832
这也是提升这个训练速度的一个重要的方案
448
00:25:58,280 --> 00:26:01,217
回到我们需要解决的问题
449
00:26:01,220 --> 00:26:01,988
工业视觉
450
00:26:02,840 --> 00:26:05,570
就是我们把刚才这个自监督的方法
451
00:26:06,230 --> 00:26:07,742
尤其是威望的的线呢
452
00:26:08,190 --> 00:26:09,884
用到了这个工业视觉里面
453
00:26:10,880 --> 00:26:14,290
这里面其实我们这个工作是去年我们做的这件事情
454
00:26:15,200 --> 00:26:20,128
当时我们收集了大概一个亿100个million的工业数据
455
00:26:20,460 --> 00:26:23,741
包含了我去开头介绍的各种各样的场景
456
00:26:24,080 --> 00:26:27,020
把所有这些数据融合在一起去训练
457
00:26:27,020 --> 00:26:31,110
用这个自监督的方法训练的一个隐蔽的出来
458
00:26:35,810 --> 00:26:41,655
当我们把这样的encode或者编码器放到这个检测模型的时候
459
00:26:42,190 --> 00:26:44,878
在工业数据里面去做的时候
460
00:26:45,130 --> 00:26:48,090
我们就会发现这个跟前面讲的coco不一样
461
00:26:48,090 --> 00:26:49,470
因为coco是公开的数据
462
00:26:49,480 --> 00:26:59,133
它面临的场景都是大家通常见到的那在工业场景里面我们就会发现它的数据的分布其实跟公开的分布不太一样的
463
00:26:59,350 --> 00:27:02,731
这个时候我们仅仅有编码器的预训练还是不够的
464
00:27:02,740 --> 00:27:04,140
我们希望去训练解码器
465
00:27:04,510 --> 00:27:07,348
这里比较有意思的就是我们训练这个解码器的时候
466
00:27:08,930 --> 00:27:10,530
也在公开数据里面去做
467
00:27:10,530 --> 00:27:11,250
但这个时候
468
00:27:11,360 --> 00:27:13,820
我们把这个编码器给固定好
469
00:27:14,230 --> 00:27:14,898
固定起来
470
00:27:15,650 --> 00:27:16,010
换句话
471
00:27:16,010 --> 00:27:18,490
编码器是在工业场景里面训练出来的
472
00:27:18,840 --> 00:27:23,280
解码器是在这个公开的数据里面去预训练出来
473
00:27:23,280 --> 00:27:26,560
然后训练完了后再放到这个真实的场景里面去
474
00:27:26,570 --> 00:27:32,776
这个翻新单独的原因就是说我们对这个解码器它在做什么有个认识
475
00:27:33,380 --> 00:27:36,194
他在找定位这个物体的这个位置
476
00:27:36,200 --> 00:27:38,648
其实就是定位物体的一个框
477
00:27:38,800 --> 00:27:45,610
这个框他的能力可能从公开的数据里面迁移到这个是工业场景里面去
478
00:27:45,790 --> 00:27:49,955
但是识别编码器主要是解决识别的任务
479
00:27:50,100 --> 00:27:52,460
这个识别的任务是跟工业场景相关的
480
00:27:55,660 --> 00:28:00,730
去年我们把这样的一个方法用在很多行业里面
481
00:28:01,400 --> 00:28:03,110
并且做了这么几个发表
482
00:28:03,750 --> 00:28:05,818
发布了几个行业的大模型
483
00:28:06,530 --> 00:28:12,642
第一个就是我们跟生态燃气一起合作的这个知识层增强的燃气行业大模型
484
00:28:13,220 --> 00:28:16,246
也是用在燃气行业的这个巡检任务里面
485
00:28:16,630 --> 00:28:17,664
你用了这样一个方法以后
486
00:28:17,670 --> 00:28:21,300
大家看到右边的这个柱状图会提升的非常明显
487
00:28:24,400 --> 00:28:29,040
同时也在人员跟国网合作的能源行业的大模型
488
00:28:29,700 --> 00:28:33,076
这个跟这个无人机的本体巡检相关的
489
00:28:33,240 --> 00:28:34,637
大家也可以看到这些结果
490
00:28:35,170 --> 00:28:40,070
右边是跟TCL一起合作的这个电子制造行业
491
00:28:40,190 --> 00:28:42,836
实际上是工业质检的这么一个行业大模型
492
00:28:44,440 --> 00:28:46,050
这里面要提到的就是说
493
00:28:46,180 --> 00:28:49,018
当我们用了这样一个行业大模预训练的大模型以后
494
00:28:49,130 --> 00:28:53,091
其实他的结果提升是他很重要的一方面
495
00:28:53,100 --> 00:28:56,725
另外一个方面我就发现利用这个大模型预训练好的大模型
496
00:28:56,830 --> 00:28:58,491
你的研发效率会提升很多
497
00:28:58,810 --> 00:29:01,660
因为在这个场这里面我们的标注数据非常少
498
00:29:01,890 --> 00:29:04,399
以往花了大量的时间去做这个
499
00:29:04,810 --> 00:29:07,870
我们今天来讲这个人炼丹式一样去调出来
500
00:29:07,980 --> 00:29:08,925
用了大模型以后
501
00:29:09,050 --> 00:29:12,670
我发现这个真正在工业场景里面做训练的时候
502
00:29:12,780 --> 00:29:13,809
速度是非常快的
503
00:29:13,810 --> 00:29:16,890
比如说这个周期可以缩短30%
504
00:29:17,410 --> 00:29:19,340
那能启动效率就是三倍
505
00:29:21,710 --> 00:29:25,321
这里面是两个具体的一些例子
506
00:29:25,830 --> 00:29:35,660
一个检测就是我们比较了参数量变大跟其他的之前的方案去对比
507
00:29:41,310 --> 00:29:43,898
这是工业质检是另外一个典型的应用
508
00:29:43,910 --> 00:29:46,166
比如说你参数量小
509
00:29:46,170 --> 00:29:52,335
v it small跟v it large之间的一些关系还可以看到娜姐当然会比这个就会更好
510
00:29:52,490 --> 00:29:55,634
大家看看这个颜色不太一样
511
00:29:55,640 --> 00:29:55,930
对
512
00:29:56,260 --> 00:30:02,356
应该这边偏绿色的这个颜色就会较它叫蒸馏的一个结果
513
00:30:02,370 --> 00:30:04,268
实际上用大模型去蒸馏小模型
514
00:30:04,630 --> 00:30:04,998
换句话讲
515
00:30:05,000 --> 00:30:10,208
你大模型其实你真正直接用在这个具体应用场景里还是非常困难
516
00:30:10,590 --> 00:30:11,340
计算量很大
517
00:30:11,770 --> 00:30:14,448
通常我们需要去做一些小型化
518
00:30:14,450 --> 00:30:16,686
这里面比如说蒸馏等等的方案
519
00:30:16,690 --> 00:30:21,450
去帮助提升这个线上模型小模型的效果
520
00:30:23,070 --> 00:30:29,952
这样我给大家汇报就是说文心大模型可能大家从各种渠道了解的比较多
521
00:30:30,550 --> 00:30:33,137
文献大模型整个是一张全景图
522
00:30:33,310 --> 00:30:37,795
我可能最下面由比如自然语言处理
523
00:30:37,800 --> 00:30:39,428
大家可能了解的比较多的
524
00:30:39,430 --> 00:30:43,180
最近关注比较多文心一言only bot然后还有视觉
525
00:30:43,660 --> 00:30:45,690
视觉我今天讲的是自监督这一块
526
00:30:45,690 --> 00:30:46,258
除此以外
527
00:30:46,350 --> 00:30:48,238
我们还有这个文档
528
00:30:49,120 --> 00:30:56,705
还有多任务的交通场景里面等等这样的还有一些low level的视觉处理里面
529
00:30:57,550 --> 00:30:58,774
第三个就是跨模态
530
00:30:58,870 --> 00:30:59,923
大家关注也比较多的
531
00:31:00,030 --> 00:31:02,000
尤其是去年这个纹身图
532
00:31:02,760 --> 00:31:05,946
最近包括facebook和google等等做的纹身视频
533
00:31:05,990 --> 00:31:07,112
当然我们自己有一个工作
534
00:31:07,650 --> 00:31:11,640
然后还有你生物计算相关的那中间层行业大模型
535
00:31:11,660 --> 00:31:14,737
去年我们一共发布了11个行业大模型
536
00:31:14,750 --> 00:31:15,806
包括NOP的
537
00:31:15,870 --> 00:31:16,980
包括视觉的
538
00:31:17,380 --> 00:31:19,300
然后上面有工具和平台
539
00:31:19,300 --> 00:31:20,851
就是有一些大模型的套件
540
00:31:21,430 --> 00:31:24,104
EZDLBML等等这样的东西
541
00:31:24,110 --> 00:31:25,410
大家还有大模型API
542
00:31:25,410 --> 00:31:29,290
可以通过API调用我们大模型的一些算法
543
00:31:31,470 --> 00:31:36,720
然后这页就是我们计算机视觉这一块CV大模型
544
00:31:36,740 --> 00:31:38,654
除了刚才讲的之前都以外
545
00:31:38,660 --> 00:31:40,480
实际上我们做的事情是三大类
546
00:31:40,850 --> 00:31:42,176
第六学的表征
547
00:31:42,360 --> 00:31:45,111
就是今天讲的跟大家汇报的实际上是表征这一块
548
00:31:45,560 --> 00:31:46,706
然后还有生成
549
00:31:47,280 --> 00:31:49,980
生成也是我们一直在做的一些工作
550
00:31:51,470 --> 00:31:55,970
最右边我们认为就是在这些大模型的这个时代
551
00:31:56,110 --> 00:31:57,727
其实这个算法依然很重要
552
00:31:57,730 --> 00:31:59,197
尤其是在落地的时候
553
00:31:59,640 --> 00:32:02,126
你的算法应该要足够有效
554
00:32:03,420 --> 00:32:05,409
你其实训练好大模型
555
00:32:05,410 --> 00:32:07,148
你怎么帮助有效的小模型
556
00:32:07,150 --> 00:32:09,430
我们称之为大模型赋能小模型
557
00:32:14,650 --> 00:32:16,758
最后我就总结一下
558
00:32:16,760 --> 00:32:17,896
今天跟大家汇报的
559
00:32:17,900 --> 00:32:24,734
实际上就是为什么大模型今天来讲对工业视觉或者整体计算机视觉非常重要呢
560
00:32:25,090 --> 00:32:27,183
实际上关键点还是数据的问题
561
00:32:27,820 --> 00:32:31,870
其实我们其实标注的数据往往是比较少的
562
00:32:32,610 --> 00:32:34,962
原因只有我们要花费大量的财力人力
563
00:32:35,310 --> 00:32:38,430
但是没有标注数据可以很容易获得
564
00:32:38,580 --> 00:32:45,230
优训练大模型实际上就是成为计算机视觉、工业视觉等等相关应用落地的重要的一个技术
565
00:32:45,370 --> 00:32:47,437
今天我给大家汇报的几个内容
566
00:32:47,440 --> 00:32:49,918
其实我们的这个代码用pado
567
00:32:50,420 --> 00:32:52,545
后来有人改成这个拍touch的形式
568
00:32:52,550 --> 00:32:55,050
在已经在网上get up up上就发布了
569
00:32:58,000 --> 00:32:58,300
好
570
00:32:58,300 --> 00:32:58,970
谢谢大家
