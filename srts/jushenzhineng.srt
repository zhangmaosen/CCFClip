0
00:00:00,110 --> 00:00:00,670
介绍啊，
1
00:00:00,670 --> 00:00:02,250
那我下面就直接进入主题，
2
00:00:02,250 --> 00:00:08,270
就讲我今天的内容就是我们居身智能的这样的一个 PI 模型感知想象和执行。
3
00:00:08,770 --> 00:00:08,930
呃，
4
00:00:08,930 --> 00:00:14,130
因为那我前面的一些就呃当然说这个愿景当然来自于今天来这里的肯定关起楚，
5
00:00:14,130 --> 00:00:15,350
就是我们的愿景的时候，
6
00:00:15,350 --> 00:00:16,970
实现通用机器人啊，
7
00:00:16,970 --> 00:00:18,510
我比较喜欢这部电影呢，
8
00:00:18,510 --> 00:00:24,250
是因为这部电影觉得是我认为在可见的未来是我们所有的东西都能够实现的啊，
9
00:00:24,310 --> 00:00:26,130
但西部世界可能比较遥远啊，
10
00:00:26,130 --> 00:00:28,330
这部电影我觉得是可能实现的。
11
00:00:28,330 --> 00:00:30,330
大家有兴趣可以看一谈啊。
12
00:00:30,470 --> 00:00:31,970
当然据身智能的话呃，
13
00:00:31,990 --> 00:00:32,930
我就不多讲了，
14
00:00:32,930 --> 00:00:37,830
这个啊在很早的图灵时代已经就提出了这样的一个居身智能的概念。
15
00:00:38,30 --> 00:00:42,210
为什么它学术上是一个嗯从智能是一个更加是一个智能，
16
00:00:42,210 --> 00:00:44,730
是一个啊非常高的一个智能的水平，
17
00:00:44,830 --> 00:00:47,670
是因为它是因有身体跟身体联合去学习。
18
00:00:47,670 --> 00:00:49,150
就像我们这个，
19
00:00:49,150 --> 00:00:51,90
当然这个实验这个个人讲了很多呢，
20
00:00:51,90 --> 00:00:52,110
我觉得快速讲一下，
21
00:00:52,110 --> 00:00:54,270
就是呃那认知学的实验室，
22
00:00:54,270 --> 00:00:54,910
它有两只的，
23
00:00:54,970 --> 00:00:57,70
然后这只猫是主动来走另一只猫，
24
00:00:57,70 --> 00:00:58,250
它出身就把它关起来，
25
00:00:58,250 --> 00:00:59,815
但是他看的东西是一样的。
26
00:01:00,60 --> 00:01:03,40
那最后这只猫把它被动的猫它下来之后，
27
00:01:03,40 --> 00:01:05,180
它就失去了行为能力啊，
28
00:01:05,180 --> 00:01:06,140
那这是为什么呢？
29
00:01:06,140 --> 00:01:10,420
这是因为啊这个这个这个它没有居于身体的智能，
30
00:01:10,600 --> 00:01:13,115
所以说他就失去了这种行为的能力。
31
00:01:13,990 --> 00:01:14,170
好，
32
00:01:14,170 --> 00:01:16,130
这也是证明了居人智能的重要性。
33
00:01:16,330 --> 00:01:19,10
当然到今天我就不用讲那么多不言而喻啊。
34
00:01:19,10 --> 00:01:22,790
因为我记得我我之前的讲座还得讲一堆什么是居生智能，
35
00:01:23,130 --> 00:01:24,810
可能就就快速跳过了。
36
00:01:24,990 --> 00:01:25,230
好，
37
00:01:25,430 --> 00:01:31,170
那么我们的我们是当然说现在居人智能肯定是大家要做很多种技术路线嘛，
38
00:01:31,170 --> 00:01:31,530
对吧？
39
00:01:31,670 --> 00:01:31,910
啊，
40
00:01:31,950 --> 00:01:37,290
那么我们也一直在探索我们一六的开始探索一种啊什么样的一个技术智能情框架，
41
00:01:37,290 --> 00:01:39,490
就是它学术的底座应该什么样子。
42
00:01:39,710 --> 00:01:41,770
那我们就类比人的模块，
43
00:01:41,790 --> 00:01:44,350
我们把三这个部分就是大居身感知，
44
00:01:44,350 --> 00:01:46,110
居然想象居人执行啊，
45
00:01:46,110 --> 00:01:47,170
这是什么逻辑呢？
46
00:01:47,170 --> 00:01:48,610
就是其实我们做件事情的话，
47
00:01:48,610 --> 00:01:52,910
我们首先要知道它这个感知到这个世界的模型。
48
00:01:52,910 --> 00:01:55,530
然后的话其实脑子里是在想象的，
49
00:01:55,530 --> 00:01:56,450
就是我们做一件事情，
50
00:01:56,450 --> 00:01:57,310
脑子是在动脑的。
51
00:01:57,310 --> 00:01:58,370
比如说我要把它拧开，
52
00:01:58,510 --> 00:02:00,290
我脑子是是过了一遍了，
53
00:02:00,390 --> 00:02:03,410
只是那个我们自己下意识没感觉而已。
54
00:02:03,510 --> 00:02:04,670
这属于是大脑功能，
55
00:02:04,670 --> 00:02:06,330
我们把这个世界抽象出来。
56
00:02:06,510 --> 00:02:09,970
然后第二部分就是执行执行是完全小脑的功能。
57
00:02:10,170 --> 00:02:10,990
大家觉得哎，
58
00:02:11,70 --> 00:02:12,410
那执行是不是比较简单，
59
00:02:12,410 --> 00:02:13,90
我想完就能做，
60
00:02:13,90 --> 00:02:14,820
其实并不是那么简单的。
61
00:02:14,900 --> 00:02:18,140
因为它涉及到我们其实身体和下意识反应啊，
62
00:02:18,140 --> 00:02:19,880
这个我会后面会专门展开讲，
63
00:02:20,100 --> 00:02:24,840
所以说就是这两部分对于世界这个功能是对于大脑对世界理解和抽象，
64
00:02:24,840 --> 00:02:26,940
最后我们要具象的去执行。
65
00:02:26,940 --> 00:02:28,620
那我们就展开讲。
66
00:02:28,740 --> 00:02:31,280
那第一个部分就是那个具身的感知了。
67
00:02:31,400 --> 00:02:34,100
当然说大家可能看过很多大家都做机缘式，
68
00:02:34,100 --> 00:02:36,180
如果做机缘视觉知道的感知，
69
00:02:36,180 --> 00:02:38,820
但是我们需要的感知有什么不一样呢？
70
00:02:39,20 --> 00:02:39,180
呃，
71
00:02:39,180 --> 00:02:40,680
其实这里面有一个科学问题，
72
00:02:40,680 --> 00:02:44,800
就是说如何去通用的去估计物理的常识啊，
73
00:02:44,920 --> 00:02:46,740
那这件事情我们觉得都很容易是吧？
74
00:02:46,740 --> 00:02:46,980
哎，
75
00:02:47,260 --> 00:02:48,540
但其实不这么容易，
76
00:02:48,540 --> 00:02:51,780
就是我们把这个箱把这个把这个盒子给它检测出来，
77
00:02:51,780 --> 00:02:52,680
是比较容易的。
78
00:02:52,760 --> 00:02:54,900
但是你要推断出这里是可以操作，
79
00:02:54,900 --> 00:02:56,320
这个轴可以翻开，
80
00:02:56,340 --> 00:02:57,520
这是有有难度的。
81
00:02:57,700 --> 00:02:58,900
那么这件事情呢，
82
00:02:59,40 --> 00:03:01,320
而我们就通用的做到我们专门去训练，
83
00:03:01,320 --> 00:03:03,380
可能就意思不不是很有意思了。
84
00:03:03,540 --> 00:03:04,720
那么这个事情呢，
85
00:03:04,720 --> 00:03:09,740
其实在呃二二年时候一篇 nature behavior 的文章也就讲了深度学习这件事情，
86
00:03:09,740 --> 00:03:11,500
其实现在是做不到的比较乏力。
87
00:03:11,800 --> 00:03:15,190
那么去仔细想它的这个问题是怎样的呢？
88
00:03:15,470 --> 00:03:15,610
啊，
89
00:03:15,610 --> 00:03:19,270
然后呢我们觉得那在大模型今天非常强大的。
90
00:03:19,270 --> 00:03:22,390
当天今天其实很多时候是卡在数据上面的。
91
00:03:22,490 --> 00:03:28,510
那么这里面就是如何大规模的获得这种带有带有这种操作常识的数据是件很难的事情。
92
00:03:28,630 --> 00:03:33,110
就比如上面我们知道这是可以翻开是可以拉的这是一件很难的事情啊，
93
00:03:33,210 --> 00:03:36,110
那么我们就去创一个什么思路呢？
94
00:03:36,250 --> 00:03:39,190
这个思路就是说我我们能推了这样的一个问题。
95
00:03:39,190 --> 00:03:44,510
就是说哎我们发现原来这个手的操作和这个这个物体的操作常识，
96
00:03:44,510 --> 00:03:46,110
它存在一种对偶关系啊，
97
00:03:46,110 --> 00:03:47,190
我们验证了说哎，
98
00:03:47,190 --> 00:03:48,610
我们们通过手操操作，
99
00:03:48,610 --> 00:03:53,410
我们就能推这样样一一个一个物体怎么怎么样的一个一种操作作识，
100
00:03:53,510 --> 00:03:58,30
而操作知识我们就能够够去生生成个手手操操作这种偶偶关系。
101
00:03:58,30 --> 00:04:00,130
那么发现种对偶关系呢有什么好处？
102
00:04:00,170 --> 00:04:02,830
首先这是可以是是最自然的数据，
103
00:04:02,830 --> 00:04:04,950
而且是准确的可以被规模化的啊。
104
00:04:05,190 --> 00:04:09,170
那这就是说呃比起你在那个三 d 里面去标，
105
00:04:09,230 --> 00:04:11,410
它是更加准确的对规模化的。
106
00:04:11,590 --> 00:04:12,630
那么为此的话呢，
107
00:04:12,630 --> 00:04:16,760
我们就去构建了那个一系列的算法去去推断这些事情啊，
108
00:04:16,760 --> 00:04:17,540
因为时间有限，
109
00:04:17,540 --> 00:04:18,760
我就不讲了啊，
110
00:04:18,840 --> 00:04:24,220
这个总之这样我们就将怎么去做手部重建以及手的在里面提取物体的知识啊，
111
00:04:24,220 --> 00:04:27,80
这是一些提取出来的一些操作知识的结果。
112
00:04:27,500 --> 00:04:27,740
啊，
113
00:04:27,980 --> 00:04:33,300
那么那么我们这里因此还构建了一个一系列的一个系统。
114
00:04:33,480 --> 00:04:33,720
啊，
115
00:04:33,800 --> 00:04:37,960
这个系统的话是说怎么样快速的去通过手的操作啊，
116
00:04:37,960 --> 00:04:39,60
上面是视频，
117
00:04:39,140 --> 00:04:40,500
下面是对视频的解析，
118
00:04:40,760 --> 00:04:42,540
下面是对手的一个分析。
119
00:04:42,840 --> 00:04:43,600
手的分析。
120
00:04:43,600 --> 00:04:46,120
我们能够去知道哎它在做一个什么概念，
121
00:04:46,320 --> 00:04:48,400
这个物体应该是被怎么操作的。
122
00:04:48,500 --> 00:04:51,400
那用这种方法我们快速能制备海量的数据。
123
00:04:51,660 --> 00:04:52,940
那有这样数据的话呢，
124
00:04:52,940 --> 00:04:56,560
我们发现这个模型的准确率是会被大大的提升啊，
125
00:04:56,560 --> 00:05:00,680
也为这样的通用的将来做一个通用的这样的一个世界模型。
126
00:05:00,680 --> 00:05:01,800
有个困角的 order model。
127
00:05:01,800 --> 00:05:04,680
就是说我们看到什么事情们知道怎么去操作它啊，
128
00:05:04,680 --> 00:05:06,900
这样的事情带来一个非常好的基础。
129
00:05:07,100 --> 00:05:08,940
当然说基将的大数据之后呢，
130
00:05:08,940 --> 00:05:12,880
我们就是会去啊去训练这样的一个物体的知识的模型。
131
00:05:12,880 --> 00:05:14,380
就比如说像这些啊，
132
00:05:14,380 --> 00:05:17,80
这本呢也得到了一个很大的提高啊。
133
00:05:17,400 --> 00:05:17,640
好，
134
00:05:17,940 --> 00:05:18,740
因为时间有限，
135
00:05:18,740 --> 00:05:19,620
我跳到下一部分。
136
00:05:19,620 --> 00:05:20,580
其实大家还可能想到，
137
00:05:20,580 --> 00:05:21,740
那哎柔性物体怎么办？
138
00:05:21,740 --> 00:05:22,80
对吧？
139
00:05:22,240 --> 00:05:24,620
我体比是简单柔性物体的话，
140
00:05:24,620 --> 00:05:26,20
其实我们也可以用这种方法。
141
00:05:26,20 --> 00:05:28,20
但是我们是在 VRAR 眼镜里面，
142
00:05:28,260 --> 00:05:30,150
然后呢我们配合上仿真引形，
143
00:05:30,170 --> 00:05:32,190
比如叠衣服这样的叠的过程中，
144
00:05:32,190 --> 00:05:35,50
其实我们是操作这个叠的知识被记录下来。
145
00:05:35,110 --> 00:05:37,630
当然这里面还有一个环节是我们衣服操作，
146
00:05:37,630 --> 00:05:39,830
完了我们要跟真机进行仿真对齐。
147
00:05:40,90 --> 00:05:42,410
就是我们是这种是在仿真里面训的，
148
00:05:42,490 --> 00:05:45,430
当然是是用手的操作记录它的操作知识。
149
00:05:45,430 --> 00:05:46,970
最后的我们要跟真机对齐，
150
00:05:46,970 --> 00:05:48,530
可能需要这么一步啊，
151
00:05:48,550 --> 00:05:49,750
这是唯一不一样的。
152
00:05:49,830 --> 00:05:50,830
那么为此的话，
153
00:05:50,830 --> 00:05:52,230
我们就能够做到了。
154
00:05:52,230 --> 00:05:59,170
应该我认为是第一个实际上第一个能够去做任意啊物体的物物体的那个呃衣服的操作。
155
00:05:59,370 --> 00:06:01,869
因为衣服它的难点是跟比如去抓个东西啊，
156
00:06:01,869 --> 00:06:02,850
或拉的东西不一样，
157
00:06:02,850 --> 00:06:04,910
它是它整个状态是不确定的。
158
00:06:04,910 --> 00:06:07,950
而且你看刚才我们扔进去的衣服是状态不确定的。
159
00:06:08,230 --> 00:06:08,470
呃，
160
00:06:08,630 --> 00:06:11,310
那么相比于呃像之前的方法的话，
161
00:06:11,310 --> 00:06:13,270
他们是首先衣服要铺的比较好，
162
00:06:13,510 --> 00:06:15,970
而且它叠的它是一个纯色的啊，
163
00:06:15,970 --> 00:06:17,10
它要叠起来的话，
164
00:06:17,10 --> 00:06:21,5
需要很多的很很很的那个就是相对来说是比较简单的，
165
00:06:21,810 --> 00:06:23,170
我们是能做复杂的。
166
00:06:23,170 --> 00:06:25,190
那么我们有了这样的一个知识之后呢，
167
00:06:25,190 --> 00:06:26,530
我们可以干什么事情呢？
168
00:06:26,650 --> 00:06:28,30
我们可以做交互感知了。
169
00:06:28,190 --> 00:06:31,270
比如说我们现在从从能感知到这个微波炉啊，
170
00:06:31,270 --> 00:06:32,490
是可以这样拉的啊。
171
00:06:32,490 --> 00:06:33,970
但是说我们感知到微波炉之后，
172
00:06:33,970 --> 00:06:35,810
我们发现还是有问题呢啊，
173
00:06:35,850 --> 00:06:37,270
就是它感知可能不准，
174
00:06:37,390 --> 00:06:40,890
那我们就引入了交互去纠正它的感知啊，
175
00:06:40,950 --> 00:06:42,10
那么怎么做呢？
176
00:06:42,50 --> 00:06:43,830
因为我们去拉的时候呢，
177
00:06:43,830 --> 00:06:46,470
我们能够预判它下一步应该怎么样子。
178
00:06:46,470 --> 00:06:47,790
因为我们有这个世界的模型，
179
00:06:47,790 --> 00:06:48,130
对吧？
180
00:06:48,150 --> 00:06:48,970
我们可以估计，
181
00:06:49,170 --> 00:06:50,870
那么我们就可以真实的对比。
182
00:06:51,50 --> 00:06:52,490
像这个是我们估计的，
183
00:06:52,490 --> 00:06:53,310
是我们真实的。
184
00:06:53,310 --> 00:06:54,830
然后我们去最小化它的误差，
185
00:06:55,70 --> 00:06:55,310
哎，
186
00:06:55,390 --> 00:06:59,670
最小化它误差就会逼着什么逼这个模型必须估计对你估计不对的话，
187
00:06:59,670 --> 00:07:02,70
你这个你这个事情就就会出错。
188
00:07:02,310 --> 00:07:03,590
那通过这个 loss 之后呢，
189
00:07:03,590 --> 00:07:05,610
我们就可以看到这个这个序的事情，
190
00:07:05,810 --> 00:07:09,350
就是我们从开始这个世界的模型有一点点的错误。
191
00:07:09,530 --> 00:07:10,750
通过操作过程中呢，
192
00:07:10,750 --> 00:07:12,230
引入这个操作者的纠正，
193
00:07:12,450 --> 00:07:18,250
然后我们就能够把这个这个误差呃大量的大大量的大量的下正。
194
00:07:18,670 --> 00:07:19,210
然后呢，
195
00:07:19,270 --> 00:07:20,70
在此的话呢，
196
00:07:20,70 --> 00:07:21,170
我们有另一个工作作，
197
00:07:21,170 --> 00:07:23,800
我们可以把把这个东西放到呃更难的。
198
00:07:23,800 --> 00:07:24,760
比如穿针上面，
199
00:07:25,20 --> 00:07:27,440
而且我们这个东西不只是对于这个真的模型，
200
00:07:27,440 --> 00:07:30,200
而且我们会带上那个观测的机械臂啊，
201
00:07:30,200 --> 00:07:31,980
这个观测机械臂的话啊，
202
00:07:32,0 --> 00:07:35,60
可以去看到说去地形不停的对它的纠正，
203
00:07:35,280 --> 00:07:41,780
这也是获得今年的呃国际机器人体会 IS 的最佳系统论文提名啊是唯一的法原单位。
204
00:07:42,320 --> 00:07:42,560
好，
205
00:07:42,820 --> 00:07:45,960
下面我们就有了感知到基本知道这个世界什么样子，
206
00:07:45,960 --> 00:07:47,520
那我们脑子肯定要过一遍了。
207
00:07:47,680 --> 00:07:49,380
那为什么这件事情那么重要的？
208
00:07:49,380 --> 00:07:51,160
因为我们脑子里的话，
209
00:07:51,160 --> 00:07:54,120
仿真是把这种物理的约束加进去的。
210
00:07:54,280 --> 00:07:58,200
所以说我们就会一个科学问题是如何把物理知识的约束啊，
211
00:07:58,200 --> 00:08:00,600
去降低开发环境下的那个决策学习。
212
00:08:00,860 --> 00:08:01,100
啊，
213
00:08:01,100 --> 00:08:02,660
那么这个机器人的话呢，
214
00:08:02,660 --> 00:08:04,300
这是一个全新的问题啊，
215
00:08:04,300 --> 00:08:04,800
为什么呢？
216
00:08:04,800 --> 00:08:06,760
因为它要求又准又快啊，
217
00:08:06,760 --> 00:08:08,260
这是很复杂的问题啊。
218
00:08:08,260 --> 00:08:10,240
当然仿真引擎大家可能知道的多了，
219
00:08:10,240 --> 00:08:11,680
有游戏仿真引擎啊，
220
00:08:11,680 --> 00:08:12,100
它快，
221
00:08:12,380 --> 00:08:14,540
但物理上不准工业引擎，
222
00:08:14,540 --> 00:08:15,160
它准，
223
00:08:15,320 --> 00:08:16,320
但是它不够快。
224
00:08:16,560 --> 00:08:19,200
因为我们机器学习是需要机械学习，
225
00:08:19,200 --> 00:08:21,585
要高速的和那个仿真引擎进行交互。
226
00:08:21,690 --> 00:08:21,930
对，
227
00:08:22,150 --> 00:08:23,185
因此的话呢，
228
00:08:24,630 --> 00:08:29,530
因此的话呢我们就呃因此我们的话我们就是对这个数学物理方程进行了重写，
229
00:08:29,610 --> 00:08:31,570
以及做了很多软件工程的工作。
230
00:08:31,870 --> 00:08:33,310
之前他们有求快呢，
231
00:08:33,490 --> 00:08:36,490
他们是为了是要把各个模态进行独立的，
232
00:08:36,490 --> 00:08:37,870
而我们建立了联合方程，
233
00:08:38,90 --> 00:08:39,930
还有很多呃细节我就不讲了。
234
00:08:39,930 --> 00:08:40,509
包括你看，
235
00:08:40,509 --> 00:08:41,770
如果他们不这么做，
236
00:08:41,770 --> 00:08:42,530
他们的穿模，
237
00:08:42,790 --> 00:08:47,990
而我们就能够非常好的在这种多态中能够取得非常好的那个仿真效果。
238
00:08:48,210 --> 00:08:51,530
我们的实验也表明了我们的速度能提升四百多倍，
239
00:08:51,710 --> 00:08:53,270
跟最好的工业软件。
240
00:08:53,490 --> 00:08:54,990
我们的误差在一毫米内，
241
00:08:55,130 --> 00:09:05,510
就是能够支持高水平的物理仿真和同时能够呃同时能够呃同时能够快速的响应这样的一个机器人仿真系统啊，
242
00:09:05,510 --> 00:09:06,870
机器人的那个真机系统。
243
00:09:07,150 --> 00:09:07,390
啊，
244
00:09:07,450 --> 00:09:10,170
这时候你会看到是我们呃我们跟力学，
245
00:09:10,170 --> 00:09:13,410
这是我们的那个用用仿真的这是真机的力学。
246
00:09:13,410 --> 00:09:15,710
我们测出来的这个误差是相对比较小的，
247
00:09:15,710 --> 00:09:19,890
也是在里面有很多数学物理万物理方程来在在支撑。
248
00:09:19,890 --> 00:09:25,560
所以我们对柔性物体的仿真呃在底层上去重写了它的那个数学物理方程，
249
00:09:25,800 --> 00:09:27,520
还有像那个输液啊等等，
250
00:09:27,520 --> 00:09:29,160
还有那个水流啊，
251
00:09:29,160 --> 00:09:33,180
这些都是一些呃非常有难度的这些呃仿真啊，
252
00:09:33,180 --> 00:09:36,60
但是关键它速度要快好啊，
253
00:09:36,60 --> 00:09:38,40
这是还有它的渗透系统啊，
254
00:09:38,40 --> 00:09:43,0
就相当于是这是把物理的那种规律嵌到这个学习系统里面，
255
00:09:43,0 --> 00:09:45,320
导致它的准确率会更高。
256
00:09:45,320 --> 00:09:45,460
呃，
257
00:09:45,460 --> 00:09:45,900
那好，
258
00:09:45,900 --> 00:09:48,700
那我们是相比于国际上的这几个仿真引擎，
259
00:09:48,700 --> 00:09:49,420
包括 stanford，
260
00:09:49,420 --> 00:09:51,340
还有那个 MIT 啊，
261
00:09:51,340 --> 00:09:53,40
我们有以下的优势。
262
00:09:53,40 --> 00:09:55,20
从公开发表的结果看啊，
263
00:09:55,20 --> 00:09:56,600
有以下和啊这种是开源的啊，
264
00:09:56,600 --> 00:09:57,520
就是 logo flow。
265
00:09:57,520 --> 00:09:58,780
大家可以搜一下啊，
266
00:09:58,780 --> 00:10:00,400
已经有康奈尔的之类，
267
00:10:00,420 --> 00:10:00,660
呃，
268
00:10:00,700 --> 00:10:02,765
多个单位在使用发了论文。
269
00:10:03,650 --> 00:10:03,890
好，
270
00:10:04,130 --> 00:10:05,430
那么有了这个事情的话，
271
00:10:05,430 --> 00:10:06,290
我们能干些什么？
272
00:10:06,290 --> 00:10:07,630
很有趣的事情呢？
273
00:10:07,630 --> 00:10:12,850
我们一个非常好的一个物理仿真是必须快跟人工智能系统那个高速的一起。
274
00:10:12,850 --> 00:10:13,90
对，
275
00:10:13,170 --> 00:10:13,870
并撞的话，
276
00:10:13,950 --> 00:10:15,930
我们通过深速学习规划之后呢，
277
00:10:15,930 --> 00:10:17,470
我们就会得到一个真实场景。
278
00:10:17,690 --> 00:10:19,650
那么脑子里其实有一个仿真引擎，
279
00:10:19,650 --> 00:10:19,950
对吧？
280
00:10:19,950 --> 00:10:21,670
我们仿真这个物理参数，
281
00:10:21,670 --> 00:10:26,830
但这物理参数我们不可能去测量的那我我们可以仿仿它的的物理参数。
282
00:10:26,830 --> 00:10:27,850
如果你仿真错了，
283
00:10:27,850 --> 00:10:29,650
你真真实情况是一个误差的。
284
00:10:29,650 --> 00:10:33,650
我们通约这个误差逼逼着它这个物参参必须得对，
285
00:10:33,670 --> 00:10:35,730
才能够跟真机跟真实场景一致，
286
00:10:35,970 --> 00:10:38,610
那就逼着它去估计这个真实的物理参数，
287
00:10:38,730 --> 00:10:39,630
像这个叠衣服，
288
00:10:39,850 --> 00:10:41,290
包括这么多量啊，
289
00:10:41,370 --> 00:10:43,770
我们就是等于说整个系统的高速的跑起来，
290
00:10:43,770 --> 00:10:46,90
仿真和深度学习能联合的跑起来。
291
00:10:46,290 --> 00:10:46,530
哎，
292
00:10:46,650 --> 00:10:48,70
那么我们就可以看到说，
293
00:10:48,70 --> 00:10:48,310
哎，
294
00:10:48,430 --> 00:10:49,330
下面是真机，
295
00:10:49,470 --> 00:10:51,430
下面是仿真这种柔性物体，
296
00:10:51,430 --> 00:10:52,750
其实钢体还比较好弄，
297
00:10:52,770 --> 00:10:54,130
柔性物体是非常难的。
298
00:10:54,290 --> 00:10:56,10
我们能够不能说分毫不差，
299
00:10:56,10 --> 00:10:56,830
那是很接近，
300
00:10:57,50 --> 00:10:59,230
导致我们的行为能比较准确啊，
301
00:10:59,250 --> 00:11:04,280
那么这是我们估计出来的各种的那个物理参数和真实的比较还是很接近的。
302
00:11:04,520 --> 00:11:04,760
啊。
303
00:11:05,500 --> 00:11:06,460
好呃，
304
00:11:06,580 --> 00:11:08,420
那么当然说这个事情的话，
305
00:11:08,420 --> 00:11:10,720
那当然我们进一步有这仿真引擎，
306
00:11:10,960 --> 00:11:16,500
同时可能也会支持这个海量的视频去学习那个实践的行为啊，
307
00:11:16,500 --> 00:11:17,940
去实现那个行为。
308
00:11:18,260 --> 00:11:18,500
啊，
309
00:11:18,540 --> 00:11:25,840
当然说为此我们还构建了从怎么样从那个演示视频和仿真系统对齐的这个一个系统啊，
310
00:11:25,920 --> 00:11:27,260
然后最后切入到真机。
311
00:11:27,280 --> 00:11:28,100
因为时间有限，
312
00:11:28,100 --> 00:11:29,380
我这就啊不讲了，
313
00:11:29,380 --> 00:11:32,80
这也获得了 i loss 的呃 ISS 的，
314
00:11:32,80 --> 00:11:35,60
不是那个按那个 i loss 的最佳论 m 机啊，
315
00:11:35,100 --> 00:11:36,780
这是我们的整个拍拍断视频，
316
00:11:36,780 --> 00:11:38,240
理解对接到仿真。
317
00:11:38,520 --> 00:11:39,560
然后最后到真机，
318
00:11:39,640 --> 00:11:41,595
把刚才那整套把它串起来。
319
00:11:42,140 --> 00:11:42,380
好，
320
00:11:42,740 --> 00:11:44,140
那么最后是居身执行。
321
00:11:44,440 --> 00:11:44,680
呃，
322
00:11:44,700 --> 00:11:47,80
这个事情其实是一个大家可能没感觉到，
323
00:11:47,80 --> 00:11:48,660
让我们觉得很疼的一个问题。
324
00:11:48,820 --> 00:11:50,20
就你看现在来说，
325
00:11:50,380 --> 00:11:52,660
为什么我们觉得大脑规划完就结束呢？
326
00:11:52,660 --> 00:11:57,460
其实不是我们实现下来是我们人为什么觉得大脑就结搞完就结束，
327
00:11:57,460 --> 00:11:59,500
是因为我们一个神经下意识反应，
328
00:11:59,700 --> 00:12:02,200
就是我们一个大小脑和这个神经系统，
329
00:12:02,460 --> 00:12:06,940
使得我们呃神经系统使得我们那个能够去去下意识，
330
00:12:06,940 --> 00:12:07,780
去做很多东西。
331
00:12:07,780 --> 00:12:12,220
但是机器人如何去实现这种鲁棒的下意识呢是很关键的。
332
00:12:12,220 --> 00:12:14,120
因为机器人存在感知误差、
333
00:12:14,120 --> 00:12:14,900
仿真误差，
334
00:12:14,900 --> 00:12:15,920
还有机械误差，
335
00:12:15,920 --> 00:12:16,840
其实人也是一样的。
336
00:12:16,840 --> 00:12:20,120
其实大家有没有觉得我们的感知其实是有误差的。
337
00:12:20,120 --> 00:12:21,940
就是比如说我们去感知个东西，
338
00:12:21,940 --> 00:12:23,160
我们脑子里其实这么想，
339
00:12:23,160 --> 00:12:24,720
但其实我们闭上眼睛的时候，
340
00:12:24,840 --> 00:12:25,840
其实很大的误差。
341
00:12:26,120 --> 00:12:26,320
嗯，
342
00:12:26,320 --> 00:12:28,520
那么这个时候呢就是这个误差怎么样？
343
00:12:28,520 --> 00:12:29,740
人工智能把它吃掉。
344
00:12:29,740 --> 00:12:29,980
呃，
345
00:12:30,360 --> 00:12:32,380
这也就是说为什么能吃掉？
346
00:12:32,380 --> 00:12:34,480
是因为我们刚才有个规划，
347
00:12:34,480 --> 00:12:39,740
其其规划里里有底层层实有一个又一睛的那个子的那个原子的那个操作。
348
00:12:39,740 --> 00:12:42,790
如果那个原子操作能够能够被通用化，
349
00:12:42,790 --> 00:12:44,570
被这个人工智能吃掉的话，
350
00:12:44,710 --> 00:12:44,950
哎，
351
00:12:45,10 --> 00:12:48,490
那么我们就有可能去做成这样的一个事情啊，
352
00:12:48,670 --> 00:12:50,470
那么这为什么能做到呢？
353
00:12:50,470 --> 00:12:53,430
是因为其实它和同一个动作，
354
00:12:53,430 --> 00:12:55,190
它其实在拓扑上同源的，
355
00:12:55,190 --> 00:12:58,710
比如这个翻的动作其实之后会拓扑同源到这个东西，
356
00:12:58,710 --> 00:13:00,250
这个不轴转这个事情，
357
00:13:00,250 --> 00:13:02,175
所以有存在一种可能性。
358
00:13:03,170 --> 00:13:03,410
嗯，
359
00:13:03,670 --> 00:13:06,790
那么为此我们会去解决这样的一个操作的问题。
360
00:13:06,790 --> 00:13:09,690
我们第一步是要解决通用的抓取问题。
361
00:13:09,950 --> 00:13:11,190
那么通用的抓取问题，
362
00:13:11,190 --> 00:13:14,310
它的最大的问题是数据困境还是回到的数据问题。
363
00:13:14,310 --> 00:13:16,530
就是说哎我们有这么多的点云，
364
00:13:16,530 --> 00:13:23,530
那我们怎么会变成怎么变成这些抓取的密集数据之前是没办法去制备这种大规模的数据的。
365
00:13:23,870 --> 00:13:24,110
呃，
366
00:13:24,290 --> 00:13:29,390
因此呢我们提出了一套啊半自动的那个数据的那个采集方式，
367
00:13:29,550 --> 00:13:32,470
我们的整个的数据效率提高了标注，
368
00:13:32,470 --> 00:13:33,350
提高了一万倍。
369
00:13:33,670 --> 00:13:33,910
啊，
370
00:13:34,10 --> 00:13:37,230
那使得我们能够快速的大规模的制备啊，
371
00:13:37,290 --> 00:13:38,570
大量的那个数据，
372
00:13:38,570 --> 00:13:40,490
不然的话很长时间啊，
373
00:13:40,690 --> 00:13:42,330
基本原理是啊，
374
00:13:42,450 --> 00:13:45,450
我们通过那个这是真实点云，
375
00:13:45,670 --> 00:13:47,850
然后我们有一套数字完成系统，
376
00:13:48,150 --> 00:13:50,430
能够去通过这枚数字完成到这里，
377
00:13:50,430 --> 00:13:53,10
然后我们可以生成它的呃抓取的，
378
00:13:53,10 --> 00:13:55,810
在用用仿真生成它抓取取点云，
379
00:13:55,810 --> 00:13:58,570
然后就可以在真实的点云上打上很多标签，
380
00:13:58,570 --> 00:14:02,815
大然在整套系统是里面嵌了很多算法和半自动的这样的系统。
381
00:14:03,270 --> 00:14:03,510
啊，
382
00:14:03,550 --> 00:14:06,210
为此我们能够大大的扩展了，
383
00:14:06,210 --> 00:14:09,90
是比目前数集啊是二十一个数据，
384
00:14:09,90 --> 00:14:13,510
有效数据能够有效的扩张到啊十万倍这样的数据，
385
00:14:13,530 --> 00:14:15,890
而且标注的效率会大大提高。
386
00:14:16,150 --> 00:14:21,210
那么也配套提出了新的那个大模型的那个抓取大模型的算法啊，
387
00:14:21,250 --> 00:14:23,550
这里就不展开来讲。
388
00:14:24,10 --> 00:14:30,640
那么呃为此的话呢呃我们就会因此我们得到了一个通用的抓取的模型。
389
00:14:31,0 --> 00:14:32,280
那么通用抓取模型，
390
00:14:32,280 --> 00:14:34,300
我们跟别的抓取有的不一样，
391
00:14:34,460 --> 00:14:35,640
我们没见过的时候，
392
00:14:35,640 --> 00:14:36,700
我们都能够去抓。
393
00:14:36,720 --> 00:14:38,300
比如我们刚才这个东西被敲碎，
394
00:14:38,720 --> 00:14:42,860
这个瓷器被敲碎瞬间其实每一个快都是没有见过的，
395
00:14:43,0 --> 00:14:43,820
但没有见过。
396
00:14:43,820 --> 00:14:46,400
我们仍然能够把它抓起来啊，
397
00:14:46,460 --> 00:14:48,640
我们是在五千个呃没有见过，
398
00:14:48,640 --> 00:14:52,860
完全没有见过的物体上面能够能够被被被通用的抓取。
399
00:14:53,100 --> 00:14:55,920
包括我们也扩张到了那个去动态的物体。
400
00:14:55,920 --> 00:14:58,520
这也是世界上第一个能够抓通用的，
401
00:14:58,520 --> 00:14:59,40
没有抓，
402
00:14:59,40 --> 00:15:01,600
没有通过见过的那个物体的抓鱼，
403
00:15:01,660 --> 00:15:04,380
就呃抓那个没有见过的动态的物体啊，
404
00:15:04,380 --> 00:15:06,320
这个也发表在那个机器人的底盘。
405
00:15:06,320 --> 00:15:07,560
 TIO 上面啊，
406
00:15:07,560 --> 00:15:11,620
我们也是首次去呃跟人类比较超过人类水平，
407
00:15:11,620 --> 00:15:13,420
通准确率和速度上面。
408
00:15:13,560 --> 00:15:15,820
所以没有见过物物的的抓取，
409
00:15:16,40 --> 00:15:16,960
超过人类水平，
410
00:15:16,960 --> 00:15:20,20
也被斯坦福的那个机器人抓取排行榜，
411
00:15:20,140 --> 00:15:23,705
被被被那个评为近十年影响力第二名。
412
00:15:25,990 --> 00:15:26,230
好，
413
00:15:26,370 --> 00:15:29,550
那么最后讲的一个就是以秘诀为中心的框架。
414
00:15:29,810 --> 00:15:30,30
呃，
415
00:15:30,30 --> 00:15:31,250
我们会思考一个问题，
416
00:15:31,290 --> 00:15:36,670
就是说哎那么我们的话就是说我们之前谷歌不是提出了一种大模型嘛，
417
00:15:36,670 --> 00:15:38,530
我们我在反思一下我们的大模型啊，
418
00:15:38,670 --> 00:15:40,390
就是它是视觉输入，
419
00:15:40,390 --> 00:15:42,770
然后通过大模型去得到一个位置决策。
420
00:15:43,130 --> 00:15:43,370
哎，
421
00:15:43,430 --> 00:15:44,750
那这种事情有什么问题呢？
422
00:15:44,750 --> 00:15:45,930
看起来是没问题的，
423
00:15:46,70 --> 00:15:47,90
但是你会没有发觉，
424
00:15:47,90 --> 00:15:49,130
他们就这个更多的做移动，
425
00:15:49,130 --> 00:15:51,30
把这个东西移到了另一个地方，
426
00:15:51,230 --> 00:15:53,530
就是没有做那种 rich contact，
427
00:15:53,590 --> 00:15:55,450
是那个那个 rich context 侧，
428
00:15:55,450 --> 00:15:57,850
是那种呃那个很丰富的接触。
429
00:15:58,230 --> 00:15:59,570
而这个事情呢，
430
00:15:59,570 --> 00:16:02,810
我们如果需要突破很复杂的接触的问题，
431
00:16:03,110 --> 00:16:06,270
那我们需要以立觉为中心的一个框出，
432
00:16:06,270 --> 00:16:09,390
就是位置引导下的率反馈的一个输出。
433
00:16:09,650 --> 00:16:12,210
那么因此构建了这样的立觉的一个事情，
434
00:16:12,210 --> 00:16:13,870
去去做这样的一个事情。
435
00:16:13,870 --> 00:16:15,490
那我们来看看有一件事情，
436
00:16:15,490 --> 00:16:18,130
就是位置控制的一定是做不了的。
437
00:16:18,130 --> 00:16:19,510
就我们光这个气球啊，
438
00:16:19,930 --> 00:16:21,370
你你你想这个问题，
439
00:16:21,370 --> 00:16:22,470
如果我们要位置控制，
440
00:16:22,590 --> 00:16:25,70
你只要往下错一毫米就爆掉了，
441
00:16:25,70 --> 00:16:27,460
网上上一毫米就刮不干净。
442
00:16:27,460 --> 00:16:30,740
所以这里面它的我们这个模型的输出是用大模型输出的，
443
00:16:30,740 --> 00:16:32,680
它的力的摁压力是多少，
444
00:16:32,680 --> 00:16:34,220
以及它倾向力是多少。
445
00:16:34,220 --> 00:16:36,260
但是可能大概有个位置的一个引导。
446
00:16:36,440 --> 00:16:43,420
所以如果未来用到的家用医疗等数和这个一个非常好的力反馈的一个力觉大模型来做这样的一个事情。
447
00:16:43,680 --> 00:16:48,460
这也是我们一直在呃希望在做这样的一个在在做的一个过程。
448
00:16:48,620 --> 00:16:54,420
那包括我们的能够做到非常精密的那种这种力的控制就是轮扩到它这样。
449
00:16:54,420 --> 00:16:59,640
那其实也是说会在你的力觉参数和这个位置引导是由模型来输出。
450
00:16:59,940 --> 00:17:06,540
这就是说以力觉为中心会是将来去解决这个小脑这部分很关键的一个一个点。
451
00:17:06,880 --> 00:17:07,100
啊，
452
00:17:07,100 --> 00:17:10,320
为此的话我们也是开源了线上的最大的。
453
00:17:10,340 --> 00:17:13,100
目前最大的以力觉为中心的数据集啊啊，
454
00:17:13,140 --> 00:17:18,420
大家欢迎下载我们的数据集量是那个 GTP 数数据量的两倍。
455
00:17:18,420 --> 00:17:19,620
我们拥有力觉、
456
00:17:19,620 --> 00:17:20,200
听觉、
457
00:17:20,220 --> 00:17:20,820
语言、
458
00:17:20,820 --> 00:17:24,780
控制等觉等等这些这样的一个事情。
459
00:17:24,780 --> 00:17:27,910
那我们的力的操作平台是比较特殊的。
460
00:17:27,910 --> 00:17:31,830
我们能够哎 osorry 哦，
461
00:17:31,830 --> 00:17:32,70
好，
462
00:17:32,90 --> 00:17:33,690
这里其实是一张一个视频。
463
00:17:33,870 --> 00:17:35,350
这本来是我们的切火机，
464
00:17:35,350 --> 00:17:37,410
就是我们人去摇操作切火机，
465
00:17:37,410 --> 00:17:41,910
能够去记录大量的那种底层的这种呃力反馈的数据。
466
00:17:42,430 --> 00:17:45,570
然后当时我们位置位置为中心的模仿学习数据库，
467
00:17:45,570 --> 00:17:46,810
我们也参与了啊，
468
00:17:46,810 --> 00:17:49,690
我们是那个大概有四十个学校啊，
469
00:17:49,690 --> 00:17:55,810
然后我们也是唯一的国内单位参与了那个对大规模的一个就呃 open x 一 body 的。
470
00:17:55,910 --> 00:17:59,530
但是是基于那个位置仿真的啊一个呃位置，
471
00:17:59,530 --> 00:18:01,870
基于位置规划的这样的一个数据啊，
472
00:18:01,870 --> 00:18:03,844
也欢迎大家来使用。
473
00:18:07,170 --> 00:18:07,410
好。
474
00:18:07,550 --> 00:18:11,330
那么就是呃我们这是我们的综合的系统的结果。
475
00:18:11,770 --> 00:18:12,175
嗯，
476
00:18:13,460 --> 00:18:13,640
呃，
477
00:18:13,640 --> 00:18:14,980
这是我们综合的系统的结果。
478
00:18:14,980 --> 00:18:23,300
就是说 expert 呃呃就是就是我们我们把我们整体的各种技术整合进去，
479
00:18:23,300 --> 00:18:25,680
做了一套呃综合的一个机器人，
480
00:18:25,820 --> 00:18:26,60
呃，
481
00:18:26,60 --> 00:18:27,900
比如做早餐啊清洁。
482
00:18:27,900 --> 00:18:29,160
那你看这个擦东西，
483
00:18:29,360 --> 00:18:30,540
其实我们为什么要擦东西？
484
00:18:30,540 --> 00:18:31,400
因为擦东西要力卷，
485
00:18:31,500 --> 00:18:32,660
因为力卷你擦不干净，
486
00:18:32,900 --> 00:18:35,180
所以我们需要那个力的那个模型，
487
00:18:35,180 --> 00:18:37,280
包括我们要去把壶拿进去啊，
488
00:18:37,480 --> 00:18:39,300
这里面会整合到我们的柔性物体。
489
00:18:39,520 --> 00:18:40,500
如果没有很好的，
490
00:18:40,500 --> 00:18:42,240
刚才我们讲的钢铁还是比较容易。
491
00:18:42,380 --> 00:18:44,760
当你柔性物体知道哪个是衣领的时候，
492
00:18:44,920 --> 00:18:47,35
它的难度就会大大的增加。
493
00:18:48,10 --> 00:18:48,750
好啊，
494
00:18:48,770 --> 00:18:49,610
这是这是。
495
00:18:49,610 --> 00:18:53,10
当然我们也把我们的很多的技术开源到我们的 mobl flow 啊，
496
00:18:53,10 --> 00:18:55,930
我们一个上面有生态 tutorial document，
497
00:18:55,930 --> 00:19:00,310
还有论坛还 github 等等连接能连接十四种机器人啊，
498
00:19:00,310 --> 00:19:03,670
也希望大家以够啊多多支持我们啊，
499
00:19:03,670 --> 00:19:05,690
当然最后我们时间有限来讲，
500
00:19:05,710 --> 00:19:08,490
我们跟怎么去研究大脑和身体间的关系，
501
00:19:08,490 --> 00:19:12,270
这是基因智能和一个行为上的一个很关键的一个事情。
502
00:19:12,290 --> 00:19:13,435
然后时间到了。
503
00:19:14,370 --> 00:19:14,610
哦，
504
00:19:14,650 --> 00:19:14,850
好，
505
00:19:14,850 --> 00:19:16,210
那那刚刚好呃，
506
00:19:16,270 --> 00:19:19,70
那最后是我们这些呃做的一些论文，
507
00:19:19,70 --> 00:19:21,390
就包括那个我们主要还在呃，
508
00:19:21,410 --> 00:19:27,70
就就除了人工智能都还有机器人的两大顶刊上面还有获得的一些呃最佳呃，
509
00:19:27,130 --> 00:19:29,445
各个会议的最机器人顶会最佳论文。
510
00:19:30,170 --> 00:19:30,330
好，
511
00:19:30,330 --> 00:19:32,690
我可能怕时间不够赶的太快啊。
512
00:19:32,690 --> 00:19:35,190
最后呢讲一句就是呃感慨一下，
513
00:19:35,190 --> 00:19:38,490
就是啊通用机器人是人工智能的终极状态。
514
00:19:38,490 --> 00:19:40,50
我们认为积分智能是个灵魂，
515
00:19:40,190 --> 00:19:42,490
也就是很有趣的一个领域啊，
516
00:19:42,490 --> 00:19:46,90
希望大家能够也能够一起来来做这么一件啊，
517
00:19:46,110 --> 00:19:48,370
让让人类更加美好的事情啊，
518
00:19:48,570 --> 00:19:50,690
然后再次推荐大家看这部电影，
519
00:19:50,690 --> 00:19:53,790
就是我每次在看的时候觉得哎你的动作我们怎么把它做成，
520
00:19:53,790 --> 00:19:55,990
也是我们可以做成的范围内啊，
521
00:19:56,210 --> 00:19:56,885
谢谢大家。
